{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zeyad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics.text.rouge import ROUGEScore\n",
    "rouge = ROUGEScore()\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summarization_algorithm import * \n",
    "from preprocessing_algorithms import*\n",
    "from efficiency_scores import *\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Articles</th>\n",
       "      <th>Summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ad sales boost Time Warner profit\\n\\nQuarterly...</td>\n",
       "      <td>TimeWarner said fourth quarter sales rose 2% t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dollar gains on Greenspan speech\\n\\nThe dollar...</td>\n",
       "      <td>The dollar has hit its highest level against t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yukos unit buyer faces loan claim\\n\\nThe owner...</td>\n",
       "      <td>Yukos' owner Menatep Group says it will ask Ro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>High fuel prices hit BA's profits\\n\\nBritish A...</td>\n",
       "      <td>Rod Eddington, BA's chief executive, said the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pernod takeover talk lifts Domecq\\n\\nShares in...</td>\n",
       "      <td>Pernod has reduced the debt it took on to fund...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Burren awarded Egyptian contracts\\n\\nBritish e...</td>\n",
       "      <td>British energy firm Burren Energy has been awa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>'Strong dollar' call halts slide\\n\\nThe US dol...</td>\n",
       "      <td>The US dollar's slide against the euro and yen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>IMF 'cuts' German growth estimate\\n\\nThe Inter...</td>\n",
       "      <td>The IMF will also reduce its growth estimate f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>GM, Ford cut output as sales fall\\n\\nUS car fi...</td>\n",
       "      <td>US sales at GM sank 12.7% in February compared...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Ebbers denies WorldCom fraud\\n\\nFormer WorldCo...</td>\n",
       "      <td>Mr Ebbers relationship to Mr Sullivan is key t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Articles  \\\n",
       "0   Ad sales boost Time Warner profit\\n\\nQuarterly...   \n",
       "1   Dollar gains on Greenspan speech\\n\\nThe dollar...   \n",
       "2   Yukos unit buyer faces loan claim\\n\\nThe owner...   \n",
       "3   High fuel prices hit BA's profits\\n\\nBritish A...   \n",
       "4   Pernod takeover talk lifts Domecq\\n\\nShares in...   \n",
       "..                                                ...   \n",
       "95  Burren awarded Egyptian contracts\\n\\nBritish e...   \n",
       "96  'Strong dollar' call halts slide\\n\\nThe US dol...   \n",
       "97  IMF 'cuts' German growth estimate\\n\\nThe Inter...   \n",
       "98  GM, Ford cut output as sales fall\\n\\nUS car fi...   \n",
       "99  Ebbers denies WorldCom fraud\\n\\nFormer WorldCo...   \n",
       "\n",
       "                                            Summaries  \n",
       "0   TimeWarner said fourth quarter sales rose 2% t...  \n",
       "1   The dollar has hit its highest level against t...  \n",
       "2   Yukos' owner Menatep Group says it will ask Ro...  \n",
       "3   Rod Eddington, BA's chief executive, said the ...  \n",
       "4   Pernod has reduced the debt it took on to fund...  \n",
       "..                                                ...  \n",
       "95  British energy firm Burren Energy has been awa...  \n",
       "96  The US dollar's slide against the euro and yen...  \n",
       "97  The IMF will also reduce its growth estimate f...  \n",
       "98  US sales at GM sank 12.7% in February compared...  \n",
       "99  Mr Ebbers relationship to Mr Sullivan is key t...  \n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"BBC_Dataset.csv\")\n",
    "df=df.iloc[:,1:]#remove first column(unnamed col)\n",
    "df.dropna(inplace=True)\n",
    "columns_titles = [\"Articles\",\"Summaries\"]\n",
    "df=df.reindex(columns=columns_titles)\n",
    "mini_df = df[:100]\n",
    "mini_df[:100]\n",
    "mini_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Daman and Diu administration on Wednesday ...</td>\n",
       "      <td>The Administration of Union Territory Daman an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From her special numbers to TV?appearances, Bo...</td>\n",
       "      <td>Malaika Arora slammed an Instagram user who tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Indira Gandhi Institute of Medical Science...</td>\n",
       "      <td>The Indira Gandhi Institute of Medical Science...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hotels in Mumbai and other Indian cities are t...</td>\n",
       "      <td>Hotels in Maharashtra will train their staff t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>An alleged suspect in a kidnapping case was fo...</td>\n",
       "      <td>A 32-year-old man on Wednesday was found hangi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Facing pressure from President Donald Trump to...</td>\n",
       "      <td>The US Air Force is negotiating with Boeing to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Allahabad, Aug 01 (PTI) The Allahabad High Cou...</td>\n",
       "      <td>The Allahabad High Court on Tuesday said it wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Motorman Mahendra Prasad called up the railway...</td>\n",
       "      <td>As many as 13 people died while travelling in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Bored in confines of a sprawling resort in Ben...</td>\n",
       "      <td>The Gujarat Congress MLAs staying in a Bengalu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>There is only a 5% chance that the Earth will ...</td>\n",
       "      <td>According to a recently published US-based res...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             original  \\\n",
       "0   The Daman and Diu administration on Wednesday ...   \n",
       "1   From her special numbers to TV?appearances, Bo...   \n",
       "2   The Indira Gandhi Institute of Medical Science...   \n",
       "3   Hotels in Mumbai and other Indian cities are t...   \n",
       "4   An alleged suspect in a kidnapping case was fo...   \n",
       "..                                                ...   \n",
       "95  Facing pressure from President Donald Trump to...   \n",
       "96  Allahabad, Aug 01 (PTI) The Allahabad High Cou...   \n",
       "97  Motorman Mahendra Prasad called up the railway...   \n",
       "98  Bored in confines of a sprawling resort in Ben...   \n",
       "99  There is only a 5% chance that the Earth will ...   \n",
       "\n",
       "                                              summary  \n",
       "0   The Administration of Union Territory Daman an...  \n",
       "1   Malaika Arora slammed an Instagram user who tr...  \n",
       "2   The Indira Gandhi Institute of Medical Science...  \n",
       "3   Hotels in Maharashtra will train their staff t...  \n",
       "4   A 32-year-old man on Wednesday was found hangi...  \n",
       "..                                                ...  \n",
       "95  The US Air Force is negotiating with Boeing to...  \n",
       "96  The Allahabad High Court on Tuesday said it wo...  \n",
       "97  As many as 13 people died while travelling in ...  \n",
       "98  The Gujarat Congress MLAs staying in a Bengalu...  \n",
       "99  According to a recently published US-based res...  \n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"dataset.csv\")\n",
    "df=df.iloc[:,1:]#remove first column(unnamed col)\n",
    "df.dropna(inplace=True)\n",
    "columns_titles = [\"original\",\"summary\"]\n",
    "df=df.reindex(columns=columns_titles)\n",
    "mini_df = df[:100]\n",
    "mini_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [00:24<04:40,  3.04s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\"\"Preprocessing\"\"\"\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m sentences, filtered_sentences \u001b[39m=\u001b[39m process_df(mini_df)\n\u001b[0;32m      3\u001b[0m \u001b[39mprint\u001b[39m(sentences)\n",
      "File \u001b[1;32mf:\\PycharmProjects\\NLP\\text_summarization\\preprocessing_algorithms.py:71\u001b[0m, in \u001b[0;36mprocess_df\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     69\u001b[0m sentences,filtered_sentences \u001b[39m=\u001b[39m[],[] \n\u001b[0;32m     70\u001b[0m \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(rows)):\n\u001b[1;32m---> 71\u001b[0m     articles_sentence , filtered_article \u001b[39m=\u001b[39m preprocessing_text_with_spacy(df\u001b[39m.\u001b[39;49miloc[row,\u001b[39m0\u001b[39;49m])\n\u001b[0;32m     72\u001b[0m     sentences\u001b[39m.\u001b[39mappend(articles_sentence)\n\u001b[0;32m     73\u001b[0m     filtered_sentences\u001b[39m.\u001b[39mappend(filtered_article)\n",
      "File \u001b[1;32mf:\\PycharmProjects\\NLP\\text_summarization\\preprocessing_algorithms.py:13\u001b[0m, in \u001b[0;36mpreprocessing_text_with_spacy\u001b[1;34m(text, lemmatization)\u001b[0m\n\u001b[0;32m     10\u001b[0m sentences \u001b[39m=\u001b[39m text\n\u001b[0;32m     12\u001b[0m \u001b[39m# Load the model (English) into spaCy\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m nlp \u001b[39m=\u001b[39m spacy\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39men_core_web_sm\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     15\u001b[0m \u001b[39m# Adding 'sentencizer' component to the pipeline\u001b[39;00m\n\u001b[0;32m     16\u001b[0m nlp\u001b[39m.\u001b[39madd_pipe(\u001b[39m'\u001b[39m\u001b[39msentencizer\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\zeyad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\__init__.py:54\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\n\u001b[0;32m     31\u001b[0m     name: Union[\u001b[39mstr\u001b[39m, Path],\n\u001b[0;32m     32\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     config: Union[Dict[\u001b[39mstr\u001b[39m, Any], Config] \u001b[39m=\u001b[39m util\u001b[39m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     38\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Language:\n\u001b[0;32m     39\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[39m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m     \u001b[39mreturn\u001b[39;00m util\u001b[39m.\u001b[39;49mload_model(\n\u001b[0;32m     55\u001b[0m         name,\n\u001b[0;32m     56\u001b[0m         vocab\u001b[39m=\u001b[39;49mvocab,\n\u001b[0;32m     57\u001b[0m         disable\u001b[39m=\u001b[39;49mdisable,\n\u001b[0;32m     58\u001b[0m         enable\u001b[39m=\u001b[39;49menable,\n\u001b[0;32m     59\u001b[0m         exclude\u001b[39m=\u001b[39;49mexclude,\n\u001b[0;32m     60\u001b[0m         config\u001b[39m=\u001b[39;49mconfig,\n\u001b[0;32m     61\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\zeyad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\util.py:432\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    430\u001b[0m     \u001b[39mreturn\u001b[39;00m get_lang_class(name\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39mblank:\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m))()\n\u001b[0;32m    431\u001b[0m \u001b[39mif\u001b[39;00m is_package(name):  \u001b[39m# installed as package\u001b[39;00m\n\u001b[1;32m--> 432\u001b[0m     \u001b[39mreturn\u001b[39;00m load_model_from_package(name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    433\u001b[0m \u001b[39mif\u001b[39;00m Path(name)\u001b[39m.\u001b[39mexists():  \u001b[39m# path to model data directory\u001b[39;00m\n\u001b[0;32m    434\u001b[0m     \u001b[39mreturn\u001b[39;00m load_model_from_path(Path(name), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\zeyad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\util.py:468\u001b[0m, in \u001b[0;36mload_model_from_package\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Load a model from an installed package.\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \n\u001b[0;32m    453\u001b[0m \u001b[39mname (str): The package name.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[39mRETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m    466\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    467\u001b[0m \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39mimport_module(name)\n\u001b[1;32m--> 468\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mload(vocab\u001b[39m=\u001b[39;49mvocab, disable\u001b[39m=\u001b[39;49mdisable, enable\u001b[39m=\u001b[39;49menable, exclude\u001b[39m=\u001b[39;49mexclude, config\u001b[39m=\u001b[39;49mconfig)\n",
      "File \u001b[1;32mc:\\Users\\zeyad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\en_core_web_sm\\__init__.py:10\u001b[0m, in \u001b[0;36mload\u001b[1;34m(**overrides)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39moverrides):\n\u001b[1;32m---> 10\u001b[0m     \u001b[39mreturn\u001b[39;00m load_model_from_init_py(\u001b[39m__file__\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moverrides)\n",
      "File \u001b[1;32mc:\\Users\\zeyad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\util.py:649\u001b[0m, in \u001b[0;36mload_model_from_init_py\u001b[1;34m(init_file, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    647\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m model_path\u001b[39m.\u001b[39mexists():\n\u001b[0;32m    648\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE052\u001b[39m.\u001b[39mformat(path\u001b[39m=\u001b[39mdata_path))\n\u001b[1;32m--> 649\u001b[0m \u001b[39mreturn\u001b[39;00m load_model_from_path(\n\u001b[0;32m    650\u001b[0m     data_path,\n\u001b[0;32m    651\u001b[0m     vocab\u001b[39m=\u001b[39;49mvocab,\n\u001b[0;32m    652\u001b[0m     meta\u001b[39m=\u001b[39;49mmeta,\n\u001b[0;32m    653\u001b[0m     disable\u001b[39m=\u001b[39;49mdisable,\n\u001b[0;32m    654\u001b[0m     enable\u001b[39m=\u001b[39;49menable,\n\u001b[0;32m    655\u001b[0m     exclude\u001b[39m=\u001b[39;49mexclude,\n\u001b[0;32m    656\u001b[0m     config\u001b[39m=\u001b[39;49mconfig,\n\u001b[0;32m    657\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\zeyad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\util.py:506\u001b[0m, in \u001b[0;36mload_model_from_path\u001b[1;34m(model_path, meta, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    504\u001b[0m overrides \u001b[39m=\u001b[39m dict_to_dot(config)\n\u001b[0;32m    505\u001b[0m config \u001b[39m=\u001b[39m load_config(config_path, overrides\u001b[39m=\u001b[39moverrides)\n\u001b[1;32m--> 506\u001b[0m nlp \u001b[39m=\u001b[39m load_model_from_config(\n\u001b[0;32m    507\u001b[0m     config,\n\u001b[0;32m    508\u001b[0m     vocab\u001b[39m=\u001b[39;49mvocab,\n\u001b[0;32m    509\u001b[0m     disable\u001b[39m=\u001b[39;49mdisable,\n\u001b[0;32m    510\u001b[0m     enable\u001b[39m=\u001b[39;49menable,\n\u001b[0;32m    511\u001b[0m     exclude\u001b[39m=\u001b[39;49mexclude,\n\u001b[0;32m    512\u001b[0m     meta\u001b[39m=\u001b[39;49mmeta,\n\u001b[0;32m    513\u001b[0m )\n\u001b[0;32m    514\u001b[0m \u001b[39mreturn\u001b[39;00m nlp\u001b[39m.\u001b[39mfrom_disk(model_path, exclude\u001b[39m=\u001b[39mexclude, overrides\u001b[39m=\u001b[39moverrides)\n",
      "File \u001b[1;32mc:\\Users\\zeyad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\util.py:554\u001b[0m, in \u001b[0;36mload_model_from_config\u001b[1;34m(config, meta, vocab, disable, enable, exclude, auto_fill, validate)\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[39m# This will automatically handle all codes registered via the languages\u001b[39;00m\n\u001b[0;32m    552\u001b[0m \u001b[39m# registry, including custom subclasses provided via entry points\u001b[39;00m\n\u001b[0;32m    553\u001b[0m lang_cls \u001b[39m=\u001b[39m get_lang_class(nlp_config[\u001b[39m\"\u001b[39m\u001b[39mlang\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m--> 554\u001b[0m nlp \u001b[39m=\u001b[39m lang_cls\u001b[39m.\u001b[39;49mfrom_config(\n\u001b[0;32m    555\u001b[0m     config,\n\u001b[0;32m    556\u001b[0m     vocab\u001b[39m=\u001b[39;49mvocab,\n\u001b[0;32m    557\u001b[0m     disable\u001b[39m=\u001b[39;49mdisable,\n\u001b[0;32m    558\u001b[0m     enable\u001b[39m=\u001b[39;49menable,\n\u001b[0;32m    559\u001b[0m     exclude\u001b[39m=\u001b[39;49mexclude,\n\u001b[0;32m    560\u001b[0m     auto_fill\u001b[39m=\u001b[39;49mauto_fill,\n\u001b[0;32m    561\u001b[0m     validate\u001b[39m=\u001b[39;49mvalidate,\n\u001b[0;32m    562\u001b[0m     meta\u001b[39m=\u001b[39;49mmeta,\n\u001b[0;32m    563\u001b[0m )\n\u001b[0;32m    564\u001b[0m \u001b[39mreturn\u001b[39;00m nlp\n",
      "File \u001b[1;32mc:\\Users\\zeyad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\language.py:1773\u001b[0m, in \u001b[0;36mLanguage.from_config\u001b[1;34m(cls, config, vocab, disable, enable, exclude, meta, auto_fill, validate)\u001b[0m\n\u001b[0;32m   1767\u001b[0m warn_if_jupyter_cupy()\n\u001b[0;32m   1769\u001b[0m \u001b[39m# Note that we don't load vectors here, instead they get loaded explicitly\u001b[39;00m\n\u001b[0;32m   1770\u001b[0m \u001b[39m# inside stuff like the spacy train function. If we loaded them here,\u001b[39;00m\n\u001b[0;32m   1771\u001b[0m \u001b[39m# then we would load them twice at runtime: once when we make from config,\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[39m# and then again when we load from disk.\u001b[39;00m\n\u001b[1;32m-> 1773\u001b[0m nlp \u001b[39m=\u001b[39m lang_cls(vocab\u001b[39m=\u001b[39;49mvocab, create_tokenizer\u001b[39m=\u001b[39;49mcreate_tokenizer, meta\u001b[39m=\u001b[39;49mmeta)\n\u001b[0;32m   1774\u001b[0m \u001b[39mif\u001b[39;00m after_creation \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1775\u001b[0m     nlp \u001b[39m=\u001b[39m after_creation(nlp)\n",
      "File \u001b[1;32mc:\\Users\\zeyad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\language.py:190\u001b[0m, in \u001b[0;36mLanguage.__init__\u001b[1;34m(self, vocab, max_length, meta, create_tokenizer, batch_size, **kwargs)\u001b[0m\n\u001b[0;32m    188\u001b[0m     tokenizer_cfg \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mtokenizer\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_config[\u001b[39m\"\u001b[39m\u001b[39mnlp\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtokenizer\u001b[39m\u001b[39m\"\u001b[39m]}\n\u001b[0;32m    189\u001b[0m     create_tokenizer \u001b[39m=\u001b[39m registry\u001b[39m.\u001b[39mresolve(tokenizer_cfg)[\u001b[39m\"\u001b[39m\u001b[39mtokenizer\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m--> 190\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer \u001b[39m=\u001b[39m create_tokenizer(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m    191\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size \u001b[39m=\u001b[39m batch_size\n\u001b[0;32m    192\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefault_error_handler \u001b[39m=\u001b[39m raise_error\n",
      "File \u001b[1;32mc:\\Users\\zeyad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\language.py:92\u001b[0m, in \u001b[0;36mcreate_tokenizer.<locals>.tokenizer_factory\u001b[1;34m(nlp)\u001b[0m\n\u001b[0;32m     90\u001b[0m suffix_search \u001b[39m=\u001b[39m util\u001b[39m.\u001b[39mcompile_suffix_regex(suffixes)\u001b[39m.\u001b[39msearch \u001b[39mif\u001b[39;00m suffixes \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     91\u001b[0m infix_finditer \u001b[39m=\u001b[39m util\u001b[39m.\u001b[39mcompile_infix_regex(infixes)\u001b[39m.\u001b[39mfinditer \u001b[39mif\u001b[39;00m infixes \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m---> 92\u001b[0m \u001b[39mreturn\u001b[39;00m Tokenizer(\n\u001b[0;32m     93\u001b[0m     nlp\u001b[39m.\u001b[39;49mvocab,\n\u001b[0;32m     94\u001b[0m     rules\u001b[39m=\u001b[39;49mnlp\u001b[39m.\u001b[39;49mDefaults\u001b[39m.\u001b[39;49mtokenizer_exceptions,\n\u001b[0;32m     95\u001b[0m     prefix_search\u001b[39m=\u001b[39;49mprefix_search,\n\u001b[0;32m     96\u001b[0m     suffix_search\u001b[39m=\u001b[39;49msuffix_search,\n\u001b[0;32m     97\u001b[0m     infix_finditer\u001b[39m=\u001b[39;49minfix_finditer,\n\u001b[0;32m     98\u001b[0m     token_match\u001b[39m=\u001b[39;49mnlp\u001b[39m.\u001b[39;49mDefaults\u001b[39m.\u001b[39;49mtoken_match,\n\u001b[0;32m     99\u001b[0m     url_match\u001b[39m=\u001b[39;49mnlp\u001b[39m.\u001b[39;49mDefaults\u001b[39m.\u001b[39;49murl_match,\n\u001b[0;32m    100\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\zeyad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\tokenizer.pyx:75\u001b[0m, in \u001b[0;36mspacy.tokenizer.Tokenizer.__init__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\zeyad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\tokenizer.pyx:574\u001b[0m, in \u001b[0;36mspacy.tokenizer.Tokenizer._load_special_cases\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\zeyad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\tokenizer.pyx:618\u001b[0m, in \u001b[0;36mspacy.tokenizer.Tokenizer.add_special_case\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\zeyad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\tokenizer.pyx:169\u001b[0m, in \u001b[0;36mspacy.tokenizer.Tokenizer._tokenize_affixes\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\zeyad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\tokens\\doc.pyx:232\u001b[0m, in \u001b[0;36mspacy.tokens.doc.Doc.__init__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\zeyad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\tokens\\_dict_proxies.py:29\u001b[0m, in \u001b[0;36mSpanGroups.__init__\u001b[1;34m(self, doc, items)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m     27\u001b[0m     \u001b[39mself\u001b[39m, doc: \u001b[39m\"\u001b[39m\u001b[39mDoc\u001b[39m\u001b[39m\"\u001b[39m, items: Iterable[Tuple[\u001b[39mstr\u001b[39m, SpanGroup]] \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m()\n\u001b[0;32m     28\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 29\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdoc_ref \u001b[39m=\u001b[39m weakref\u001b[39m.\u001b[39;49mref(doc)\n\u001b[0;32m     30\u001b[0m     UserDict\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, items)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"Preprocessing\"\"\"\n",
    "sentences, filtered_sentences = process_df(mini_df)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               luhn_algorithm summary\n",
      "0   The Daman and Diu administration on Wednesday ...\n",
      "1   A post shared by Malaika Arora Khan (@malaikaa...\n",
      "2   In its response, the management of the autonom...\n",
      "3   Human trafficking is the world's fastest growi...\n",
      "4   A native of Kasganj in UP, Kumar was unmarried...\n",
      "..                                                ...\n",
      "95  Facing pressure from President Donald Trump to...\n",
      "96  The Allahabad High Court today said it would h...\n",
      "97  Motorman Mahendra Prasad called up the railway...\n",
      "98  Following strict orders of the Karnataka Power...\n",
      "99  Rather than look at how greenhouse gases will ...\n",
      "\n",
      "[100 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Parameters are sentences,filtered_sentences ,summary_algorithm, size = 2(default value)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "lunh_algorithm_summary = summarize_with(list_of_articles= sentences, list_of_filtered_articles= filtered_sentences, summary_algorithm= luhn_algorithm, size=5) \n",
    "print(lunh_algorithm_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1_fmeasure</th>\n",
       "      <th>rouge1_precision</th>\n",
       "      <th>rouge1_recall</th>\n",
       "      <th>rouge2_fmeasure</th>\n",
       "      <th>rouge2_precision</th>\n",
       "      <th>rouge2_recall</th>\n",
       "      <th>rougeL_fmeasure</th>\n",
       "      <th>rougeL_precision</th>\n",
       "      <th>rougeL_recall</th>\n",
       "      <th>rougeLsum_fmeasure</th>\n",
       "      <th>rougeLsum_precision</th>\n",
       "      <th>rougeLsum_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tensor(0.4711)</td>\n",
       "      <td>tensor(0.3212)</td>\n",
       "      <td>tensor(0.8833)</td>\n",
       "      <td>tensor(0.2691)</td>\n",
       "      <td>tensor(0.1829)</td>\n",
       "      <td>tensor(0.5085)</td>\n",
       "      <td>tensor(0.2400)</td>\n",
       "      <td>tensor(0.1636)</td>\n",
       "      <td>tensor(0.4500)</td>\n",
       "      <td>tensor(0.2933)</td>\n",
       "      <td>tensor(0.2000)</td>\n",
       "      <td>tensor(0.5500)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tensor(0.1645)</td>\n",
       "      <td>tensor(0.1131)</td>\n",
       "      <td>tensor(0.3016)</td>\n",
       "      <td>tensor(0.0262)</td>\n",
       "      <td>tensor(0.0180)</td>\n",
       "      <td>tensor(0.0484)</td>\n",
       "      <td>tensor(0.1212)</td>\n",
       "      <td>tensor(0.0833)</td>\n",
       "      <td>tensor(0.2222)</td>\n",
       "      <td>tensor(0.1385)</td>\n",
       "      <td>tensor(0.0952)</td>\n",
       "      <td>tensor(0.2540)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tensor(0.3553)</td>\n",
       "      <td>tensor(0.2555)</td>\n",
       "      <td>tensor(0.5833)</td>\n",
       "      <td>tensor(0.2359)</td>\n",
       "      <td>tensor(0.1691)</td>\n",
       "      <td>tensor(0.3898)</td>\n",
       "      <td>tensor(0.2132)</td>\n",
       "      <td>tensor(0.1533)</td>\n",
       "      <td>tensor(0.3500)</td>\n",
       "      <td>tensor(0.2741)</td>\n",
       "      <td>tensor(0.1971)</td>\n",
       "      <td>tensor(0.4500)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tensor(0.1967)</td>\n",
       "      <td>tensor(0.1304)</td>\n",
       "      <td>tensor(0.4000)</td>\n",
       "      <td>tensor(0.0331)</td>\n",
       "      <td>tensor(0.0219)</td>\n",
       "      <td>tensor(0.0678)</td>\n",
       "      <td>tensor(0.1148)</td>\n",
       "      <td>tensor(0.0761)</td>\n",
       "      <td>tensor(0.2333)</td>\n",
       "      <td>tensor(0.1721)</td>\n",
       "      <td>tensor(0.1141)</td>\n",
       "      <td>tensor(0.3500)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tensor(0.4245)</td>\n",
       "      <td>tensor(0.2842)</td>\n",
       "      <td>tensor(0.8387)</td>\n",
       "      <td>tensor(0.1893)</td>\n",
       "      <td>tensor(0.1264)</td>\n",
       "      <td>tensor(0.3770)</td>\n",
       "      <td>tensor(0.2367)</td>\n",
       "      <td>tensor(0.1585)</td>\n",
       "      <td>tensor(0.4677)</td>\n",
       "      <td>tensor(0.3510)</td>\n",
       "      <td>tensor(0.2350)</td>\n",
       "      <td>tensor(0.6935)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>tensor(0.3566)</td>\n",
       "      <td>tensor(0.2277)</td>\n",
       "      <td>tensor(0.8214)</td>\n",
       "      <td>tensor(0.1875)</td>\n",
       "      <td>tensor(0.1194)</td>\n",
       "      <td>tensor(0.4364)</td>\n",
       "      <td>tensor(0.2791)</td>\n",
       "      <td>tensor(0.1782)</td>\n",
       "      <td>tensor(0.6429)</td>\n",
       "      <td>tensor(0.3023)</td>\n",
       "      <td>tensor(0.1931)</td>\n",
       "      <td>tensor(0.6964)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>tensor(0.6188)</td>\n",
       "      <td>tensor(0.4553)</td>\n",
       "      <td>tensor(0.9655)</td>\n",
       "      <td>tensor(0.5028)</td>\n",
       "      <td>tensor(0.3689)</td>\n",
       "      <td>tensor(0.7895)</td>\n",
       "      <td>tensor(0.5635)</td>\n",
       "      <td>tensor(0.4146)</td>\n",
       "      <td>tensor(0.8793)</td>\n",
       "      <td>tensor(0.5746)</td>\n",
       "      <td>tensor(0.4228)</td>\n",
       "      <td>tensor(0.8966)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>tensor(0.3069)</td>\n",
       "      <td>tensor(0.2246)</td>\n",
       "      <td>tensor(0.4844)</td>\n",
       "      <td>tensor(0.0800)</td>\n",
       "      <td>tensor(0.0584)</td>\n",
       "      <td>tensor(0.1270)</td>\n",
       "      <td>tensor(0.1782)</td>\n",
       "      <td>tensor(0.1304)</td>\n",
       "      <td>tensor(0.2812)</td>\n",
       "      <td>tensor(0.1881)</td>\n",
       "      <td>tensor(0.1377)</td>\n",
       "      <td>tensor(0.2969)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>tensor(0.3765)</td>\n",
       "      <td>tensor(0.2909)</td>\n",
       "      <td>tensor(0.5333)</td>\n",
       "      <td>tensor(0.1786)</td>\n",
       "      <td>tensor(0.1376)</td>\n",
       "      <td>tensor(0.2542)</td>\n",
       "      <td>tensor(0.2353)</td>\n",
       "      <td>tensor(0.1818)</td>\n",
       "      <td>tensor(0.3333)</td>\n",
       "      <td>tensor(0.2824)</td>\n",
       "      <td>tensor(0.2182)</td>\n",
       "      <td>tensor(0.4000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>tensor(0.2327)</td>\n",
       "      <td>tensor(0.1517)</td>\n",
       "      <td>tensor(0.5000)</td>\n",
       "      <td>tensor(0.0220)</td>\n",
       "      <td>tensor(0.0143)</td>\n",
       "      <td>tensor(0.0476)</td>\n",
       "      <td>tensor(0.0945)</td>\n",
       "      <td>tensor(0.0616)</td>\n",
       "      <td>tensor(0.2031)</td>\n",
       "      <td>tensor(0.1745)</td>\n",
       "      <td>tensor(0.1137)</td>\n",
       "      <td>tensor(0.3750)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   rouge1_fmeasure rouge1_precision   rouge1_recall rouge2_fmeasure  \\\n",
       "0   tensor(0.4711)   tensor(0.3212)  tensor(0.8833)  tensor(0.2691)   \n",
       "1   tensor(0.1645)   tensor(0.1131)  tensor(0.3016)  tensor(0.0262)   \n",
       "2   tensor(0.3553)   tensor(0.2555)  tensor(0.5833)  tensor(0.2359)   \n",
       "3   tensor(0.1967)   tensor(0.1304)  tensor(0.4000)  tensor(0.0331)   \n",
       "4   tensor(0.4245)   tensor(0.2842)  tensor(0.8387)  tensor(0.1893)   \n",
       "..             ...              ...             ...             ...   \n",
       "95  tensor(0.3566)   tensor(0.2277)  tensor(0.8214)  tensor(0.1875)   \n",
       "96  tensor(0.6188)   tensor(0.4553)  tensor(0.9655)  tensor(0.5028)   \n",
       "97  tensor(0.3069)   tensor(0.2246)  tensor(0.4844)  tensor(0.0800)   \n",
       "98  tensor(0.3765)   tensor(0.2909)  tensor(0.5333)  tensor(0.1786)   \n",
       "99  tensor(0.2327)   tensor(0.1517)  tensor(0.5000)  tensor(0.0220)   \n",
       "\n",
       "   rouge2_precision   rouge2_recall rougeL_fmeasure rougeL_precision  \\\n",
       "0    tensor(0.1829)  tensor(0.5085)  tensor(0.2400)   tensor(0.1636)   \n",
       "1    tensor(0.0180)  tensor(0.0484)  tensor(0.1212)   tensor(0.0833)   \n",
       "2    tensor(0.1691)  tensor(0.3898)  tensor(0.2132)   tensor(0.1533)   \n",
       "3    tensor(0.0219)  tensor(0.0678)  tensor(0.1148)   tensor(0.0761)   \n",
       "4    tensor(0.1264)  tensor(0.3770)  tensor(0.2367)   tensor(0.1585)   \n",
       "..              ...             ...             ...              ...   \n",
       "95   tensor(0.1194)  tensor(0.4364)  tensor(0.2791)   tensor(0.1782)   \n",
       "96   tensor(0.3689)  tensor(0.7895)  tensor(0.5635)   tensor(0.4146)   \n",
       "97   tensor(0.0584)  tensor(0.1270)  tensor(0.1782)   tensor(0.1304)   \n",
       "98   tensor(0.1376)  tensor(0.2542)  tensor(0.2353)   tensor(0.1818)   \n",
       "99   tensor(0.0143)  tensor(0.0476)  tensor(0.0945)   tensor(0.0616)   \n",
       "\n",
       "     rougeL_recall rougeLsum_fmeasure rougeLsum_precision rougeLsum_recall  \n",
       "0   tensor(0.4500)     tensor(0.2933)      tensor(0.2000)   tensor(0.5500)  \n",
       "1   tensor(0.2222)     tensor(0.1385)      tensor(0.0952)   tensor(0.2540)  \n",
       "2   tensor(0.3500)     tensor(0.2741)      tensor(0.1971)   tensor(0.4500)  \n",
       "3   tensor(0.2333)     tensor(0.1721)      tensor(0.1141)   tensor(0.3500)  \n",
       "4   tensor(0.4677)     tensor(0.3510)      tensor(0.2350)   tensor(0.6935)  \n",
       "..             ...                ...                 ...              ...  \n",
       "95  tensor(0.6429)     tensor(0.3023)      tensor(0.1931)   tensor(0.6964)  \n",
       "96  tensor(0.8793)     tensor(0.5746)      tensor(0.4228)   tensor(0.8966)  \n",
       "97  tensor(0.2812)     tensor(0.1881)      tensor(0.1377)   tensor(0.2969)  \n",
       "98  tensor(0.3333)     tensor(0.2824)      tensor(0.2182)   tensor(0.4000)  \n",
       "99  tensor(0.2031)     tensor(0.1745)      tensor(0.1137)   tensor(0.3750)  \n",
       "\n",
       "[100 rows x 12 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Parameters are dataframe of original summary and dataframe of system summary\n",
    "NOTE system summary is calculated in the cell above\n",
    "\"\"\"\n",
    "rouge_scores_df(df=mini_df, algorithm_summary_df=lunh_algorithm_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# horizontal_concat_df = pd.concat(\n",
    "#     [\n",
    "#         mini_df,\n",
    "#         luhn,\n",
    "#         lsa,\n",
    "#         text_matching,\n",
    "#         LexRank\n",
    "#     ],\n",
    "#     axis=1,\n",
    "# )\n",
    "# #horizontal_concat_df[horizontal_concat_df.isnull().any(axis=1)]\n",
    "# horizontal_concat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1_fmeasure': tensor(0.4711),\n",
       " 'rouge1_precision': tensor(0.3212),\n",
       " 'rouge1_recall': tensor(0.8833),\n",
       " 'rouge2_fmeasure': tensor(0.2691),\n",
       " 'rouge2_precision': tensor(0.1829),\n",
       " 'rouge2_recall': tensor(0.5085),\n",
       " 'rougeL_fmeasure': tensor(0.2400),\n",
       " 'rougeL_precision': tensor(0.1636),\n",
       " 'rougeL_recall': tensor(0.4500),\n",
       " 'rougeLsum_fmeasure': tensor(0.2933),\n",
       " 'rougeLsum_precision': tensor(0.2000),\n",
       " 'rougeLsum_recall': tensor(0.5500)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_efficiency(predicted_summary=lunh_algorithm_summary.iloc[0,0], original_summary=mini_df.iloc[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In its response, the management of the autonomous super-specialty health facility had clarified on Wednesday that it was in adherence to the central civil services rules followed by the All India Institute of Medical Sciences in New Delhi.IGIMS medical superintendent Dr Manish Mandal said institute director Dr NR Biswas held a meeting on Thursday morning before directing that the word ?virgin?Earlier, Bihar health minister Mangal Pandey had ended up redefining the very meaning of virginity in his attempts to justify the awkward phrasing of the question in the form.The Indira Gandhi Institute of Medical Sciences (IGIMS) in Patna amended its marital declaration form on Thursday, replacing the word ?Until now, new recruits to the super-specialty medical institute in the state capital were required to declare if they were bachelors, widowers or virgins.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lunh_algorithm_summary.iloc[2,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The Indira Gandhi Institute of Medical Sciences (IGIMS) in Patna on Thursday made corrections in its Marital Declaration Form by changing 'Virgin' option to 'Unmarried'. Earlier, Bihar Health Minister defined virgin as being an unmarried woman and did not consider the term objectionable. The institute, however, faced strong backlash for asking new recruits to declare their virginity in the form.\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_df.iloc[2,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not find the DLL(s) 'msvcp140_1.dll'. TensorFlow requires that these DLLs be installed in a directory that is named in your %PATH% environment variable. You may install these DLLs by downloading \"Microsoft C++ Redistributable for Visual Studio 2015, 2017 and 2019\" for your platform from this URL: https://support.microsoft.com/help/2977003/the-latest-supported-visual-c-downloads",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mNum GPUs Available: \u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mlen\u001b[39m(tf\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mlist_physical_devices(\u001b[39m'\u001b[39m\u001b[39mGPU\u001b[39m\u001b[39m'\u001b[39m)))\n",
      "File \u001b[1;32mc:\\Users\\zeyad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\__init__.py:37\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_sys\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_typing\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtools\u001b[39;00m \u001b[39mimport\u001b[39;00m module_util \u001b[39mas\u001b[39;00m _module_util\n\u001b[0;32m     38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlazy_loader\u001b[39;00m \u001b[39mimport\u001b[39;00m LazyLoader \u001b[39mas\u001b[39;00m _LazyLoader\n\u001b[0;32m     40\u001b[0m \u001b[39m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\zeyad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\__init__.py:36\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtraceback\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[39m# We aim to keep this file minimal and ideally remove completely.\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[39m# If you are adding a new file with @tf_export decorators,\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[39m# import it in modules_with_exports.py instead.\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \n\u001b[0;32m     33\u001b[0m \u001b[39m# go/tf-wildcard-import\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[39m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m pywrap_tensorflow \u001b[39mas\u001b[39;00m _pywrap_tensorflow\n\u001b[0;32m     37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39meager\u001b[39;00m \u001b[39mimport\u001b[39;00m context\n\u001b[0;32m     39\u001b[0m \u001b[39m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[39m# Bring in subpackages.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\zeyad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:26\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mplatform\u001b[39;00m \u001b[39mimport\u001b[39;00m self_check\n\u001b[0;32m     23\u001b[0m \u001b[39m# TODO(mdan): Cleanup antipattern: import for side effects.\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \n\u001b[0;32m     25\u001b[0m \u001b[39m# Perform pre-load sanity checks in order to produce a more actionable error.\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m self_check\u001b[39m.\u001b[39;49mpreload_check()\n\u001b[0;32m     28\u001b[0m \u001b[39m# pylint: disable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     31\u001b[0m   \u001b[39m# This import is expected to fail if there is an explicit shared object\u001b[39;00m\n\u001b[0;32m     32\u001b[0m   \u001b[39m# dependency (with_framework_lib=true), since we do not need RTLD_GLOBAL.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\zeyad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\platform\\self_check.py:50\u001b[0m, in \u001b[0;36mpreload_check\u001b[1;34m()\u001b[0m\n\u001b[0;32m     48\u001b[0m         missing\u001b[39m.\u001b[39mappend(dll_name)\n\u001b[0;32m     49\u001b[0m     \u001b[39mif\u001b[39;00m missing:\n\u001b[1;32m---> 50\u001b[0m       \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[0;32m     51\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mCould not find the DLL(s) \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m. TensorFlow requires that these DLLs \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     52\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mbe installed in a directory that is named in your \u001b[39m\u001b[39m%%\u001b[39;00m\u001b[39mPATH\u001b[39m\u001b[39m%%\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     53\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39menvironment variable. You may install these DLLs by downloading \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     54\u001b[0m           \u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMicrosoft C++ Redistributable for Visual Studio 2015, 2017 and \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     55\u001b[0m           \u001b[39m'\u001b[39m\u001b[39m2019\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m for your platform from this URL: \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     56\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mhttps://support.microsoft.com/help/2977003/the-latest-supported-visual-c-downloads\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     57\u001b[0m           \u001b[39m%\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m or \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(missing))\n\u001b[0;32m     58\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m   \u001b[39m# Load a library that performs CPU feature guard checking.  Doing this here\u001b[39;00m\n\u001b[0;32m     60\u001b[0m   \u001b[39m# as a preload check makes it more likely that we detect any CPU feature\u001b[39;00m\n\u001b[0;32m     61\u001b[0m   \u001b[39m# incompatibilities before we trigger them (which would typically result in\u001b[39;00m\n\u001b[0;32m     62\u001b[0m   \u001b[39m# SIGILL).\u001b[39;00m\n\u001b[0;32m     63\u001b[0m   \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mplatform\u001b[39;00m \u001b[39mimport\u001b[39;00m _pywrap_cpu_feature_guard\n",
      "\u001b[1;31mImportError\u001b[0m: Could not find the DLL(s) 'msvcp140_1.dll'. TensorFlow requires that these DLLs be installed in a directory that is named in your %PATH% environment variable. You may install these DLLs by downloading \"Microsoft C++ Redistributable for Visual Studio 2015, 2017 and 2019\" for your platform from this URL: https://support.microsoft.com/help/2977003/the-latest-supported-visual-c-downloads"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "afbd2e3b20535035cb123f2b315173d4c27faa26889583774d241154cf5dbaf6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
