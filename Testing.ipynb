{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imported_libraries import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Articles</th>\n",
       "      <th>Summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ad sales boost Time Warner profit\\r\\n\\r\\nQuart...</td>\n",
       "      <td>TimeWarner said fourth quarter sales rose 2% t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dollar gains on Greenspan speech\\r\\n\\r\\nThe do...</td>\n",
       "      <td>The dollar has hit its highest level against t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yukos unit buyer faces loan claim\\r\\n\\r\\nThe o...</td>\n",
       "      <td>Yukos' owner Menatep Group says it will ask Ro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>High fuel prices hit BA's profits\\r\\n\\r\\nBriti...</td>\n",
       "      <td>Rod Eddington, BA's chief executive, said the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pernod takeover talk lifts Domecq\\r\\n\\r\\nShare...</td>\n",
       "      <td>Pernod has reduced the debt it took on to fund...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Japan narrowly escapes recession\\r\\n\\r\\nJapan'...</td>\n",
       "      <td>On an annual basis, the data suggests annual g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Jobs growth still slow in the US\\r\\n\\r\\nThe US...</td>\n",
       "      <td>The job gains mean that President Bush can cel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>India calls for fair trade rules\\r\\n\\r\\nIndia,...</td>\n",
       "      <td>At a conference on developing enterprise hoste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ethiopia's crop production up 24%\\r\\n\\r\\nEthio...</td>\n",
       "      <td>In 2003, crop production totalled 11.49 millio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Court rejects $280bn tobacco case\\r\\n\\r\\nA US ...</td>\n",
       "      <td>A US government claim accusing the country's b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Articles  \\\n",
       "0  Ad sales boost Time Warner profit\\r\\n\\r\\nQuart...   \n",
       "1  Dollar gains on Greenspan speech\\r\\n\\r\\nThe do...   \n",
       "2  Yukos unit buyer faces loan claim\\r\\n\\r\\nThe o...   \n",
       "3  High fuel prices hit BA's profits\\r\\n\\r\\nBriti...   \n",
       "4  Pernod takeover talk lifts Domecq\\r\\n\\r\\nShare...   \n",
       "5  Japan narrowly escapes recession\\r\\n\\r\\nJapan'...   \n",
       "6  Jobs growth still slow in the US\\r\\n\\r\\nThe US...   \n",
       "7  India calls for fair trade rules\\r\\n\\r\\nIndia,...   \n",
       "8  Ethiopia's crop production up 24%\\r\\n\\r\\nEthio...   \n",
       "9  Court rejects $280bn tobacco case\\r\\n\\r\\nA US ...   \n",
       "\n",
       "                                           Summaries  \n",
       "0  TimeWarner said fourth quarter sales rose 2% t...  \n",
       "1  The dollar has hit its highest level against t...  \n",
       "2  Yukos' owner Menatep Group says it will ask Ro...  \n",
       "3  Rod Eddington, BA's chief executive, said the ...  \n",
       "4  Pernod has reduced the debt it took on to fund...  \n",
       "5  On an annual basis, the data suggests annual g...  \n",
       "6  The job gains mean that President Bush can cel...  \n",
       "7  At a conference on developing enterprise hoste...  \n",
       "8  In 2003, crop production totalled 11.49 millio...  \n",
       "9  A US government claim accusing the country's b...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"BBC_Dataset.csv\")\n",
    "df=df.iloc[:,1:]#remove first column(unnamed col)\n",
    "df.dropna(inplace=True)\n",
    "columns_titles = [\"Articles\",\"Summaries\"]\n",
    "df=df.reindex(columns=columns_titles)\n",
    "mini_df = df[:10]\n",
    "mini_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"dataset.csv\")\n",
    "# df=df.iloc[:,1:]#remove first column(unnamed col)\n",
    "# df.dropna(inplace=True)\n",
    "# columns_titles = [\"original\",\"summary\"]\n",
    "# df=df.reindex(columns=columns_titles)\n",
    "# mini_df = df[:10]\n",
    "# mini_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\"\"Preprocessing\"\"\"\u001b[39;00m                            \u001b[39m#(lemm,stopwords)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m sentences, filtered_sentences \u001b[39m=\u001b[39m process_df(mini_df,\u001b[39mTrue\u001b[39;49;00m,\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\basel\\OneDrive\\Desktop\\GP_Project\\text_summarization\\pretesting.py:73\u001b[0m, in \u001b[0;36mprocess_df\u001b[1;34m(df, lemmatization, remove_stopwords)\u001b[0m\n\u001b[0;32m     71\u001b[0m sentences,processed_sentences \u001b[39m=\u001b[39m[],[] \n\u001b[0;32m     72\u001b[0m \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(rows)):\n\u001b[1;32m---> 73\u001b[0m     articles_sentence , filtered_article \u001b[39m=\u001b[39m preprocessing_text_with_spacy(df\u001b[39m.\u001b[39;49miloc[row,\u001b[39m0\u001b[39;49m], lemmatization , remove_stopwords)\n\u001b[0;32m     74\u001b[0m     sentences\u001b[39m.\u001b[39mappend(articles_sentence)\n\u001b[0;32m     75\u001b[0m     processed_sentences\u001b[39m.\u001b[39mappend(filtered_article)\n",
      "File \u001b[1;32mc:\\Users\\basel\\OneDrive\\Desktop\\GP_Project\\text_summarization\\pretesting.py:31\u001b[0m, in \u001b[0;36mpreprocessing_text_with_spacy\u001b[1;34m(article, lemmatization, remove_stopwords)\u001b[0m\n\u001b[0;32m     29\u001b[0m         lemmatized_sentences \u001b[39m=\u001b[39m lemmatize_sentence(article_sentences , \u001b[39m'\u001b[39m\u001b[39men\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     30\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 31\u001b[0m         lemmatized_sentences \u001b[39m=\u001b[39m lemmatize_sentence(article_sentences, \u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     32\u001b[0m     \u001b[39mreturn\u001b[39;00m article_str_sentences, lemmatized_sentences\n\u001b[0;32m     34\u001b[0m \u001b[39mif\u001b[39;00m remove_stopwords:\n",
      "File \u001b[1;32mc:\\Users\\basel\\OneDrive\\Desktop\\GP_Project\\text_summarization\\pretesting.py:51\u001b[0m, in \u001b[0;36mlemmatize_sentence\u001b[1;34m(article_sentences, remove)\u001b[0m\n\u001b[0;32m     49\u001b[0m lemmatized_sentences \u001b[39m=\u001b[39m []\n\u001b[0;32m     50\u001b[0m \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m article_sentences:\n\u001b[1;32m---> 51\u001b[0m         tokens \u001b[39m=\u001b[39m tokenize(sentence\u001b[39m.\u001b[39mtext,stopwords\u001b[39m=\u001b[39mSTOPWORDS[remove],keep_numbers\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,keep_emails\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,keep_urls\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,)\n\u001b[0;32m     52\u001b[0m         cleaned_text \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(tokens)\n\u001b[0;32m     53\u001b[0m         spacy_tokens \u001b[39m=\u001b[39m nlp(cleaned_text)\n",
      "File \u001b[1;32mc:\\Users\\basel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyrsistent\\_pmap.py:169\u001b[0m, in \u001b[0;36mPMap.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key):\n\u001b[1;32m--> 169\u001b[0m     \u001b[39mreturn\u001b[39;00m PMap\u001b[39m.\u001b[39;49m_getitem(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_buckets, key)\n",
      "File \u001b[1;32mc:\\Users\\basel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyrsistent\\_pmap.py:166\u001b[0m, in \u001b[0;36mPMap._getitem\u001b[1;34m(buckets, key)\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[39mif\u001b[39;00m k \u001b[39m==\u001b[39m key:\n\u001b[0;32m    164\u001b[0m             \u001b[39mreturn\u001b[39;00m v\n\u001b[1;32m--> 166\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: ''"
     ]
    }
   ],
   "source": [
    "\"\"\"Preprocessing\"\"\"                            #(lemm,stopwords)\n",
    "sentences, filtered_sentences = process_df(mini_df,True,False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ad sales boost Time Warner profit\\r\\n\\r\\nQuarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (Â£600m) for the three months to December, from $639m year-earlier.\\r\\n\\r\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ad sale boost time warner profit quarterly profit medium giant timewarner jump 76 1 13bn â600 m month december 639 m year - early'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_sentences[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 10034.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              luhn_algorithm summary\n",
      "0  The company said it was unable to estimate the...\n",
      "1  On Friday, Federal Reserve chairman Mr Greensp...\n",
      "2  Menatep Group's managing director Tim Osborne ...\n",
      "3  Yet aviation analyst Mike Powell of Dresdner K...\n",
      "4  Allied Domecq's big names include Malibu rum, ...\n",
      "5  \"I maintain the view that Japan's economy rema...\n",
      "6  \"It suggests that employment is continuing to ...\n",
      "7  Palaniappan Chidambaram, India's finance minis...\n",
      "8  \"Local purchase of cereals for food assistance...\n",
      "9  In its case, the government claimed tobacco fi...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Parameters are sentences,filtered_sentences ,summary_algorithm, size = 2(default value)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "lunh_algorithm_summary = summarize_with(list_of_articles= sentences, list_of_filtered_articles= filtered_sentences, summary_algorithm= luhn_algorithm, size=5) \n",
    "print(lunh_algorithm_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:13<00:00,  1.30s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1_fmeasure</th>\n",
       "      <th>rouge1_precision</th>\n",
       "      <th>rouge1_recall</th>\n",
       "      <th>rouge2_fmeasure</th>\n",
       "      <th>rouge2_precision</th>\n",
       "      <th>rouge2_recall</th>\n",
       "      <th>rougeL_fmeasure</th>\n",
       "      <th>rougeL_precision</th>\n",
       "      <th>rougeL_recall</th>\n",
       "      <th>rougeLsum_fmeasure</th>\n",
       "      <th>rougeLsum_precision</th>\n",
       "      <th>rougeLsum_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tensor(0.4230)</td>\n",
       "      <td>tensor(0.3933)</td>\n",
       "      <td>tensor(0.4575)</td>\n",
       "      <td>tensor(0.1702)</td>\n",
       "      <td>tensor(0.1582)</td>\n",
       "      <td>tensor(0.1842)</td>\n",
       "      <td>tensor(0.1934)</td>\n",
       "      <td>tensor(0.1798)</td>\n",
       "      <td>tensor(0.2092)</td>\n",
       "      <td>tensor(0.2417)</td>\n",
       "      <td>tensor(0.2247)</td>\n",
       "      <td>tensor(0.2614)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tensor(0.7123)</td>\n",
       "      <td>tensor(0.6906)</td>\n",
       "      <td>tensor(0.7353)</td>\n",
       "      <td>tensor(0.6017)</td>\n",
       "      <td>tensor(0.5833)</td>\n",
       "      <td>tensor(0.6213)</td>\n",
       "      <td>tensor(0.4160)</td>\n",
       "      <td>tensor(0.4033)</td>\n",
       "      <td>tensor(0.4294)</td>\n",
       "      <td>tensor(0.6838)</td>\n",
       "      <td>tensor(0.6630)</td>\n",
       "      <td>tensor(0.7059)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tensor(0.6058)</td>\n",
       "      <td>tensor(0.5764)</td>\n",
       "      <td>tensor(0.6385)</td>\n",
       "      <td>tensor(0.4706)</td>\n",
       "      <td>tensor(0.4476)</td>\n",
       "      <td>tensor(0.4961)</td>\n",
       "      <td>tensor(0.3212)</td>\n",
       "      <td>tensor(0.3056)</td>\n",
       "      <td>tensor(0.3385)</td>\n",
       "      <td>tensor(0.5547)</td>\n",
       "      <td>tensor(0.5278)</td>\n",
       "      <td>tensor(0.5846)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tensor(0.7654)</td>\n",
       "      <td>tensor(0.7828)</td>\n",
       "      <td>tensor(0.7488)</td>\n",
       "      <td>tensor(0.6600)</td>\n",
       "      <td>tensor(0.6751)</td>\n",
       "      <td>tensor(0.6456)</td>\n",
       "      <td>tensor(0.3901)</td>\n",
       "      <td>tensor(0.3990)</td>\n",
       "      <td>tensor(0.3816)</td>\n",
       "      <td>tensor(0.7457)</td>\n",
       "      <td>tensor(0.7626)</td>\n",
       "      <td>tensor(0.7295)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tensor(0.6966)</td>\n",
       "      <td>tensor(0.6200)</td>\n",
       "      <td>tensor(0.7949)</td>\n",
       "      <td>tensor(0.6566)</td>\n",
       "      <td>tensor(0.5839)</td>\n",
       "      <td>tensor(0.7500)</td>\n",
       "      <td>tensor(0.4869)</td>\n",
       "      <td>tensor(0.4333)</td>\n",
       "      <td>tensor(0.5556)</td>\n",
       "      <td>tensor(0.6592)</td>\n",
       "      <td>tensor(0.5867)</td>\n",
       "      <td>tensor(0.7521)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tensor(0.4082)</td>\n",
       "      <td>tensor(0.3279)</td>\n",
       "      <td>tensor(0.5405)</td>\n",
       "      <td>tensor(0.2268)</td>\n",
       "      <td>tensor(0.1818)</td>\n",
       "      <td>tensor(0.3014)</td>\n",
       "      <td>tensor(0.2245)</td>\n",
       "      <td>tensor(0.1803)</td>\n",
       "      <td>tensor(0.2973)</td>\n",
       "      <td>tensor(0.3367)</td>\n",
       "      <td>tensor(0.2705)</td>\n",
       "      <td>tensor(0.4459)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tensor(0.6792)</td>\n",
       "      <td>tensor(0.7258)</td>\n",
       "      <td>tensor(0.6383)</td>\n",
       "      <td>tensor(0.5627)</td>\n",
       "      <td>tensor(0.6016)</td>\n",
       "      <td>tensor(0.5286)</td>\n",
       "      <td>tensor(0.3245)</td>\n",
       "      <td>tensor(0.3468)</td>\n",
       "      <td>tensor(0.3050)</td>\n",
       "      <td>tensor(0.6415)</td>\n",
       "      <td>tensor(0.6855)</td>\n",
       "      <td>tensor(0.6028)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tensor(0.7121)</td>\n",
       "      <td>tensor(0.7372)</td>\n",
       "      <td>tensor(0.6886)</td>\n",
       "      <td>tensor(0.5919)</td>\n",
       "      <td>tensor(0.6129)</td>\n",
       "      <td>tensor(0.5723)</td>\n",
       "      <td>tensor(0.4582)</td>\n",
       "      <td>tensor(0.4744)</td>\n",
       "      <td>tensor(0.4431)</td>\n",
       "      <td>tensor(0.4768)</td>\n",
       "      <td>tensor(0.4936)</td>\n",
       "      <td>tensor(0.4611)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tensor(0.6154)</td>\n",
       "      <td>tensor(0.5315)</td>\n",
       "      <td>tensor(0.7308)</td>\n",
       "      <td>tensor(0.4816)</td>\n",
       "      <td>tensor(0.4155)</td>\n",
       "      <td>tensor(0.5728)</td>\n",
       "      <td>tensor(0.3158)</td>\n",
       "      <td>tensor(0.2727)</td>\n",
       "      <td>tensor(0.3750)</td>\n",
       "      <td>tensor(0.5344)</td>\n",
       "      <td>tensor(0.4615)</td>\n",
       "      <td>tensor(0.6346)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tensor(0.6320)</td>\n",
       "      <td>tensor(0.5368)</td>\n",
       "      <td>tensor(0.7684)</td>\n",
       "      <td>tensor(0.5066)</td>\n",
       "      <td>tensor(0.4296)</td>\n",
       "      <td>tensor(0.6170)</td>\n",
       "      <td>tensor(0.3377)</td>\n",
       "      <td>tensor(0.2868)</td>\n",
       "      <td>tensor(0.4105)</td>\n",
       "      <td>tensor(0.5628)</td>\n",
       "      <td>tensor(0.4779)</td>\n",
       "      <td>tensor(0.6842)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  rouge1_fmeasure rouge1_precision   rouge1_recall rouge2_fmeasure  \\\n",
       "0  tensor(0.4230)   tensor(0.3933)  tensor(0.4575)  tensor(0.1702)   \n",
       "1  tensor(0.7123)   tensor(0.6906)  tensor(0.7353)  tensor(0.6017)   \n",
       "2  tensor(0.6058)   tensor(0.5764)  tensor(0.6385)  tensor(0.4706)   \n",
       "3  tensor(0.7654)   tensor(0.7828)  tensor(0.7488)  tensor(0.6600)   \n",
       "4  tensor(0.6966)   tensor(0.6200)  tensor(0.7949)  tensor(0.6566)   \n",
       "5  tensor(0.4082)   tensor(0.3279)  tensor(0.5405)  tensor(0.2268)   \n",
       "6  tensor(0.6792)   tensor(0.7258)  tensor(0.6383)  tensor(0.5627)   \n",
       "7  tensor(0.7121)   tensor(0.7372)  tensor(0.6886)  tensor(0.5919)   \n",
       "8  tensor(0.6154)   tensor(0.5315)  tensor(0.7308)  tensor(0.4816)   \n",
       "9  tensor(0.6320)   tensor(0.5368)  tensor(0.7684)  tensor(0.5066)   \n",
       "\n",
       "  rouge2_precision   rouge2_recall rougeL_fmeasure rougeL_precision  \\\n",
       "0   tensor(0.1582)  tensor(0.1842)  tensor(0.1934)   tensor(0.1798)   \n",
       "1   tensor(0.5833)  tensor(0.6213)  tensor(0.4160)   tensor(0.4033)   \n",
       "2   tensor(0.4476)  tensor(0.4961)  tensor(0.3212)   tensor(0.3056)   \n",
       "3   tensor(0.6751)  tensor(0.6456)  tensor(0.3901)   tensor(0.3990)   \n",
       "4   tensor(0.5839)  tensor(0.7500)  tensor(0.4869)   tensor(0.4333)   \n",
       "5   tensor(0.1818)  tensor(0.3014)  tensor(0.2245)   tensor(0.1803)   \n",
       "6   tensor(0.6016)  tensor(0.5286)  tensor(0.3245)   tensor(0.3468)   \n",
       "7   tensor(0.6129)  tensor(0.5723)  tensor(0.4582)   tensor(0.4744)   \n",
       "8   tensor(0.4155)  tensor(0.5728)  tensor(0.3158)   tensor(0.2727)   \n",
       "9   tensor(0.4296)  tensor(0.6170)  tensor(0.3377)   tensor(0.2868)   \n",
       "\n",
       "    rougeL_recall rougeLsum_fmeasure rougeLsum_precision rougeLsum_recall  \n",
       "0  tensor(0.2092)     tensor(0.2417)      tensor(0.2247)   tensor(0.2614)  \n",
       "1  tensor(0.4294)     tensor(0.6838)      tensor(0.6630)   tensor(0.7059)  \n",
       "2  tensor(0.3385)     tensor(0.5547)      tensor(0.5278)   tensor(0.5846)  \n",
       "3  tensor(0.3816)     tensor(0.7457)      tensor(0.7626)   tensor(0.7295)  \n",
       "4  tensor(0.5556)     tensor(0.6592)      tensor(0.5867)   tensor(0.7521)  \n",
       "5  tensor(0.2973)     tensor(0.3367)      tensor(0.2705)   tensor(0.4459)  \n",
       "6  tensor(0.3050)     tensor(0.6415)      tensor(0.6855)   tensor(0.6028)  \n",
       "7  tensor(0.4431)     tensor(0.4768)      tensor(0.4936)   tensor(0.4611)  \n",
       "8  tensor(0.3750)     tensor(0.5344)      tensor(0.4615)   tensor(0.6346)  \n",
       "9  tensor(0.4105)     tensor(0.5628)      tensor(0.4779)   tensor(0.6842)  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Parameters are dataframe of original summary and dataframe of system summary\n",
    "NOTE system summary is calculated in the cell above\n",
    "\"\"\"\n",
    "rouge_scores_df(df=mini_df, algorithm_summary_df=lunh_algorithm_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# horizontal_concat_df = pd.concat(\n",
    "#     [\n",
    "#         mini_df,\n",
    "#         luhn,\n",
    "#         lsa,\n",
    "#         text_matching,\n",
    "#         LexRank\n",
    "#     ],\n",
    "#     axis=1,\n",
    "# )\n",
    "# #horizontal_concat_df[horizontal_concat_df.isnull().any(axis=1)]\n",
    "# horizontal_concat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "480c14cd519d4cac626ed4e2ff26eb76a48d8461070c1f960eb23b2ab109b24d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
