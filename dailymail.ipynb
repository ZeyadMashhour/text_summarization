{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cnn_dailymail (C:/Users/basel/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0d375a5aaf143dfb148dd62d135fb73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the CNN/Daily Mail dataset\n",
    "dataset = load_dataset('cnn_dailymail', '3.0.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cnn_dailymail (C:/Users/basel/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d02b2a88d481491aada475fea11bc43f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CNN/Daily Mail dataset\n",
    "dataset = load_dataset('cnn_dailymail', '3.0.0')\n",
    "\n",
    "# Create a DataFrame with Original Articles and Original Summary\n",
    "cnn_df = pd.DataFrame({\n",
    "    'Original Articles': dataset['train']['article'],\n",
    "    'Original Summary': dataset['train']['highlights']\n",
    "})\n",
    "cnn_mini_df = cnn_df[:1500]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mentally ill inmates in Miami are housed on the \"forgotten floor\"\\nJudge Steven Leifman says most are there as a result of \"avoidable felonies\"\\nWhile CNN tours facility, patient shouts: \"I am the son of the president\"\\nLeifman says the system is unjust and he\\'s fighting for change .'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_mini_df['Original Summary'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summarization_algorithm import *\n",
    "from preprocessing_algorithms import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [36:28<00:00,  1.46s/it]\n"
     ]
    }
   ],
   "source": [
    "sentences , filtered_sentences = process_one_column_df(cnn_mini_df['Original Articles'],True,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      Original Articles   \n",
      "0     LONDON, England (Reuters) -- Harry Potter star...  \\\n",
      "1     Editor's note: In our Behind the Scenes series...   \n",
      "2     MINNEAPOLIS, Minnesota (CNN) -- Drivers who we...   \n",
      "3     WASHINGTON (CNN) -- Doctors removed five small...   \n",
      "4     (CNN)  -- The National Football League has ind...   \n",
      "...                                                 ...   \n",
      "1495  WASHINGTON (CNN) -- A court Thursday rejected ...   \n",
      "1496  ORLANDO, Florida (CNN) -- The meter reader who...   \n",
      "1497  (CNN) -- Former first lady Barbara Bush was mo...   \n",
      "1498  DENVER, Colorado (CNN)  -- Investigators will ...   \n",
      "1499  WASHINGTON (CNN) -- Iraq's smaller religious g...   \n",
      "\n",
      "                                       Original Summary  \n",
      "0     Harry Potter star Daniel Radcliffe gets £20M f...  \n",
      "1     Mentally ill inmates in Miami are housed on th...  \n",
      "2     NEW: \"I thought I was going to die,\" driver sa...  \n",
      "3     Five small polyps found during procedure; \"non...  \n",
      "4     NEW: NFL chief, Atlanta Falcons owner critical...  \n",
      "...                                                 ...  \n",
      "1495  Appeals court rejects appeal of failed $54 mil...  \n",
      "1496  Officials say worker who found child's remains...  \n",
      "1497  Barbara Bush had surgery to repair and seal a ...  \n",
      "1498  Flight data recorder, cockpit voice recorder r...  \n",
      "1499  Religious freedom watchdog group urges protect...  \n",
      "\n",
      "[1500 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Save the mini DataFrame to a text file in JSON format\n",
    "with open('cnn_mini_df.txt', 'w') as f:\n",
    "    f.write(cnn_mini_df.to_json())\n",
    "\n",
    "# Load the mini DataFrame from the text file in JSON format\n",
    "with open('cnn_mini_df.txt', 'r') as f:\n",
    "    cnn_mini_df_json = f.read()\n",
    "\n",
    "cnn_mini_df_loaded = pd.read_json(cnn_mini_df_json)\n",
    "\n",
    "# Print the mini DataFrame\n",
    "print(cnn_mini_df_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_with(list_of_filtered_articles, list_of_articles, summary_algorithm, size=0.2):\n",
    "    rows = len(list_of_articles)\n",
    "    summarized_text = []\n",
    "    for row in tqdm(range(rows)):\n",
    "        sentences = list_of_articles[row]\n",
    "        filtered_sentences = list_of_filtered_articles[row]\n",
    "        summary_size = int(size * len(sentences)) if 0 < size < 1 else int(size)\n",
    "        summary = summary_algorithm(filtered_sentences, sentences, size=summary_size)\n",
    "        summarized_text.append(summary)\n",
    "    summary_df = pd.DataFrame(summarized_text, columns=[f'{summary_algorithm.__name__} summary'])\n",
    "    return summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [00:00<00:00, 4832.82it/s]\n"
     ]
    }
   ],
   "source": [
    "luhn_summary = summarize_with(filtered_sentences,sentences,luhn_algorithm,0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>luhn_algorithm summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LONDON, England (Reuters) -- Harry Potter star...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mentally ill people often won't do what they'r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MINNEAPOLIS, Minnesota (CNN) -- Drivers who we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>He planned to have lunch at Camp David and hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Vick said he would plead guilty to one count o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>The District of Columbia Court of Appeals \"rul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>Sheriff's spokesman Carlos Padilla said last w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>(CNN) -- Former first lady Barbara Bush was mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>All 115 people aboard the Continental Airlines...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>\"Iraq's non-Muslim religious minorities -- par...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 luhn_algorithm summary\n",
       "0     LONDON, England (Reuters) -- Harry Potter star...\n",
       "1     Mentally ill people often won't do what they'r...\n",
       "2     MINNEAPOLIS, Minnesota (CNN) -- Drivers who we...\n",
       "3     He planned to have lunch at Camp David and hav...\n",
       "4     Vick said he would plead guilty to one count o...\n",
       "...                                                 ...\n",
       "1495  The District of Columbia Court of Appeals \"rul...\n",
       "1496  Sheriff's spokesman Carlos Padilla said last w...\n",
       "1497  (CNN) -- Former first lady Barbara Bush was mo...\n",
       "1498  All 115 people aboard the Continental Airlines...\n",
       "1499  \"Iraq's non-Muslim religious minorities -- par...\n",
       "\n",
       "[1500 rows x 1 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "luhn_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficiency_scores import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [38:10<00:00,  1.53s/it]\n"
     ]
    }
   ],
   "source": [
    "luhn_scores = rouge_scores_df(cnn_mini_df,luhn_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1_fmeasure</th>\n",
       "      <th>rouge1_precision</th>\n",
       "      <th>rouge1_recall</th>\n",
       "      <th>rouge2_fmeasure</th>\n",
       "      <th>rouge2_precision</th>\n",
       "      <th>rouge2_recall</th>\n",
       "      <th>rougeL_fmeasure</th>\n",
       "      <th>rougeL_precision</th>\n",
       "      <th>rougeL_recall</th>\n",
       "      <th>rougeLsum_fmeasure</th>\n",
       "      <th>rougeLsum_precision</th>\n",
       "      <th>rougeLsum_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tensor(0.2500)</td>\n",
       "      <td>tensor(0.1525)</td>\n",
       "      <td>tensor(0.6923)</td>\n",
       "      <td>tensor(0.1776)</td>\n",
       "      <td>tensor(0.1080)</td>\n",
       "      <td>tensor(0.5000)</td>\n",
       "      <td>tensor(0.2315)</td>\n",
       "      <td>tensor(0.1412)</td>\n",
       "      <td>tensor(0.6410)</td>\n",
       "      <td>tensor(0.2407)</td>\n",
       "      <td>tensor(0.1469)</td>\n",
       "      <td>tensor(0.6667)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tensor(0.2207)</td>\n",
       "      <td>tensor(0.1320)</td>\n",
       "      <td>tensor(0.6735)</td>\n",
       "      <td>tensor(0.0741)</td>\n",
       "      <td>tensor(0.0442)</td>\n",
       "      <td>tensor(0.2292)</td>\n",
       "      <td>tensor(0.1338)</td>\n",
       "      <td>tensor(0.0800)</td>\n",
       "      <td>tensor(0.4082)</td>\n",
       "      <td>tensor(0.1338)</td>\n",
       "      <td>tensor(0.0800)</td>\n",
       "      <td>tensor(0.4082)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tensor(0.1164)</td>\n",
       "      <td>tensor(0.0684)</td>\n",
       "      <td>tensor(0.3902)</td>\n",
       "      <td>tensor(0.0220)</td>\n",
       "      <td>tensor(0.0129)</td>\n",
       "      <td>tensor(0.0750)</td>\n",
       "      <td>tensor(0.0655)</td>\n",
       "      <td>tensor(0.0385)</td>\n",
       "      <td>tensor(0.2195)</td>\n",
       "      <td>tensor(0.1091)</td>\n",
       "      <td>tensor(0.0641)</td>\n",
       "      <td>tensor(0.3659)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tensor(0.1266)</td>\n",
       "      <td>tensor(0.0746)</td>\n",
       "      <td>tensor(0.4167)</td>\n",
       "      <td>tensor(0.0256)</td>\n",
       "      <td>tensor(0.0150)</td>\n",
       "      <td>tensor(0.0870)</td>\n",
       "      <td>tensor(0.0506)</td>\n",
       "      <td>tensor(0.0299)</td>\n",
       "      <td>tensor(0.1667)</td>\n",
       "      <td>tensor(0.0886)</td>\n",
       "      <td>tensor(0.0522)</td>\n",
       "      <td>tensor(0.2917)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tensor(0.1183)</td>\n",
       "      <td>tensor(0.0669)</td>\n",
       "      <td>tensor(0.5122)</td>\n",
       "      <td>tensor(0.0397)</td>\n",
       "      <td>tensor(0.0224)</td>\n",
       "      <td>tensor(0.1750)</td>\n",
       "      <td>tensor(0.0676)</td>\n",
       "      <td>tensor(0.0382)</td>\n",
       "      <td>tensor(0.2927)</td>\n",
       "      <td>tensor(0.1070)</td>\n",
       "      <td>tensor(0.0605)</td>\n",
       "      <td>tensor(0.4634)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>tensor(0.1294)</td>\n",
       "      <td>tensor(0.0774)</td>\n",
       "      <td>tensor(0.3939)</td>\n",
       "      <td>tensor(0.0101)</td>\n",
       "      <td>tensor(0.0060)</td>\n",
       "      <td>tensor(0.0312)</td>\n",
       "      <td>tensor(0.0697)</td>\n",
       "      <td>tensor(0.0417)</td>\n",
       "      <td>tensor(0.2121)</td>\n",
       "      <td>tensor(0.0796)</td>\n",
       "      <td>tensor(0.0476)</td>\n",
       "      <td>tensor(0.2424)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>tensor(0.2927)</td>\n",
       "      <td>tensor(0.1818)</td>\n",
       "      <td>tensor(0.7500)</td>\n",
       "      <td>tensor(0.1148)</td>\n",
       "      <td>tensor(0.0711)</td>\n",
       "      <td>tensor(0.2979)</td>\n",
       "      <td>tensor(0.1626)</td>\n",
       "      <td>tensor(0.1010)</td>\n",
       "      <td>tensor(0.4167)</td>\n",
       "      <td>tensor(0.2520)</td>\n",
       "      <td>tensor(0.1566)</td>\n",
       "      <td>tensor(0.6458)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>tensor(0.3019)</td>\n",
       "      <td>tensor(0.2462)</td>\n",
       "      <td>tensor(0.3902)</td>\n",
       "      <td>tensor(0.1923)</td>\n",
       "      <td>tensor(0.1562)</td>\n",
       "      <td>tensor(0.2500)</td>\n",
       "      <td>tensor(0.2830)</td>\n",
       "      <td>tensor(0.2308)</td>\n",
       "      <td>tensor(0.3659)</td>\n",
       "      <td>tensor(0.2830)</td>\n",
       "      <td>tensor(0.2308)</td>\n",
       "      <td>tensor(0.3659)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>tensor(0.1799)</td>\n",
       "      <td>tensor(0.1037)</td>\n",
       "      <td>tensor(0.6757)</td>\n",
       "      <td>tensor(0.0507)</td>\n",
       "      <td>tensor(0.0292)</td>\n",
       "      <td>tensor(0.1944)</td>\n",
       "      <td>tensor(0.1079)</td>\n",
       "      <td>tensor(0.0622)</td>\n",
       "      <td>tensor(0.4054)</td>\n",
       "      <td>tensor(0.1655)</td>\n",
       "      <td>tensor(0.0954)</td>\n",
       "      <td>tensor(0.6216)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>tensor(0.1208)</td>\n",
       "      <td>tensor(0.0865)</td>\n",
       "      <td>tensor(0.2000)</td>\n",
       "      <td>tensor(0.0272)</td>\n",
       "      <td>tensor(0.0194)</td>\n",
       "      <td>tensor(0.0455)</td>\n",
       "      <td>tensor(0.0940)</td>\n",
       "      <td>tensor(0.0673)</td>\n",
       "      <td>tensor(0.1556)</td>\n",
       "      <td>tensor(0.1074)</td>\n",
       "      <td>tensor(0.0769)</td>\n",
       "      <td>tensor(0.1778)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     rouge1_fmeasure rouge1_precision   rouge1_recall rouge2_fmeasure   \n",
       "0     tensor(0.2500)   tensor(0.1525)  tensor(0.6923)  tensor(0.1776)  \\\n",
       "1     tensor(0.2207)   tensor(0.1320)  tensor(0.6735)  tensor(0.0741)   \n",
       "2     tensor(0.1164)   tensor(0.0684)  tensor(0.3902)  tensor(0.0220)   \n",
       "3     tensor(0.1266)   tensor(0.0746)  tensor(0.4167)  tensor(0.0256)   \n",
       "4     tensor(0.1183)   tensor(0.0669)  tensor(0.5122)  tensor(0.0397)   \n",
       "...              ...              ...             ...             ...   \n",
       "1495  tensor(0.1294)   tensor(0.0774)  tensor(0.3939)  tensor(0.0101)   \n",
       "1496  tensor(0.2927)   tensor(0.1818)  tensor(0.7500)  tensor(0.1148)   \n",
       "1497  tensor(0.3019)   tensor(0.2462)  tensor(0.3902)  tensor(0.1923)   \n",
       "1498  tensor(0.1799)   tensor(0.1037)  tensor(0.6757)  tensor(0.0507)   \n",
       "1499  tensor(0.1208)   tensor(0.0865)  tensor(0.2000)  tensor(0.0272)   \n",
       "\n",
       "     rouge2_precision   rouge2_recall rougeL_fmeasure rougeL_precision   \n",
       "0      tensor(0.1080)  tensor(0.5000)  tensor(0.2315)   tensor(0.1412)  \\\n",
       "1      tensor(0.0442)  tensor(0.2292)  tensor(0.1338)   tensor(0.0800)   \n",
       "2      tensor(0.0129)  tensor(0.0750)  tensor(0.0655)   tensor(0.0385)   \n",
       "3      tensor(0.0150)  tensor(0.0870)  tensor(0.0506)   tensor(0.0299)   \n",
       "4      tensor(0.0224)  tensor(0.1750)  tensor(0.0676)   tensor(0.0382)   \n",
       "...               ...             ...             ...              ...   \n",
       "1495   tensor(0.0060)  tensor(0.0312)  tensor(0.0697)   tensor(0.0417)   \n",
       "1496   tensor(0.0711)  tensor(0.2979)  tensor(0.1626)   tensor(0.1010)   \n",
       "1497   tensor(0.1562)  tensor(0.2500)  tensor(0.2830)   tensor(0.2308)   \n",
       "1498   tensor(0.0292)  tensor(0.1944)  tensor(0.1079)   tensor(0.0622)   \n",
       "1499   tensor(0.0194)  tensor(0.0455)  tensor(0.0940)   tensor(0.0673)   \n",
       "\n",
       "       rougeL_recall rougeLsum_fmeasure rougeLsum_precision rougeLsum_recall  \n",
       "0     tensor(0.6410)     tensor(0.2407)      tensor(0.1469)   tensor(0.6667)  \n",
       "1     tensor(0.4082)     tensor(0.1338)      tensor(0.0800)   tensor(0.4082)  \n",
       "2     tensor(0.2195)     tensor(0.1091)      tensor(0.0641)   tensor(0.3659)  \n",
       "3     tensor(0.1667)     tensor(0.0886)      tensor(0.0522)   tensor(0.2917)  \n",
       "4     tensor(0.2927)     tensor(0.1070)      tensor(0.0605)   tensor(0.4634)  \n",
       "...              ...                ...                 ...              ...  \n",
       "1495  tensor(0.2121)     tensor(0.0796)      tensor(0.0476)   tensor(0.2424)  \n",
       "1496  tensor(0.4167)     tensor(0.2520)      tensor(0.1566)   tensor(0.6458)  \n",
       "1497  tensor(0.3659)     tensor(0.2830)      tensor(0.2308)   tensor(0.3659)  \n",
       "1498  tensor(0.4054)     tensor(0.1655)      tensor(0.0954)   tensor(0.6216)  \n",
       "1499  tensor(0.1556)     tensor(0.1074)      tensor(0.0769)   tensor(0.1778)  \n",
       "\n",
       "[1500 rows x 12 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "luhn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "luhn_avg = df_avg_by_column(luhn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rouge1_fmeasure        tensor(0.2153)\n",
       "rouge1_precision       tensor(0.1442)\n",
       "rouge1_recall          tensor(0.5482)\n",
       "rouge2_fmeasure        tensor(0.0767)\n",
       "rouge2_precision       tensor(0.0519)\n",
       "rouge2_recall          tensor(0.1905)\n",
       "rougeL_fmeasure        tensor(0.1361)\n",
       "rougeL_precision       tensor(0.0916)\n",
       "rougeL_recall          tensor(0.3441)\n",
       "rougeLsum_fmeasure     tensor(0.1845)\n",
       "rougeLsum_precision    tensor(0.1235)\n",
       "rougeLsum_recall       tensor(0.4713)\n",
       "dtype: object"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "luhn_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "luhn_scores.to_csv('cnn_luhn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Original Articles', 'Original Summary'], dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_mini_df_loaded.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average length of the original summaries over the entire dataset is 5.06 sentences.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Calculate the number of summary sentences for each article\n",
    "num_summary_sentences = [len(article.split('.')) for article in cnn_mini_df['Original Summary']]\n",
    "\n",
    "# Calculate the average number of summary sentences per article\n",
    "avg_num_summary_sentences = sum(num_summary_sentences) / len(num_summary_sentences)\n",
    "\n",
    "# Print the result\n",
    "print(f\"The average length of the original summaries over the entire dataset is {avg_num_summary_sentences:.2f} sentences.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum summary length: 13\n",
      "Minimum summary length: 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get the number of summary sentences for each article\n",
    "\n",
    "# Calculate the number of summary sentences for each article\n",
    "summary_lengths = [len(article.split('.')) for article in cnn_mini_df['Original Summary']]\n",
    "max_summary_length = max(summary_lengths)\n",
    "min_summary_length = min(summary_lengths)\n",
    "\n",
    "# Get the maximum and minimum length of summary sentences in the dataset\n",
    "max_summary_length = max(summary_lengths)\n",
    "min_summary_length = min(summary_lengths)\n",
    "\n",
    "# Print the results\n",
    "print(\"Maximum summary length:\", max_summary_length)\n",
    "print(\"Minimum summary length:\", min_summary_length)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
