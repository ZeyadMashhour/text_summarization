{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2358b1a8",
   "metadata": {},
   "source": [
    "<h1> imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "946228a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_name = \"summary Input2.txt\"\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c154778c-9df1-4595-98b1-f7e5989b094a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "from lexrank.utils.text import tokenize\n",
    "from lexrank.mappings.stopwords import STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e321a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def timeNow(text):\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "    print(text, \"Current Time =\", current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd40aa51",
   "metadata": {},
   "source": [
    "<h1> Pre Processing </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48ac4705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    filtered_sentences = remove_stop_words(sentences)\n",
    "    return filtered_sentences,sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fee1d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(sentences):\n",
    "    # define a set of stop words \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_sentences = []\n",
    "    for sentence in sentences:\n",
    "        # tokenize each sentence into words\n",
    "        word_tokens = word_tokenize(sentence)\n",
    "        # remove all stop words from each sentence\n",
    "        filtered_words = [word for word in word_tokens if not word in stop_words]\n",
    "        # join all filtered words back into a single sentence\n",
    "        filtered_sentence = ' '.join(filtered_words)\n",
    "        filtered_sentences.append(filtered_sentence)\n",
    "    return filtered_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dc771a",
   "metadata": {},
   "source": [
    "<h1> Text Matching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22053e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_matching(filtered_sentences, sentences):\n",
    "    # timeNow(\"Starting Text Matching.\")\n",
    "    # summary_size = 5\n",
    "    # f = open(file_name, \"r\")\n",
    "    # text = \"\"\n",
    "    # for line in f:\n",
    "    #     text += line.replace('!','.').replace('?','.').replace('\\n',' ')\n",
    "    # f.close()\n",
    "    \n",
    "    # sentences = sent_tokenize(text)  # Tokenize the text into sentences\n",
    "    # sentences = remove_stop_words(sentences)\n",
    "    \n",
    "    word_frequencies = {}  # Create an empty dictionary to store the word frequencies\n",
    "    for sentence in filtered_sentences:  # Loop through each sentence in the text\n",
    "        words = nltk.word_tokenize(sentence)  # Tokenize each sentence into words\n",
    "        for word in words:  # Loop through each word in the sentence\n",
    "            if word not in word_frequencies.keys():  # Check if the word is already in the dictionary\n",
    "                word_frequencies[word] = 1  # If not, set its count to 1\n",
    "            else:\n",
    "                word_frequencies[word] += 1  # If yes, increment its count by 1\n",
    "    summary = []  # Create an empty list to store the summary sentences\n",
    "    sentence_map = {} # Create an empty dictionary to store the sentence scores\n",
    "    sent_scores = []\n",
    "    sent_index = 0\n",
    "    for sentence in sentences:  # Loop through each sentence in the text again\n",
    "        words = nltk.word_tokenize(sentence)  # Tokenize each sentence into words again\n",
    "        score = 0  # Initialize a score variable to 0\n",
    "        for word in words:  # Loop through each word in the sentence\n",
    "            if word in word_frequencies.keys():  # Check if the current word is present in our dictionary of word frequencies\n",
    "                score += word_frequencies[word]  # If yes, add its frequency to our score variable\n",
    "        sentence_map[sent_index] = [sentence]\n",
    "        sent_scores.append(score)\n",
    "        sent_index += 1\n",
    "    \n",
    "    # scores = sorted(sentence_map.keys())\n",
    "    # scores = scores[len(scores)-summary_size:]\n",
    "    maxScore = max(sent_scores)    \n",
    "    sortedScored = list(reversed(sorted(sent_scores)))\n",
    "    \n",
    "    for i in range(len(sent_scores)):\n",
    "        sentence_map[i].append(sent_scores[i] / maxScore)\n",
    "        sentence_map[i].append(sortedScored.index(sent_scores[i]))\n",
    "    \n",
    "    # for score in scores:\n",
    "    #     summary.append(sentence_map[score])\n",
    "        # print(\"Score :[\",score,\"] Sentence :\", sentence_map[score])\n",
    "    # timeNow(\"Finished Text Matching\")\n",
    "    return sentence_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11eed9d",
   "metadata": {},
   "source": [
    "<h1> Luhn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56ac7536",
   "metadata": {},
   "outputs": [],
   "source": [
    "def luhn_algorithm(filtered_sentences, sentences):\n",
    "    # timeNow(\"Starting Luhn\")\n",
    "    # Initialize a list to store the summary\n",
    "    summary = []\n",
    "    # # Split the text into sentences\n",
    "    # sentences = text.split('.')\n",
    "    # Initialize a list to store the sentence scores\n",
    "    sentence_scores = []\n",
    "    sentence_map = {}\n",
    "    sent_index = 0\n",
    "    # Iterate through each sentence\n",
    "    for sentence in filtered_sentences:\n",
    "        # Split the sentence into words\n",
    "        words = sentence.split()\n",
    "        # Initialize a score for the sentence\n",
    "        score = 0\n",
    "        # Iterate through each word\n",
    "        for word in words:\n",
    "            # Calculate the score for the word\n",
    "            score += len(word)\n",
    "        # Add the score to the sentence scores list\n",
    "        sentence_scores.append(score)\n",
    "        sentence_map[sent_index] = [sentences[sent_index]]\n",
    "        sent_index += 1\n",
    "\n",
    "    maxScore = max(sentence_scores)\n",
    "    sortedScored = list(reversed(sorted(sentence_scores)))\n",
    "    \n",
    "    for i in range(len(sentence_scores)):\n",
    "        sentence_map[i].append(sentence_scores[i] / maxScore)\n",
    "        sentence_map[i].append(sortedScored.index(sentence_scores[i]))\n",
    "    # timeNow(\"Finished Luhn\")\n",
    "    return sentence_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5953c6cc",
   "metadata": {},
   "source": [
    "<h1> Latent Semantic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eadf61f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_idf(filtered_sentences):\n",
    "    tfidfconverter = TfidfVectorizer()\n",
    "    X = tfidfconverter.fit_transform(filtered_sentences).toarray()\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e28c9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsa_algorithm(X):\n",
    "    svdmodel = TruncatedSVD(n_components=2)\n",
    "    svdmodel.fit(X)\n",
    "    result = svdmodel.transform(X)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ecd5caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsa_summarization(filtered_sentences, sentences):\n",
    "    # timeNow(\"Starting LSA\")\n",
    "    # sentences = tokenization(text)\n",
    "    # filtered_sentences = remove_stop_words(sentences)\n",
    "    X = create_tf_idf(filtered_sentences)\n",
    "    result = lsa_algorithm(X)\n",
    "    scores = result[:,1]\n",
    "    summary = \"\"\n",
    "    sentence_map = {}\n",
    "    normalized = (scores-min(scores))/(max(scores)-min(scores))\n",
    "    # summarize our text by selecting only those sentences with higher scores\n",
    "    sortedScored = list(reversed(sorted(normalized)))\n",
    "    for i in range (len (normalized)):\n",
    "        sentence_map[i] = [sentences[i], normalized[i], sortedScored.index(normalized[i])]\n",
    "    # timeNow(\"Finished LSA\")\n",
    "    return sentence_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353a70df",
   "metadata": {},
   "source": [
    "<h1> Text Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e98e9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4a5d957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Big File...\n",
      "Finished Loading Big File.\n"
     ]
    }
   ],
   "source": [
    "if not(word_embeddings):\n",
    "    print(\"Loading Big File...\")\n",
    "    f = open('glove.6B.100d.txt', encoding='utf-8')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        word_embeddings[word] = coefs\n",
    "    f.close()\n",
    "    print(\"Finished Loading Big File.\")\n",
    "def textRank(filtered_sentences, sentences):\n",
    "    # timeNow(\"Starting Text Rank\")\n",
    "    sentence_vectors = []\n",
    "    for i in filtered_sentences:\n",
    "        if len(i) != 0:\n",
    "            v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "        else:\n",
    "            v = np.zeros((100))\n",
    "        sentence_vectors.append(v)\n",
    "\n",
    "    # similarity matrix\n",
    "    sim_mat = np.zeros([len(sentences), len(sentences)])\n",
    "\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(len(sentences)):\n",
    "            if i != j:\n",
    "                sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]   \n",
    "    #begining Graph stap\n",
    "    import networkx as nx\n",
    "    nx_graph = nx.from_numpy_array(sim_mat)\n",
    "    scores = nx.pagerank(nx_graph)\n",
    "    maxScore = max(scores.values())\n",
    "    for key in scores:\n",
    "        scores[key] = scores[key] / maxScore\n",
    "    sent_map = {}\n",
    "    sortedScores = list(reversed(sorted(scores.values())))\n",
    "    for index in range(len(sentences)):\n",
    "        sent_map[index] = [sentences[index] , scores[index], sortedScores.index(scores[index])]\n",
    "    #ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
    "    # timeNow(\"Finished Text Rank\")\n",
    "    return sent_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ba6c76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createEnsembledic(tm_dic,luhn_dic,lsa_dic,tr_dic,lex_dic):\n",
    "    ensemble_dic = {}\n",
    "    finalScores = []\n",
    "    for key in tm_dic:\n",
    "        final_score = tm_dic[key][1] + luhn_dic[key][1]+ lsa_dic[key][1] + tr_dic[key][1] + lex_dic[key][1]\n",
    "        finalScores.append(final_score)\n",
    "        scores_arr = [tm_dic[key][1],luhn_dic[key][1],lsa_dic[key][1],tr_dic[key][1],lex_dic[key][1]]\n",
    "        sent = tm_dic[key][0]\n",
    "        ensemble_dic[key] = [final_score, scores_arr, sent]\n",
    "    return ensemble_dic,finalScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8eedbdf7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ensembleSummary(ensemble_dic,finalScores,size):\n",
    "    finalScores = list(reversed(sorted(finalScores)))[:]\n",
    "    summaryScores = finalScores[:size] # number of summary sentences\n",
    "    summary = []\n",
    "    for key in ensemble_dic:\n",
    "        if(ensemble_dic[key][0] in summaryScores):\n",
    "            summary.append(ensemble_dic[key][2])\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5264d8c2-a10f-409f-b7a2-7c1f2b4a06a6",
   "metadata": {},
   "source": [
    "<h1> Lex Rank </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0314214-ab42-4945-83da-f0703d9175c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LexRank_algorithm(filtered_sentences,sentences,size=5,threshold = 0.095):\n",
    "    tfidfconverter = TfidfVectorizer()\n",
    "    tf_idf = tfidfconverter.fit_transform(filtered_sentences).toarray()\n",
    "    length = len(tf_idf)\n",
    "    similarity_matrix = np.zeros([length] * 2)\n",
    "    \n",
    "    for i in range(length):\n",
    "        for j in range(i, length):\n",
    "            similarity = cosine_similarity(tf_idf[i],tf_idf[j],i,j)\n",
    "\n",
    "            if similarity:\n",
    "                similarity_matrix[i, j] = similarity\n",
    "                similarity_matrix[j, i] = similarity    \n",
    "    \n",
    "    def get_summary(sentences,similarity_matrix,threshold,summary_size=1):\n",
    "\n",
    "        if not isinstance(summary_size, int) or summary_size < 1:\n",
    "            raise ValueError('\\'summary_size\\' should be a positive integer')\n",
    "\n",
    "        lex_scores = rank_sentences(sentences,similarity_matrix,threshold)\n",
    "\n",
    "        sorted_ix = np.argsort(lex_scores)[::-1]\n",
    "\n",
    "        summary_index=[]\n",
    "        for i in sorted_ix[:summary_size]:\n",
    "            summary_index.append(i)\n",
    "        #print(summary_index)\n",
    "        return lex_scores,summary_index\n",
    "\n",
    "    scores , summary_index = get_summary(sentences,similarity_matrix,threshold,size)\n",
    "\n",
    "    sentence_map = {}\n",
    "    normalized = (scores-min(scores))/(max(scores)-min(scores))\n",
    "    sortedScored = list(reversed(sorted(normalized)))\n",
    "    for i in range (len (normalized)):\n",
    "        sentence_map[i] = [sentences[i], normalized[i], sortedScored.index(normalized[i])]\n",
    "    return sentence_map\n",
    "\n",
    "def connected_nodes(matrix):\n",
    "    _, labels = connected_components(matrix)\n",
    "    z = csr_matrix(matrix)\n",
    "    groups = []\n",
    "    for tag in np.unique(labels):\n",
    "        group = np.where(labels == tag)[0]\n",
    "        groups.append(group)\n",
    "    return groups\n",
    "\n",
    "def cosine_similarity(list_1, list_2,i,j):\n",
    "        if i == j :\n",
    "            return 1\n",
    "        dot = np.dot(list_1, list_2)\n",
    "        if math.isclose(dot, 0):\n",
    "            return 0\n",
    "        norm = (np.linalg.norm(list_1) * np.linalg.norm(list_2))\n",
    "        cos_sim = dot / norm\n",
    "        return cos_sim\n",
    "    \n",
    "    \n",
    "def stationary_distribution(transition_matrix,normalized=True):\n",
    "    n_1, n_2 = transition_matrix.shape\n",
    "    if n_1 != n_2:\n",
    "        raise ValueError('\\'transition_matrix\\' should be square')\n",
    "\n",
    "    distribution = np.zeros(n_1)\n",
    "    grouped_indices = connected_nodes(transition_matrix)\n",
    "\n",
    "    for group in grouped_indices:\n",
    "        t_matrix = transition_matrix[np.ix_(group, group)]\n",
    "        eigenvector = _power_method(t_matrix)\n",
    "        distribution[group] = eigenvector\n",
    "    if normalized:\n",
    "        distribution /= n_1\n",
    "    return distribution\n",
    "\n",
    "def _power_method(transition_matrix):\n",
    "    sentences_count = len(transition_matrix)\n",
    "    eigenvector = np.ones(sentences_count)\n",
    "    if len(eigenvector) == 1:\n",
    "        return eigenvector\n",
    "    transposed_matrix = transition_matrix.T\n",
    "    lambda_val = 1.0\n",
    "\n",
    "    while np.allclose(lambda_val, eigenvector):\n",
    "        eigenvector_next = np.dot(transposed_matrix, eigenvector)\n",
    "        lambda_val = np.linalg.norm(np.subtract(eigenvector_next, eigenvector))\n",
    "        eigenvector = eigenvector_next\n",
    "    return eigenvector\n",
    "\n",
    "\n",
    "def create_markov_matrix(weights_matrix):\n",
    "    n_1, n_2 = weights_matrix.shape\n",
    "    if n_1 != n_2:\n",
    "        raise ValueError('\\'weights_matrix\\' should be square')\n",
    "\n",
    "    row_sum = weights_matrix.sum(axis=1, keepdims=True)\n",
    "\n",
    "    return weights_matrix / row_sum\n",
    "\n",
    "def create_markov_matrix_discrete(weights_matrix, threshold):\n",
    "    discrete_weights_matrix = weights_matrix#np.zeros(weights_matrix.shape)\n",
    "    #print(discrete_weights_matrix)\n",
    "    ixs = np.where(weights_matrix >= threshold)\n",
    "    discrete_weights_matrix[ixs] = 1\n",
    "    #print(discrete_weights_matrix)\n",
    "\n",
    "    return create_markov_matrix(discrete_weights_matrix)\n",
    "\n",
    "def degree_centrality_scores(similarity_matrix,threshold=None,increase_power=True):\n",
    "    if not (threshold is None or isinstance(threshold, float) and 0 <= threshold < 1):\n",
    "        raise ValueError(\n",
    "            '\\'threshold\\' should be a floating-point number '\n",
    "            'from the interval [0, 1) or None')\n",
    "\n",
    "    if threshold is None:\n",
    "        markov_matrix = create_markov_matrix(similarity_matrix)\n",
    "\n",
    "    else:\n",
    "        markov_matrix = create_markov_matrix_discrete(similarity_matrix,threshold)\n",
    "\n",
    "    scores = stationary_distribution(markov_matrix,normalized=True)\n",
    "    return scores\n",
    "\n",
    "def rank_sentences(sentences,similarity_matrix,threshold=0.03):  \n",
    "    scores = degree_centrality_scores(similarity_matrix,threshold)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d07358",
   "metadata": {},
   "source": [
    "<h1> Summarizing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7db1186a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractSummaryFromDic(dic, size):\n",
    "    summaryScores = []\n",
    "    summarySent = []\n",
    "    for i in range(size):\n",
    "        summaryScores.append(dic[i][1])\n",
    "        summarySent.append(dic[i][0])\n",
    "        \n",
    "    for i in range(size, len(dic), 1):\n",
    "        if(dic[i][1] > min(summaryScores)):\n",
    "            ind = summaryScores.index(min(summaryScores))\n",
    "            summaryScores[ind] = dic[i][1]\n",
    "            summarySent[ind] = dic[i][0]\n",
    "    return summarySent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16950de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTextRanks(filtered_sentences, sentences):\n",
    "    tm_dic = text_matching(filtered_sentences, sentences)\n",
    "    luhn_dic = luhn_algorithm(filtered_sentences, sentences)\n",
    "    lsa_dic = lsa_summarization(filtered_sentences, sentences)\n",
    "    tr_dic = textRank(filtered_sentences, sentences)\n",
    "    lex_dic = LexRank_algorithm(filtered_sentences, sentences)\n",
    "    ensemble_dic, ensemble_scores = createEnsembledic(tm_dic, luhn_dic, lsa_dic, tr_dic, lex_dic)\n",
    "    return ensemble_dic#, ensemble_scores, tm_dic, luhn_dic, lsa_dic, tr_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c8afe88-38a2-442c-a79e-505b9e60547e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def allCombs(ensemble_dic):\n",
    "    lst = []\n",
    "    lstCols = []\n",
    "    for i in ensemble_dic:\n",
    "        ensemble = ensemble_dic[i][0]\n",
    "        tm = ensemble_dic[i][1][0]\n",
    "        luhn = ensemble_dic[i][1][1]\n",
    "        lsa = ensemble_dic[i][1][2]\n",
    "        tr = ensemble_dic[i][1][3]\n",
    "        lex = ensemble_dic[i][1][4]\n",
    "        \n",
    "        lst.append([ensemble, tm, luhn, lsa, tr, lex,tm+luhn, tm+lsa, tm+tr, tm+lex, luhn+lsa, luhn+tr, luhn+lex, lsa+tr, lsa+lex, tr+lex,tm+luhn+lsa, tm+luhn+tr, tm+luhn+lex, tm+lsa+tr, tm+lsa+lex, tm+tr+lex, luhn+lsa+tr, luhn+lsa+lex, luhn+tr+lex, lsa+tr+lex,(ensemble-tm),(ensemble-luhn),(ensemble-lsa),(ensemble-tr),(ensemble-lex)])\n",
    "    \n",
    "    combinationScores = pd.DataFrame(lst, columns = ['ensemble', 'tm', 'luhn', 'lsa', 'tr', 'lex','tm luhn', 'tm lsa', 'tm tr', 'tm lex', 'luhn lsa', 'luhn tr', 'luhn lex', 'lsa tr', 'lsa lex', 'tr lex','tm luhn lsa', 'tm luhn tr', 'tm luhn lex', 'tm lsa tr', 'tm lsa lex', 'tm tr lex', 'luhn lsa tr', 'luhn lsa lex', 'luhn tr lex', 'lsa tr lex','luhn lsa tr lex','tm lsa tr lex','tm luhn tr lex','tm luhn lsa lex','tm luhn lsa tr'])     \n",
    "    return combinationScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c73d2545-2a0f-40ff-96ba-e5ce89125086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildDF(filtered_sentences, sentences):\n",
    "    ensemble_dic = getTextRanks(filtered_sentences, sentences)\n",
    "    df = allCombs(ensemble_dic)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6b6876d2-1f2e-4f0e-980b-0484a82c5e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarizeWith(sentences, df, algorithm, percentage):\n",
    "    if(percentage >= 1):\n",
    "        sent = int(percentage)\n",
    "    else:\n",
    "        sent = int(percentage * len(df))\n",
    "    summInds = df.nlargest(n=sent, columns=[algorithm])[algorithm].keys()\n",
    "    # summInds = sorted(summInds)\n",
    "    summ = \"\"\n",
    "    for i in summInds:\n",
    "        summ += sentences[i]\n",
    "    return summ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bd9c80-dde1-49e7-ae03-4770325c6210",
   "metadata": {},
   "outputs": [],
   "source": [
    "onesCols = ['ensemble', 'tm', 'luhn', 'lsa', 'tr', 'lex']\n",
    "twosCols = ['tm luhn', 'tm lsa', 'tm tr', 'tm lex', 'luhn lsa', 'luhn tr', 'luhn lex', 'lsa tr', 'lsa lex', 'tr lex']\n",
    "threesCols = ['tm luhn lsa', 'tm luhn tr', 'tm luhn lex', 'tm lsa tr', 'tm lsa lex', 'tm tr lex', 'luhn lsa tr', 'luhn lsa lex', 'luhn tr lex', 'lsa tr lex']\n",
    "foursCols = ['luhn lsa tr lex','tm lsa tr lex','tm luhn tr lex','tm luhn lsa lex','tm luhn lsa tr']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
