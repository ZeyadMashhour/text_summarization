{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9302e92c-e72e-4457-aa01-01e896dff51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1013d6f-b192-435e-bf59-7460a4b53ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All directories: ['10 Sentences', '5 Sentences', '6 Sentences', '7 Sentences', '8 Sentences', '9 Sentences', 'Dynamic Sentences']\n"
     ]
    }
   ],
   "source": [
    "path = \"BBC_dataset/Algorithm Eval Scores\"\n",
    "directories = [f for f in os.listdir(path) if os.path.isdir(os.path.join(path, f))]\n",
    "directories = directories[1:]\n",
    "print(\"All directories:\", directories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a402ab0c-2c7a-419c-954d-1344e57e6d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: float64)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1_fmeasure</th>\n",
       "      <th>rouge1_precision</th>\n",
       "      <th>rouge1_recall</th>\n",
       "      <th>rouge2_fmeasure</th>\n",
       "      <th>rouge2_precision</th>\n",
       "      <th>rouge2_recall</th>\n",
       "      <th>rougeL_fmeasure</th>\n",
       "      <th>rougeL_precision</th>\n",
       "      <th>rougeL_recall</th>\n",
       "      <th>rougeLsum_fmeasure</th>\n",
       "      <th>rougeLsum_precision</th>\n",
       "      <th>rougeLsum_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tensor(0.5124)</td>\n",
       "      <td>tensor(0.3904)</td>\n",
       "      <td>tensor(0.7451)</td>\n",
       "      <td>tensor(0.3928)</td>\n",
       "      <td>tensor(0.2990)</td>\n",
       "      <td>tensor(0.5724)</td>\n",
       "      <td>tensor(0.2787)</td>\n",
       "      <td>tensor(0.2123)</td>\n",
       "      <td>tensor(0.4052)</td>\n",
       "      <td>tensor(0.4315)</td>\n",
       "      <td>tensor(0.3288)</td>\n",
       "      <td>tensor(0.6275)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tensor(0.6735)</td>\n",
       "      <td>tensor(0.5156)</td>\n",
       "      <td>tensor(0.9706)</td>\n",
       "      <td>tensor(0.6352)</td>\n",
       "      <td>tensor(0.4859)</td>\n",
       "      <td>tensor(0.9172)</td>\n",
       "      <td>tensor(0.5184)</td>\n",
       "      <td>tensor(0.3969)</td>\n",
       "      <td>tensor(0.7471)</td>\n",
       "      <td>tensor(0.6653)</td>\n",
       "      <td>tensor(0.5094)</td>\n",
       "      <td>tensor(0.9588)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tensor(0.6897)</td>\n",
       "      <td>tensor(0.5263)</td>\n",
       "      <td>tensor(1.)</td>\n",
       "      <td>tensor(0.6720)</td>\n",
       "      <td>tensor(0.5122)</td>\n",
       "      <td>tensor(0.9767)</td>\n",
       "      <td>tensor(0.4509)</td>\n",
       "      <td>tensor(0.3441)</td>\n",
       "      <td>tensor(0.6538)</td>\n",
       "      <td>tensor(0.6419)</td>\n",
       "      <td>tensor(0.4899)</td>\n",
       "      <td>tensor(0.9308)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tensor(0.7077)</td>\n",
       "      <td>tensor(0.5879)</td>\n",
       "      <td>tensor(0.8889)</td>\n",
       "      <td>tensor(0.6023)</td>\n",
       "      <td>tensor(0.5000)</td>\n",
       "      <td>tensor(0.7573)</td>\n",
       "      <td>tensor(0.3423)</td>\n",
       "      <td>tensor(0.2843)</td>\n",
       "      <td>tensor(0.4300)</td>\n",
       "      <td>tensor(0.6731)</td>\n",
       "      <td>tensor(0.5591)</td>\n",
       "      <td>tensor(0.8454)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tensor(0.5984)</td>\n",
       "      <td>tensor(0.4370)</td>\n",
       "      <td>tensor(0.9487)</td>\n",
       "      <td>tensor(0.5474)</td>\n",
       "      <td>tensor(0.3992)</td>\n",
       "      <td>tensor(0.8707)</td>\n",
       "      <td>tensor(0.4259)</td>\n",
       "      <td>tensor(0.3110)</td>\n",
       "      <td>tensor(0.6752)</td>\n",
       "      <td>tensor(0.4582)</td>\n",
       "      <td>tensor(0.3346)</td>\n",
       "      <td>tensor(0.7265)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2219</th>\n",
       "      <td>tensor(0.6358)</td>\n",
       "      <td>tensor(0.5302)</td>\n",
       "      <td>tensor(0.7940)</td>\n",
       "      <td>tensor(0.5333)</td>\n",
       "      <td>tensor(0.4444)</td>\n",
       "      <td>tensor(0.6667)</td>\n",
       "      <td>tensor(0.3340)</td>\n",
       "      <td>tensor(0.2785)</td>\n",
       "      <td>tensor(0.4171)</td>\n",
       "      <td>tensor(0.5553)</td>\n",
       "      <td>tensor(0.4631)</td>\n",
       "      <td>tensor(0.6935)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>tensor(0.5707)</td>\n",
       "      <td>tensor(0.4317)</td>\n",
       "      <td>tensor(0.8417)</td>\n",
       "      <td>tensor(0.4902)</td>\n",
       "      <td>tensor(0.3704)</td>\n",
       "      <td>tensor(0.7246)</td>\n",
       "      <td>tensor(0.3512)</td>\n",
       "      <td>tensor(0.2657)</td>\n",
       "      <td>tensor(0.5180)</td>\n",
       "      <td>tensor(0.4244)</td>\n",
       "      <td>tensor(0.3210)</td>\n",
       "      <td>tensor(0.6259)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>tensor(0.6251)</td>\n",
       "      <td>tensor(0.7557)</td>\n",
       "      <td>tensor(0.5331)</td>\n",
       "      <td>tensor(0.4876)</td>\n",
       "      <td>tensor(0.5897)</td>\n",
       "      <td>tensor(0.4157)</td>\n",
       "      <td>tensor(0.3243)</td>\n",
       "      <td>tensor(0.3920)</td>\n",
       "      <td>tensor(0.2766)</td>\n",
       "      <td>tensor(0.3643)</td>\n",
       "      <td>tensor(0.4403)</td>\n",
       "      <td>tensor(0.3106)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>tensor(0.5118)</td>\n",
       "      <td>tensor(0.3927)</td>\n",
       "      <td>tensor(0.7347)</td>\n",
       "      <td>tensor(0.3667)</td>\n",
       "      <td>tensor(0.2810)</td>\n",
       "      <td>tensor(0.5274)</td>\n",
       "      <td>tensor(0.2322)</td>\n",
       "      <td>tensor(0.1782)</td>\n",
       "      <td>tensor(0.3333)</td>\n",
       "      <td>tensor(0.3507)</td>\n",
       "      <td>tensor(0.2691)</td>\n",
       "      <td>tensor(0.5034)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>tensor(0.3658)</td>\n",
       "      <td>tensor(0.8895)</td>\n",
       "      <td>tensor(0.2302)</td>\n",
       "      <td>tensor(0.3079)</td>\n",
       "      <td>tensor(0.7500)</td>\n",
       "      <td>tensor(0.1937)</td>\n",
       "      <td>tensor(0.2061)</td>\n",
       "      <td>tensor(0.5013)</td>\n",
       "      <td>tensor(0.1297)</td>\n",
       "      <td>tensor(0.3309)</td>\n",
       "      <td>tensor(0.8046)</td>\n",
       "      <td>tensor(0.2083)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2224 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     rouge1_fmeasure rouge1_precision   rouge1_recall rouge2_fmeasure  \\\n",
       "0     tensor(0.5124)   tensor(0.3904)  tensor(0.7451)  tensor(0.3928)   \n",
       "1     tensor(0.6735)   tensor(0.5156)  tensor(0.9706)  tensor(0.6352)   \n",
       "2     tensor(0.6897)   tensor(0.5263)      tensor(1.)  tensor(0.6720)   \n",
       "3     tensor(0.7077)   tensor(0.5879)  tensor(0.8889)  tensor(0.6023)   \n",
       "4     tensor(0.5984)   tensor(0.4370)  tensor(0.9487)  tensor(0.5474)   \n",
       "...              ...              ...             ...             ...   \n",
       "2219  tensor(0.6358)   tensor(0.5302)  tensor(0.7940)  tensor(0.5333)   \n",
       "2220  tensor(0.5707)   tensor(0.4317)  tensor(0.8417)  tensor(0.4902)   \n",
       "2221  tensor(0.6251)   tensor(0.7557)  tensor(0.5331)  tensor(0.4876)   \n",
       "2222  tensor(0.5118)   tensor(0.3927)  tensor(0.7347)  tensor(0.3667)   \n",
       "2223  tensor(0.3658)   tensor(0.8895)  tensor(0.2302)  tensor(0.3079)   \n",
       "\n",
       "     rouge2_precision   rouge2_recall rougeL_fmeasure rougeL_precision  \\\n",
       "0      tensor(0.2990)  tensor(0.5724)  tensor(0.2787)   tensor(0.2123)   \n",
       "1      tensor(0.4859)  tensor(0.9172)  tensor(0.5184)   tensor(0.3969)   \n",
       "2      tensor(0.5122)  tensor(0.9767)  tensor(0.4509)   tensor(0.3441)   \n",
       "3      tensor(0.5000)  tensor(0.7573)  tensor(0.3423)   tensor(0.2843)   \n",
       "4      tensor(0.3992)  tensor(0.8707)  tensor(0.4259)   tensor(0.3110)   \n",
       "...               ...             ...             ...              ...   \n",
       "2219   tensor(0.4444)  tensor(0.6667)  tensor(0.3340)   tensor(0.2785)   \n",
       "2220   tensor(0.3704)  tensor(0.7246)  tensor(0.3512)   tensor(0.2657)   \n",
       "2221   tensor(0.5897)  tensor(0.4157)  tensor(0.3243)   tensor(0.3920)   \n",
       "2222   tensor(0.2810)  tensor(0.5274)  tensor(0.2322)   tensor(0.1782)   \n",
       "2223   tensor(0.7500)  tensor(0.1937)  tensor(0.2061)   tensor(0.5013)   \n",
       "\n",
       "       rougeL_recall rougeLsum_fmeasure rougeLsum_precision rougeLsum_recall  \n",
       "0     tensor(0.4052)     tensor(0.4315)      tensor(0.3288)   tensor(0.6275)  \n",
       "1     tensor(0.7471)     tensor(0.6653)      tensor(0.5094)   tensor(0.9588)  \n",
       "2     tensor(0.6538)     tensor(0.6419)      tensor(0.4899)   tensor(0.9308)  \n",
       "3     tensor(0.4300)     tensor(0.6731)      tensor(0.5591)   tensor(0.8454)  \n",
       "4     tensor(0.6752)     tensor(0.4582)      tensor(0.3346)   tensor(0.7265)  \n",
       "...              ...                ...                 ...              ...  \n",
       "2219  tensor(0.4171)     tensor(0.5553)      tensor(0.4631)   tensor(0.6935)  \n",
       "2220  tensor(0.5180)     tensor(0.4244)      tensor(0.3210)   tensor(0.6259)  \n",
       "2221  tensor(0.2766)     tensor(0.3643)      tensor(0.4403)   tensor(0.3106)  \n",
       "2222  tensor(0.3333)     tensor(0.3507)      tensor(0.2691)   tensor(0.5034)  \n",
       "2223  tensor(0.1297)     tensor(0.3309)      tensor(0.8046)   tensor(0.2083)  \n",
       "\n",
       "[2224 rows x 12 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = []\n",
    "dire = path + \"/\" + directories[0]\n",
    "for file in os.listdir(dire):\n",
    "    if os.path.isfile(os.path.join(dire, file)):\n",
    "        files.append(dire+\"/\"+file)\n",
    "for file in files:\n",
    "    data = pd.read_csv(file)\n",
    "    data = data.iloc[:, 1:]\n",
    "    std = data.std(numeric_only = True)\n",
    "    break\n",
    "print(std)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5bb61bb8-cd1d-4618-aecd-09608776352a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('LDA',).csv\",\n",
       " \"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('Lex', 'LDA').csv\",\n",
       " \"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('Lex', 'Lsa').csv\",\n",
       " \"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('Lex', 'Lsa', 'LDA').csv\",\n",
       " \"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('Lex', 'Lsa', 'Tr').csv\",\n",
       " \"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('Lex', 'Lsa', 'Tr', 'LDA').csv\",\n",
       " \"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('Lex', 'Luhn').csv\",\n",
       " \"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('Lex', 'Luhn', 'LDA').csv\",\n",
       " \"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('Lex', 'Luhn', 'Lsa').csv\",\n",
       " \"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('Lex', 'Luhn', 'Lsa', 'LDA').csv\",\n",
       " \"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('Lex', 'Luhn', 'Lsa', 'Tr').csv\",\n",
       " \"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('Lex', 'Luhn', 'Lsa', 'Tr', 'LDA').csv\",\n",
       " \"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('Lex', 'Luhn', 'Tr').csv\",\n",
       " \"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('Lex', 'Luhn', 'Tr', 'LDA').csv\",\n",
       " \"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('Lex', 'Tr').csv\",\n",
       " \"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('Lex', 'Tr', 'LDA').csv\",\n",
       " \"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('Lex',).csv\",\n",
       " \"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('Lsa', 'LDA').csv\",\n",
       " \"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('Lsa', 'Tr').csv\",\n",
       " \"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('Lsa', 'Tr', 'LDA').csv\",\n",
       " \"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('Lsa',).csv\",\n",
       " \"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('Luhn', 'LDA').csv\",\n",
       " \"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('Luhn', 'Lsa').csv\",\n",
       " \"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('Luhn', 'Lsa', 'LDA').csv\",\n",
       " \"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('Luhn', 'Lsa', 'Tr').csv\",\n",
       " \"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('Luhn', 'Lsa', 'Tr', 'LDA').csv\",\n",
       " \"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('Luhn', 'Tr').csv\",\n",
       " \"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('Luhn', 'Tr', 'LDA').csv\",\n",
       " \"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('Luhn',).csv\",\n",
       " \"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('Tm', 'LDA').csv\",\n",
       " \"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('Tm', 'Lex').csv\",\n",
       " \"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('Tm', 'Lex', 'LDA').csv\",\n",
       " \"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('Tm', 'Lex', 'Lsa').csv\",\n",
       " \"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('Tm', 'Lex', 'Lsa', 'LDA').csv\",\n",
       " \"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('Tm', 'Lex', 'Lsa', 'Tr').csv\",\n",
       " \"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('Tm', 'Lex', 'Lsa', 'Tr', 'LDA').csv\",\n",
       " \"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('Tm', 'Lex', 'Luhn').csv\",\n",
       " \"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('Tm', 'Lex', 'Luhn', 'LDA').csv\",\n",
       " \"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('Tm', 'Lex', 'Luhn', 'Lsa').csv\",\n",
       " \"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('Tm', 'Lex', 'Luhn', 'Lsa', 'LDA').csv\",\n",
       " \"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('Tm', 'Lex', 'Luhn', 'Lsa', 'Tr').csv\",\n",
       " \"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('Tm', 'Lex', 'Luhn', 'Lsa', 'Tr', 'LDA').csv\",\n",
       " \"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('Tm', 'Lex', 'Luhn', 'Tr').csv\",\n",
       " \"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('Tm', 'Lex', 'Luhn', 'Tr', 'LDA').csv\",\n",
       " \"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('Tm', 'Lex', 'Tr').csv\",\n",
       " \"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('Tm', 'Lex', 'Tr', 'LDA').csv\",\n",
       " \"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('Tm', 'Lsa').csv\",\n",
       " \"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('Tm', 'Lsa', 'LDA').csv\",\n",
       " \"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('Tm', 'Lsa', 'Tr').csv\",\n",
       " \"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('Tm', 'Lsa', 'Tr', 'LDA').csv\",\n",
       " \"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('Tm', 'Luhn').csv\",\n",
       " \"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('Tm', 'Luhn', 'LDA').csv\",\n",
       " \"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('Tm', 'Luhn', 'Lsa').csv\",\n",
       " \"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('Tm', 'Luhn', 'Lsa', 'LDA').csv\",\n",
       " \"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('Tm', 'Luhn', 'Lsa', 'Tr').csv\",\n",
       " \"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('Tm', 'Luhn', 'Lsa', 'Tr', 'LDA').csv\",\n",
       " \"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('Tm', 'Luhn', 'Tr').csv\",\n",
       " \"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('Tm', 'Luhn', 'Tr', 'LDA').csv\",\n",
       " \"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('Tm', 'Tr').csv\",\n",
       " \"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('Tm', 'Tr', 'LDA').csv\",\n",
       " \"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('Tm',).csv\",\n",
       " \"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('Tr', 'LDA').csv\",\n",
       " \"BBC_dataset/Algorithm Eval Scores/10 Sentences/Eval_Score_('Tr',).csv\"]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abaeb2b-1254-4e64-91f0-b7b45f803c13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
