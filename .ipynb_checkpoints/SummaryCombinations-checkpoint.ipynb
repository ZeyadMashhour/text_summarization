{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4957ab89-14bd-4278-9f0e-08a43dfb6cef",
   "metadata": {},
   "source": [
    "# BASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be4a7e02-76f1-4fde-bfc1-c5d46afbe828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Big File...\n",
      "Finished Loading Big File.\n"
     ]
    }
   ],
   "source": [
    "# %run summarizationBackbone.ipynb\n",
    "%run summarizationBackboneDocLvl.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab683111-e3fc-4ef1-9337-e7b4abfe199b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing_algorithms import *\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa96b13-e850-4a11-b771-c90a55d0f7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, percentage):\n",
    "    try:\n",
    "        sentences, filtered_sentences = preprocessing_text_with_spacy(text,True,False)\n",
    "        df = buildDF(filtered_sentences, sentences)\n",
    "        summarizedRow = {\"Original Article\": ds[articleCol][i]}\n",
    "        for key in df.keys():\n",
    "            element = summarizeWith(sentences, df, key, percentage)\n",
    "            summarizedRow[key] = element\n",
    "        return summarizedRow\n",
    "    except Exception as e:\n",
    "        print(\"Error\",i, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7955734c-66f1-4264-8e5b-9f33ea851075",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = pd.read_csv(\"Duc_dataset_first_ref_summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9740f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Summarization Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6cb9aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Column Names\n",
    "articleCol = \"Original Article\"\n",
    "summaryCol = \"Original Summary\"\n",
    "# Documents to be summarized\n",
    "start = 0\n",
    "end = len(ds)\n",
    "# Summarization Percentage [0-1]% if > 1 acts as number of sentences\n",
    "percentage = 5\n",
    "# Pre processing settings\n",
    "lemmatization = True\n",
    "remove_stopwords = True\n",
    "# Normalization Type\n",
    "isNormOnDataset = True \n",
    "# output file name [MAKE SURE TO CHANGE TO NOT OVERWRITE]\n",
    "outFileName = \"DUC_1st_5Sent_lemT_swT.csv\"\n",
    "ppFileName = \"PP_DUC_1st_5Sent_lemT_swT.csv\"\n",
    "ppSpecial = \"#!@!#\"\n",
    "# Weights Settings\n",
    "useWeights = True\n",
    "weights = {\"tm\":0.92,\"luhn\":1,\"lsa\":0.44,\"tr\":0.84,\"lex\":0.88,\"lda\":0.8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af32bf3d-e8eb-4b98-9f87-df6e64b11ae4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "Algorithms = [\"Tm\",\"Lex\",\"Luhn\",\"Lsa\",\"Tr\",\"LDA\"]\n",
    "lstCols = get_combinations(Algorithms)\n",
    "cols = [\"Original Article\",\"Original Summary\"] + lstCols\n",
    "df3 = pd.DataFrame(columns = cols)\n",
    "df3.to_csv(outFileName, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c1f36d-3745-4111-9bc1-47a3505b5b22",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c908dbc7-2103-43d0-9b9e-2bfd15cef295",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [08:33<00:00, 10.48s/it]\n"
     ]
    }
   ],
   "source": [
    "sentences, processed_sentences = process_one_column_df(ds[articleCol],lemmatization,remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cbed59b-0851-4b57-901f-0e13bf319f04",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "collected = []\n",
    "for i in range(len(sentences)):\n",
    "    collected.append([ds[articleCol][i],ds[summaryCol][i],ppSpecial.join(sentences[i]),ppSpecial.join(processed_sentences[i])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93d46318-a2ac-48f4-bfa2-027712766b1c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(collected, columns =[articleCol,summaryCol,\"Sentences\",\"Filtered Sent\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9226ad9-a2d4-42f7-a4dd-933a50cfef92",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_csv(ppFileName, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c68bcbd5-cc6b-4268-a8e3-cad964049e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(ppFileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad0caeac-ced0-4f9b-9a3a-b1f98163865d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Article</th>\n",
       "      <th>Original Summary</th>\n",
       "      <th>Sentences</th>\n",
       "      <th>Filtered Sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nCambodian leader Hun Sen on Friday rejected ...</td>\n",
       "      <td>Prospects were dim for resolution of the polit...</td>\n",
       "      <td>\\nCambodian leader Hun Sen on Friday rejected ...</td>\n",
       "      <td>cambodian leader hun sen friday reject opposit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nHonduras braced for potential catastrophe Tu...</td>\n",
       "      <td>Hurricane Mitch approached Honduras on Oct. 27...</td>\n",
       "      <td>\\nHonduras braced for potential catastrophe Tu...</td>\n",
       "      <td>honduras brace potential catastrophe tuesday h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nCuban President Fidel Castro said Sunday he ...</td>\n",
       "      <td>Britain caused international controversy and C...</td>\n",
       "      <td>\\nCuban President Fidel Castro said Sunday he ...</td>\n",
       "      <td>cuban president fidel castro sunday disagree a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nMUNICH, Germany (AP) _ U.S. prosecutors have...</td>\n",
       "      <td>After the bombing of U.S. embassies in East Af...</td>\n",
       "      <td>\\nMUNICH, Germany (AP) _#!@!#U.S. prosecutors ...</td>\n",
       "      <td>munich germany ap#!@!#prosecutor ask 20 - day ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\nIn a critical ruling for the North American ...</td>\n",
       "      <td>In a dispute over a new collective bargaining ...</td>\n",
       "      <td>\\nIn a critical ruling for the North American ...</td>\n",
       "      <td>critical rule north american national basketba...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Original Article  \\\n",
       "0  \\nCambodian leader Hun Sen on Friday rejected ...   \n",
       "1  \\nHonduras braced for potential catastrophe Tu...   \n",
       "2  \\nCuban President Fidel Castro said Sunday he ...   \n",
       "3  \\nMUNICH, Germany (AP) _ U.S. prosecutors have...   \n",
       "4  \\nIn a critical ruling for the North American ...   \n",
       "\n",
       "                                    Original Summary  \\\n",
       "0  Prospects were dim for resolution of the polit...   \n",
       "1  Hurricane Mitch approached Honduras on Oct. 27...   \n",
       "2  Britain caused international controversy and C...   \n",
       "3  After the bombing of U.S. embassies in East Af...   \n",
       "4  In a dispute over a new collective bargaining ...   \n",
       "\n",
       "                                           Sentences  \\\n",
       "0  \\nCambodian leader Hun Sen on Friday rejected ...   \n",
       "1  \\nHonduras braced for potential catastrophe Tu...   \n",
       "2  \\nCuban President Fidel Castro said Sunday he ...   \n",
       "3  \\nMUNICH, Germany (AP) _#!@!#U.S. prosecutors ...   \n",
       "4  \\nIn a critical ruling for the North American ...   \n",
       "\n",
       "                                       Filtered Sent  \n",
       "0  cambodian leader hun sen friday reject opposit...  \n",
       "1  honduras brace potential catastrophe tuesday h...  \n",
       "2  cuban president fidel castro sunday disagree a...  \n",
       "3  munich germany ap#!@!#prosecutor ask 20 - day ...  \n",
       "4  critical rule north american national basketba...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323f0948-8d29-4414-8ccc-00948524e45a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# one by one docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "05cf673d-6f12-4d33-aeae-ac8e3ee27f61",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def onebyoneSummarization(ds):\n",
    "#     summarizedDataset = []\n",
    "#     for i in tqdm(range(start,end,1)):\n",
    "#         try:\n",
    "#             # sentences, filtered_sentences = preprocessing_text_with_spacy(ds[articleCol][i],lemmatization,remove_stopwords)\n",
    "#             # Read Pre Processed Data\n",
    "#             data = pd.read_csv(ppFileName)\n",
    "#             sentences = data[\"Sentences\"][i].split(ppSpecial)\n",
    "#             filtered_sentences = data[\"Filtered Sent\"][i].split(ppSpecial)\n",
    "#             # build combination dataframe\n",
    "#             df = buildDF(filtered_sentences, sentences, useWeights, weights)\n",
    "\n",
    "#             summarizedRow = {\"Original Article\": ds[articleCol][i],\"Original Summary\": ds[summaryCol][i]}\n",
    "#             for key in df.keys():\n",
    "#                 element = summarizeWith(sentences, df, key, percentage)\n",
    "#                 summarizedRow[key] = element\n",
    "#             pd.DataFrame.from_dict(summarizedRow, orient='index').T.to_csv(outFileName,mode='a',header=False,index=False)\n",
    "#             # break\n",
    "#         except Exception as e:\n",
    "#             print(\"Error\",i, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfabaaa-67f6-4b44-87f1-fddff60a2e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [59:45<00:00, 73.17s/it]   \n"
     ]
    }
   ],
   "source": [
    "# onebyoneSummarization(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf8bd1c-6fae-4d5d-a003-af9f79323608",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Doc scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e3d3b97-4d18-453c-af11-b3317cd8e030",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstCols = Algorithms\n",
    "data = pd.read_csv(ppFileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9914a36-385a-4e41-827b-49789361845b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [52:37<00:00, 64.44s/it] \n"
     ]
    }
   ],
   "source": [
    "    maxes = []\n",
    "    mines = []\n",
    "    for i in tqdm(range(start,end,1)):\n",
    "        try:\n",
    "            sentences = data[\"Sentences\"][i].split(ppSpecial)\n",
    "            filtered_sentences = data[\"Filtered Sent\"][i].split(ppSpecial)\n",
    "            df = buildDF(filtered_sentences, sentences, useWeights, weights)\n",
    "            df.to_csv(\"DUC Sentence Score by Document/Document \" + str(i) + \".csv\", index=False)\n",
    "            maxes.append([df[Algorithms[0]].max(),df[Algorithms[1]].max(),df[Algorithms[2]].max(),df[Algorithms[3]].max(),df[Algorithms[4]].max(),df[Algorithms[5]].max()])\n",
    "            mines.append([df[Algorithms[0]].min(),df[Algorithms[1]].min(),df[Algorithms[2]].min(),df[Algorithms[3]].min(),df[Algorithms[4]].min(),df[Algorithms[5]].min()])\n",
    "        except Exception as e:\n",
    "            print(\"Error\",i, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73e430e7-1556-4371-a803-bc089f666bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(maxes, columns = Algorithms) \n",
    "df.to_csv(\"DUC Sentence Score by Document/max.csv\", index=False)\n",
    "df = pd.DataFrame(mines, columns = Algorithms) \n",
    "df.to_csv(\"DUC Sentence Score by Document/min.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16332642-b984-4bb8-a0a1-8b739f660233",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxdf = pd.read_csv(\"DUC Sentence Score by Document/max.csv\")\n",
    "mindf = pd.read_csv(\"DUC Sentence Score by Document/min.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "349b56c7-db7f-45ce-8549-39929b3290b1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "absmax = [maxdf[Algorithms[0]].max(),maxdf[Algorithms[1]].max(),maxdf[Algorithms[2]].max(),maxdf[Algorithms[3]].max(),maxdf[Algorithms[4]].max(),maxdf[Algorithms[5]].max()]\n",
    "absmin = [mindf[Algorithms[0]].min(),mindf[Algorithms[1]].min(),mindf[Algorithms[2]].min(),mindf[Algorithms[3]].min(),mindf[Algorithms[4]].min(),mindf[Algorithms[5]].min()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170d6373-4633-44ed-a4f2-f786efafd164",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fff39fd-701d-4701-8767-f8097ee8f795",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(values, min_value, max_value):\n",
    "    normalized_values = [(x - min_value) / (max_value - min_value) for x in values]\n",
    "    return normalized_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91612f11-1a3a-46c4-8b10-6f90b18961f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "Main Function for dataset level norm (Merged on next cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "d7f43594-2b7c-4ad6-9b9d-2aec312fdf10",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "    # # dataset = []\n",
    "    # for i in tqdm(range(start,end,1)):\n",
    "    #     # try:\n",
    "    #         # Normalization\n",
    "    #         df = pd.read_csv(\"DUC Sentence Score by Document/Document \"+ str(i) +\".csv\")\n",
    "    #         norm2dlist = pd.DataFrame()\n",
    "    #         norm2dlist[\"Sent\"] = df[\"Sent\"]\n",
    "    #         for key in range(len(Algorithms)):\n",
    "    #             norm2dlist[Algorithms[key]] = normalize(df[Algorithms[key]],absmin[key],absmax[key])\n",
    "    #         # Combinations  \n",
    "    #         combdf = allCombsDf(norm2dlist)\n",
    "    #         # Summarization\n",
    "    #         summarizedRow = {\"Original Article\": ds[articleCol][i],\"Original Summary\": ds[summaryCol][i]}\n",
    "    #         for key in combdf.keys():\n",
    "    #             element = summarizeWith(df[\"Sent\"], combdf, key, percentage)\n",
    "    #             summarizedRow[key] = element\n",
    "    #         pd.DataFrame.from_dict(summarizedRow, orient='index').T.to_csv(outFileName,mode='a',header=False,index=False)\n",
    "    #     # except Exception as e:\n",
    "    #     #     print(\"Error\",i, e)\n",
    "    #     #     break\n",
    "    #     # dataset.append(norm2dlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1104c5b-69db-40f4-9b3b-0692aaeda4f9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bee92620-7c9f-46be-a5cc-6f0fc4fb208e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [00:13<00:00,  3.67it/s]\n"
     ]
    }
   ],
   "source": [
    "    # dataset = []\n",
    "    for i in tqdm(range(start,end,1)):\n",
    "        # try:\n",
    "            # Normalization\n",
    "            df = pd.read_csv(\"DUC Sentence Score by Document/Document \"+ str(i) +\".csv\")\n",
    "            norm2dlist = pd.DataFrame()\n",
    "            norm2dlist[\"Sent\"] = df[\"Sent\"]\n",
    "            for key in range(len(Algorithms)):\n",
    "                if(isNormOnDataset):\n",
    "                    normMax = absmax[key]\n",
    "                    normMin = absmin[key]\n",
    "                else:\n",
    "                    normMax = df[Algorithms[key]].max()\n",
    "                    normMin = df[Algorithms[key]].min()\n",
    "                norm2dlist[Algorithms[key]] = normalize(df[Algorithms[key]],normMin,normMax)\n",
    "            # Combinations  \n",
    "            combdf = allCombsDf(norm2dlist)\n",
    "            # Summarization\n",
    "            summarizedRow = {\"Original Article\": ds[articleCol][i],\"Original Summary\": ds[summaryCol][i]}\n",
    "            for key in combdf.keys():\n",
    "                element = summarizeWith(df[\"Sent\"], combdf, key, percentage)\n",
    "                summarizedRow[key] = element\n",
    "            pd.DataFrame.from_dict(summarizedRow, orient='index').T.to_csv(outFileName,mode='a',header=False,index=False)\n",
    "        # except Exception as e:\n",
    "        #     print(\"Error\",i, e)\n",
    "        #     break\n",
    "        # dataset.append(norm2dlist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
