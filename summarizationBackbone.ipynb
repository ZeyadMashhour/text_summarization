{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2358b1a8",
   "metadata": {},
   "source": [
    "<h1> imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "946228a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Specialist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm as tqdm\n",
    "import itertools\n",
    "#lda\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pandas as pd\n",
    "from preprocessing_algorithms import *\n",
    "from efficiency_scores import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c154778c-9df1-4595-98b1-f7e5989b094a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "from lexrank.utils.text import tokenize\n",
    "from lexrank.mappings.stopwords import STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd40aa51",
   "metadata": {},
   "source": [
    "<h1> Pre Processing </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48ac4705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    filtered_sentences = remove_stop_words(sentences)\n",
    "    return filtered_sentences,sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fee1d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(sentences):\n",
    "    # define a set of stop words \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_sentences = []\n",
    "    for sentence in sentences:\n",
    "        # tokenize each sentence into words\n",
    "        word_tokens = word_tokenize(sentence)\n",
    "        # remove all stop words from each sentence\n",
    "        filtered_words = [word for word in word_tokens if not word in stop_words]\n",
    "        # join all filtered words back into a single sentence\n",
    "        filtered_sentence = ' '.join(filtered_words)\n",
    "        filtered_sentences.append(filtered_sentence)\n",
    "    return filtered_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dc771a",
   "metadata": {},
   "source": [
    "<h1> Text Matching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22053e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_matching(filtered_sentences, sentences):\n",
    "    # timeNow(\"Starting Text Matching.\")\n",
    "    # summary_size = 5\n",
    "    # f = open(file_name, \"r\")\n",
    "    # text = \"\"\n",
    "    # for line in f:\n",
    "    #     text += line.replace('!','.').replace('?','.').replace('\\n',' ')\n",
    "    # f.close()\n",
    "    \n",
    "    # sentences = sent_tokenize(text)  # Tokenize the text into sentences\n",
    "    # sentences = remove_stop_words(sentences)\n",
    "    \n",
    "    word_frequencies = {}  # Create an empty dictionary to store the word frequencies\n",
    "    for sentence in filtered_sentences:  # Loop through each sentence in the text\n",
    "        words = nltk.word_tokenize(sentence)  # Tokenize each sentence into words\n",
    "        for word in words:  # Loop through each word in the sentence\n",
    "            if word not in word_frequencies.keys():  # Check if the word is already in the dictionary\n",
    "                word_frequencies[word] = 1  # If not, set its count to 1\n",
    "            else:\n",
    "                word_frequencies[word] += 1  # If yes, increment its count by 1\n",
    "    summary = []  # Create an empty list to store the summary sentences\n",
    "    sentence_map = {} # Create an empty dictionary to store the sentence scores\n",
    "    sent_scores = []\n",
    "    sent_index = 0\n",
    "    for sentence in sentences:  # Loop through each sentence in the text again\n",
    "        words = nltk.word_tokenize(sentence)  # Tokenize each sentence into words again\n",
    "        score = 0  # Initialize a score variable to 0\n",
    "        for word in words:  # Loop through each word in the sentence\n",
    "            if word in word_frequencies.keys():  # Check if the current word is present in our dictionary of word frequencies\n",
    "                score += word_frequencies[word]  # If yes, add its frequency to our score variable\n",
    "        sentence_map[sent_index] = [sentence]\n",
    "        sent_scores.append(score)\n",
    "        sent_index += 1\n",
    "    \n",
    "    # scores = sorted(sentence_map.keys())\n",
    "    # scores = scores[len(scores)-summary_size:]\n",
    "    maxScore = max(sent_scores)    \n",
    "    sortedScored = list(reversed(sorted(sent_scores)))\n",
    "    \n",
    "    for i in range(len(sent_scores)):\n",
    "        sentence_map[i].append(sent_scores[i] / maxScore)\n",
    "        sentence_map[i].append(sortedScored.index(sent_scores[i]))\n",
    "    \n",
    "    # for score in scores:\n",
    "    #     summary.append(sentence_map[score])\n",
    "        # print(\"Score :[\",score,\"] Sentence :\", sentence_map[score])\n",
    "    # timeNow(\"Finished Text Matching\")\n",
    "    return sentence_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11eed9d",
   "metadata": {},
   "source": [
    "<h1> Luhn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56ac7536",
   "metadata": {},
   "outputs": [],
   "source": [
    "def luhn_algorithm(filtered_sentences, sentences):\n",
    "    # timeNow(\"Starting Luhn\")\n",
    "    # Initialize a list to store the summary\n",
    "    summary = []\n",
    "    # # Split the text into sentences\n",
    "    # sentences = text.split('.')\n",
    "    # Initialize a list to store the sentence scores\n",
    "    sentence_scores = []\n",
    "    sentence_map = {}\n",
    "    sent_index = 0\n",
    "    # Iterate through each sentence\n",
    "    for sentence in filtered_sentences:\n",
    "        # Split the sentence into words\n",
    "        words = sentence.split()\n",
    "        # Initialize a score for the sentence\n",
    "        score = 0\n",
    "        # Iterate through each word\n",
    "        for word in words:\n",
    "            # Calculate the score for the word\n",
    "            score += len(word)\n",
    "        # Add the score to the sentence scores list\n",
    "        sentence_scores.append(score)\n",
    "        sentence_map[sent_index] = [sentences[sent_index]]\n",
    "        sent_index += 1\n",
    "\n",
    "    maxScore = max(sentence_scores)\n",
    "    sortedScored = list(reversed(sorted(sentence_scores)))\n",
    "    \n",
    "    for i in range(len(sentence_scores)):\n",
    "        sentence_map[i].append(sentence_scores[i] / maxScore)\n",
    "        sentence_map[i].append(sortedScored.index(sentence_scores[i]))\n",
    "    # timeNow(\"Finished Luhn\")\n",
    "    return sentence_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5953c6cc",
   "metadata": {},
   "source": [
    "<h1> Latent Semantic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eadf61f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_idf(filtered_sentences):\n",
    "    tfidfconverter = TfidfVectorizer()\n",
    "    X = tfidfconverter.fit_transform(filtered_sentences).toarray()\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e28c9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsa_algorithm(X):\n",
    "    svdmodel = TruncatedSVD(n_components=2)\n",
    "    svdmodel.fit(X)\n",
    "    result = svdmodel.transform(X)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ecd5caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsa_summarization(filtered_sentences, sentences):\n",
    "    # timeNow(\"Starting LSA\")\n",
    "    # sentences = tokenization(text)\n",
    "    # filtered_sentences = remove_stop_words(sentences)\n",
    "    X = create_tf_idf(filtered_sentences)\n",
    "    result = lsa_algorithm(X)\n",
    "    scores = result[:,1]\n",
    "    summary = \"\"\n",
    "    sentence_map = {}\n",
    "    normalized = (scores-min(scores))/(max(scores)-min(scores))\n",
    "    # summarize our text by selecting only those sentences with higher scores\n",
    "    sortedScored = list(reversed(sorted(normalized)))\n",
    "    for i in range (len (normalized)):\n",
    "        sentence_map[i] = [sentences[i], normalized[i], sortedScored.index(normalized[i])]\n",
    "    # timeNow(\"Finished LSA\")\n",
    "    return sentence_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353a70df",
   "metadata": {},
   "source": [
    "<h1> Text Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e98e9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4a5d957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Big File...\n",
      "Finished Loading Big File.\n"
     ]
    }
   ],
   "source": [
    "if not(word_embeddings):\n",
    "    print(\"Loading Big File...\")\n",
    "    f = open('glove.6B.100d.txt', encoding='utf-8')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        word_embeddings[word] = coefs\n",
    "    f.close()\n",
    "    print(\"Finished Loading Big File.\")\n",
    "def textRank(filtered_sentences, sentences):\n",
    "    # timeNow(\"Starting Text Rank\")\n",
    "    sentence_vectors = []\n",
    "    for i in filtered_sentences:\n",
    "        if len(i) != 0:\n",
    "            v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "        else:\n",
    "            v = np.zeros((100))\n",
    "        sentence_vectors.append(v)\n",
    "\n",
    "    # similarity matrix\n",
    "    sim_mat = np.zeros([len(sentences), len(sentences)])\n",
    "\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(len(sentences)):\n",
    "            if i != j:\n",
    "                sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]   \n",
    "    #begining Graph stap\n",
    "    import networkx as nx\n",
    "    nx_graph = nx.from_numpy_array(sim_mat)\n",
    "    scores = nx.pagerank(nx_graph)\n",
    "    maxScore = max(scores.values())\n",
    "    for key in scores:\n",
    "        scores[key] = scores[key] / maxScore\n",
    "    sent_map = {}\n",
    "    sortedScores = list(reversed(sorted(scores.values())))\n",
    "    for index in range(len(sentences)):\n",
    "        sent_map[index] = [sentences[index] , scores[index], sortedScores.index(scores[index])]\n",
    "    #ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
    "    # timeNow(\"Finished Text Rank\")\n",
    "    return sent_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5264d8c2-a10f-409f-b7a2-7c1f2b4a06a6",
   "metadata": {},
   "source": [
    "<h1> Lex Rank </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0314214-ab42-4945-83da-f0703d9175c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LexRank_algorithm(filtered_sentences,sentences,size=5,threshold = 0.095):\n",
    "    tfidfconverter = TfidfVectorizer()\n",
    "    tf_idf = tfidfconverter.fit_transform(filtered_sentences).toarray()\n",
    "    length = len(tf_idf)\n",
    "    similarity_matrix = np.zeros([length] * 2)\n",
    "    \n",
    "    for i in range(length):\n",
    "        for j in range(i, length):\n",
    "            similarity = cosine_similarity(tf_idf[i],tf_idf[j],i,j)\n",
    "\n",
    "            if similarity:\n",
    "                similarity_matrix[i, j] = similarity\n",
    "                similarity_matrix[j, i] = similarity    \n",
    "    \n",
    "    def get_summary(sentences,similarity_matrix,threshold,summary_size=1):\n",
    "\n",
    "        if not isinstance(summary_size, int) or summary_size < 1:\n",
    "            raise ValueError('\\'summary_size\\' should be a positive integer')\n",
    "\n",
    "        lex_scores = rank_sentences(sentences,similarity_matrix,threshold)\n",
    "\n",
    "        sorted_ix = np.argsort(lex_scores)[::-1]\n",
    "\n",
    "        summary_index=[]\n",
    "        for i in sorted_ix[:summary_size]:\n",
    "            summary_index.append(i)\n",
    "        #print(summary_index)\n",
    "        return lex_scores,summary_index\n",
    "\n",
    "    scores , summary_index = get_summary(sentences,similarity_matrix,threshold,size)\n",
    "\n",
    "    sentence_map = {}\n",
    "    normalized = (scores-min(scores))/(max(scores)-min(scores))\n",
    "    sortedScored = list(reversed(sorted(normalized)))\n",
    "    for i in range (len (normalized)):\n",
    "        sentence_map[i] = [sentences[i], normalized[i], sortedScored.index(normalized[i])]\n",
    "    return sentence_map\n",
    "\n",
    "def connected_nodes(matrix):\n",
    "    _, labels = connected_components(matrix)\n",
    "    z = csr_matrix(matrix)\n",
    "    groups = []\n",
    "    for tag in np.unique(labels):\n",
    "        group = np.where(labels == tag)[0]\n",
    "        groups.append(group)\n",
    "    return groups\n",
    "\n",
    "def cosine_similarity(list_1, list_2,i,j):\n",
    "        if i == j :\n",
    "            return 1\n",
    "        dot = np.dot(list_1, list_2)\n",
    "        if math.isclose(dot, 0):\n",
    "            return 0\n",
    "        norm = (np.linalg.norm(list_1) * np.linalg.norm(list_2))\n",
    "        cos_sim = dot / norm\n",
    "        return cos_sim\n",
    "    \n",
    "    \n",
    "def stationary_distribution(transition_matrix,normalized=True):\n",
    "    n_1, n_2 = transition_matrix.shape\n",
    "    if n_1 != n_2:\n",
    "        raise ValueError('\\'transition_matrix\\' should be square')\n",
    "\n",
    "    distribution = np.zeros(n_1)\n",
    "    grouped_indices = connected_nodes(transition_matrix)\n",
    "\n",
    "    for group in grouped_indices:\n",
    "        t_matrix = transition_matrix[np.ix_(group, group)]\n",
    "        eigenvector = _power_method(t_matrix)\n",
    "        distribution[group] = eigenvector\n",
    "    if normalized:\n",
    "        distribution /= n_1\n",
    "    return distribution\n",
    "\n",
    "def _power_method(transition_matrix):\n",
    "    sentences_count = len(transition_matrix)\n",
    "    eigenvector = np.ones(sentences_count)\n",
    "    if len(eigenvector) == 1:\n",
    "        return eigenvector\n",
    "    transposed_matrix = transition_matrix.T\n",
    "    lambda_val = 1.0\n",
    "\n",
    "    while np.allclose(lambda_val, eigenvector):\n",
    "        eigenvector_next = np.dot(transposed_matrix, eigenvector)\n",
    "        lambda_val = np.linalg.norm(np.subtract(eigenvector_next, eigenvector))\n",
    "        eigenvector = eigenvector_next\n",
    "    return eigenvector\n",
    "\n",
    "\n",
    "def create_markov_matrix(weights_matrix):\n",
    "    n_1, n_2 = weights_matrix.shape\n",
    "    if n_1 != n_2:\n",
    "        raise ValueError('\\'weights_matrix\\' should be square')\n",
    "\n",
    "    row_sum = weights_matrix.sum(axis=1, keepdims=True)\n",
    "\n",
    "    return weights_matrix / row_sum\n",
    "\n",
    "def create_markov_matrix_discrete(weights_matrix, threshold):\n",
    "    discrete_weights_matrix = weights_matrix#np.zeros(weights_matrix.shape)\n",
    "    #print(discrete_weights_matrix)\n",
    "    ixs = np.where(weights_matrix >= threshold)\n",
    "    discrete_weights_matrix[ixs] = 1\n",
    "    #print(discrete_weights_matrix)\n",
    "\n",
    "    return create_markov_matrix(discrete_weights_matrix)\n",
    "\n",
    "def degree_centrality_scores(similarity_matrix,threshold=None,increase_power=True):\n",
    "    if not (threshold is None or isinstance(threshold, float) and 0 <= threshold < 1):\n",
    "        raise ValueError(\n",
    "            '\\'threshold\\' should be a floating-point number '\n",
    "            'from the interval [0, 1) or None')\n",
    "\n",
    "    if threshold is None:\n",
    "        markov_matrix = create_markov_matrix(similarity_matrix)\n",
    "\n",
    "    else:\n",
    "        markov_matrix = create_markov_matrix_discrete(similarity_matrix,threshold)\n",
    "\n",
    "    scores = stationary_distribution(markov_matrix,normalized=True)\n",
    "    return scores\n",
    "\n",
    "def rank_sentences(sentences,similarity_matrix,threshold=0.03):  \n",
    "    scores = degree_centrality_scores(similarity_matrix,threshold)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f0e2ab-75a9-40fd-82cf-d6b596678f8a",
   "metadata": {},
   "source": [
    "<h1> LDA </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1cbb38b-4a6f-4299-989c-b53e03ac072c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_algorithm(filtered_sentences,sentences,num_topics = 3):\n",
    "    # Create a CountVectorizer object to preprocess the text\n",
    "    vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "    # Fit and transform the vectorizer to the text\n",
    "    doc_term_matrix = vectorizer.fit_transform(filtered_sentences)\n",
    "\n",
    "    # Create an LDA object using Gibbs sampling\n",
    "    lda = LatentDirichletAllocation(n_components=num_topics, learning_method='batch', random_state=42)\n",
    "\n",
    "    # Fit the LDA model to the document-term matrix\n",
    "    lda.fit(doc_term_matrix)\n",
    "\n",
    "    # Extract the topic summaries\n",
    "    topic_word_distributions = lda.components_\n",
    "    for i, topic_dist in enumerate(topic_word_distributions):\n",
    "        topic_words = [vectorizer.get_feature_names_out()[j] for j in topic_dist.argsort()[:-6:-1]]\n",
    "        #print(f\"Topic {i}: {' '.join(topic_words)}\")\n",
    "\n",
    "    # Extract the topic distribution for each sentence\n",
    "    sentence_topic_distributions = lda.transform(doc_term_matrix)\n",
    "\n",
    "    # Calculate the sentence scores\n",
    "    sentence_scores = sentence_topic_distributions.max(axis=1)\n",
    "\n",
    "    # Sort the sentences by score in descending order\n",
    "    sorted_indices = sentence_scores.argsort()[::-1]\n",
    "\n",
    "    # Extract the top N sentences\n",
    "    # top_indices = sorted_indices[:size]\n",
    "    # top_sentences = [sentences[i] for i in top_indices]\n",
    "\n",
    "    # Print the summary\n",
    "    #print('\\n'.join(top_sentences))\n",
    "    sent_map = {}\n",
    "    for i in range(len(sentences)):\n",
    "        sent_map[i] = [sentences[i],sentence_scores[i],sorted_indices[i]]\n",
    "        \n",
    "    return sent_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d07358",
   "metadata": {},
   "source": [
    "<h1> Summarizing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2695a682-7afc-46e8-8fc9-fc3bded6af73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createEnsembledic(tm_dic,luhn_dic,lsa_dic,tr_dic,lex_dic, lda_dic):\n",
    "    ensemble_dic = {}\n",
    "    finalScores = []\n",
    "    for key in tm_dic:\n",
    "        final_score = tm_dic[key][1] + luhn_dic[key][1]+ lsa_dic[key][1] + tr_dic[key][1] + lex_dic[key][1] + lda_dic[key][1]\n",
    "        finalScores.append(final_score)\n",
    "        scores_arr = [tm_dic[key][1],luhn_dic[key][1],lsa_dic[key][1],tr_dic[key][1],lex_dic[key][1],lda_dic[key][1]]\n",
    "        sent = tm_dic[key][0]\n",
    "        ensemble_dic[key] = [final_score, scores_arr, sent]\n",
    "    return ensemble_dic,finalScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df9a3251-bf35-49e6-ae46-de89cd994402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensembleSummary(ensemble_dic,finalScores,size):\n",
    "    finalScores = list(reversed(sorted(finalScores)))[:]\n",
    "    summaryScores = finalScores[:size] # number of summary sentences\n",
    "    summary = []\n",
    "    for key in ensemble_dic:\n",
    "        if(ensemble_dic[key][0] in summaryScores):\n",
    "            summary.append(ensemble_dic[key][2])\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7db1186a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractSummaryFromDic(dic, size):\n",
    "    summaryScores = []\n",
    "    summarySent = []\n",
    "    for i in range(size):\n",
    "        summaryScores.append(dic[i][1])\n",
    "        summarySent.append(dic[i][0])\n",
    "        \n",
    "    for i in range(size, len(dic), 1):\n",
    "        if(dic[i][1] > min(summaryScores)):\n",
    "            ind = summaryScores.index(min(summaryScores))\n",
    "            summaryScores[ind] = dic[i][1]\n",
    "            summarySent[ind] = dic[i][0]\n",
    "    return summarySent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16950de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTextRanks(filtered_sentences, sentences):\n",
    "    tm_dic = text_matching(filtered_sentences, sentences)\n",
    "    luhn_dic = luhn_algorithm(filtered_sentences, sentences)\n",
    "    lsa_dic = lsa_summarization(filtered_sentences, sentences)\n",
    "    tr_dic = textRank(filtered_sentences, sentences)\n",
    "    lex_dic = LexRank_algorithm(filtered_sentences, sentences)\n",
    "    lda_dic = lda_algorithm(filtered_sentences, sentences)\n",
    "    ensemble_dic, ensemble_scores = createEnsembledic(tm_dic, luhn_dic, lsa_dic, tr_dic, lex_dic, lda_dic)\n",
    "    return ensemble_dic#, ensemble_scores, tm_dic, luhn_dic, lsa_dic, tr_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe9b1bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combinations(arr):\n",
    "    combinations = []\n",
    "    for i in range(1,len(arr)+1):\n",
    "        for element in itertools.combinations(arr, i):\n",
    "            combinations.append(element)\n",
    "    return combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79151fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"summary input.txt\", \"r\") as myfile:\n",
    "    data = myfile.read()\n",
    "    filtered_sentences,sentences = tokenization(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c8afe88-38a2-442c-a79e-505b9e60547e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def allCombs(ensemble_dic, useWeights, weights):\n",
    "    Algorithms = [\"Tm\",\"Lex\",\"Luhn\",\"Lsa\",\"Tr\",\"LDA\"]\n",
    "    lst = []\n",
    "    lstCols = get_combinations(Algorithms)\n",
    "    wtm = 1\n",
    "    wlex = 1\n",
    "    wluhn = 1\n",
    "    wlsa = 1\n",
    "    wtr = 1\n",
    "    wlda = 1\n",
    "    if(useWeights):\n",
    "        wtm = weights[\"tm\"]\n",
    "        wlex =  weights[\"lex\"]\n",
    "        wluhn =  weights[\"luhn\"]\n",
    "        wlsa =  weights[\"lsa\"]\n",
    "        wtr =  weights[\"tr\"]\n",
    "        wlda =  weights[\"lda\"]\n",
    "    for i in ensemble_dic:\n",
    "        tm = ensemble_dic[i][1][0] * wtm\n",
    "        luhn = ensemble_dic[i][1][1] * wluhn\n",
    "        lsa = ensemble_dic[i][1][2] * wlsa\n",
    "        tr = ensemble_dic[i][1][3] * wtr\n",
    "        lex = ensemble_dic[i][1][4] * wlex\n",
    "        lda = ensemble_dic[i][1][5] * wlda\n",
    "        values = [tm,lex,luhn,lsa,tr,lda]\n",
    "\n",
    "        CombNums = get_combinations(values)\n",
    "        for i,comb in enumerate(CombNums):\n",
    "            CombNums[i] = sum(comb)/len(comb)\n",
    "        lst.append(CombNums)\n",
    "    combinationScores = pd.DataFrame(lst, columns = lstCols)     \n",
    "    return combinationScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c73d2545-2a0f-40ff-96ba-e5ce89125086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildDF(filtered_sentences, sentences, useWeights, weights):\n",
    "    ensemble_dic = getTextRanks(filtered_sentences, sentences)\n",
    "    df = allCombs(ensemble_dic, useWeights, weights)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b6876d2-1f2e-4f0e-980b-0484a82c5e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarizeWith(sentences, df, algorithm, percentage):\n",
    "    if(percentage >= 1):\n",
    "        sent = int(percentage)\n",
    "    else:\n",
    "        sent = int(percentage * len(df))\n",
    "    summInds = df.nlargest(n=sent, columns=[algorithm])[algorithm].keys()\n",
    "    # summInds = sorted(summInds)\n",
    "    summ = \"\"\n",
    "    for i in summInds:\n",
    "        summ += sentences[i]\n",
    "    return summ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6390a82c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(Tm,)</th>\n",
       "      <th>(Lex,)</th>\n",
       "      <th>(Luhn,)</th>\n",
       "      <th>(Lsa,)</th>\n",
       "      <th>(Tr,)</th>\n",
       "      <th>(LDA,)</th>\n",
       "      <th>(Tm, Lex)</th>\n",
       "      <th>(Tm, Luhn)</th>\n",
       "      <th>(Tm, Lsa)</th>\n",
       "      <th>(Tm, Tr)</th>\n",
       "      <th>...</th>\n",
       "      <th>(Lex, Luhn, Tr, LDA)</th>\n",
       "      <th>(Lex, Lsa, Tr, LDA)</th>\n",
       "      <th>(Luhn, Lsa, Tr, LDA)</th>\n",
       "      <th>(Tm, Lex, Luhn, Lsa, Tr)</th>\n",
       "      <th>(Tm, Lex, Luhn, Lsa, LDA)</th>\n",
       "      <th>(Tm, Lex, Luhn, Tr, LDA)</th>\n",
       "      <th>(Tm, Lex, Lsa, Tr, LDA)</th>\n",
       "      <th>(Tm, Luhn, Lsa, Tr, LDA)</th>\n",
       "      <th>(Lex, Luhn, Lsa, Tr, LDA)</th>\n",
       "      <th>(Tm, Lex, Luhn, Lsa, Tr, LDA)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.272813</td>\n",
       "      <td>0.481029</td>\n",
       "      <td>0.502793</td>\n",
       "      <td>0.310268</td>\n",
       "      <td>0.876167</td>\n",
       "      <td>0.946820</td>\n",
       "      <td>0.753842</td>\n",
       "      <td>0.775606</td>\n",
       "      <td>0.583080</td>\n",
       "      <td>1.148979</td>\n",
       "      <td>...</td>\n",
       "      <td>2.806809</td>\n",
       "      <td>2.614283</td>\n",
       "      <td>2.636048</td>\n",
       "      <td>2.443069</td>\n",
       "      <td>2.513723</td>\n",
       "      <td>3.079622</td>\n",
       "      <td>2.887096</td>\n",
       "      <td>2.908860</td>\n",
       "      <td>3.117076</td>\n",
       "      <td>3.389889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.170273</td>\n",
       "      <td>0.170805</td>\n",
       "      <td>0.430168</td>\n",
       "      <td>0.242011</td>\n",
       "      <td>0.855048</td>\n",
       "      <td>0.927874</td>\n",
       "      <td>0.341078</td>\n",
       "      <td>0.600440</td>\n",
       "      <td>0.412284</td>\n",
       "      <td>1.025321</td>\n",
       "      <td>...</td>\n",
       "      <td>2.383895</td>\n",
       "      <td>2.195738</td>\n",
       "      <td>2.455101</td>\n",
       "      <td>1.868304</td>\n",
       "      <td>1.941131</td>\n",
       "      <td>2.554167</td>\n",
       "      <td>2.366011</td>\n",
       "      <td>2.625374</td>\n",
       "      <td>2.625906</td>\n",
       "      <td>2.796179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.808090</td>\n",
       "      <td>0.077315</td>\n",
       "      <td>0.452514</td>\n",
       "      <td>0.378279</td>\n",
       "      <td>0.933820</td>\n",
       "      <td>0.931024</td>\n",
       "      <td>0.885405</td>\n",
       "      <td>1.260604</td>\n",
       "      <td>1.186370</td>\n",
       "      <td>1.741910</td>\n",
       "      <td>...</td>\n",
       "      <td>2.394673</td>\n",
       "      <td>2.320438</td>\n",
       "      <td>2.695637</td>\n",
       "      <td>2.650018</td>\n",
       "      <td>2.647223</td>\n",
       "      <td>3.202763</td>\n",
       "      <td>3.128528</td>\n",
       "      <td>3.503728</td>\n",
       "      <td>2.772952</td>\n",
       "      <td>3.581042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.516463</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.525140</td>\n",
       "      <td>0.691638</td>\n",
       "      <td>0.975627</td>\n",
       "      <td>0.952457</td>\n",
       "      <td>1.516463</td>\n",
       "      <td>1.041603</td>\n",
       "      <td>1.208101</td>\n",
       "      <td>1.492090</td>\n",
       "      <td>...</td>\n",
       "      <td>3.453224</td>\n",
       "      <td>3.619723</td>\n",
       "      <td>3.144862</td>\n",
       "      <td>3.708868</td>\n",
       "      <td>3.685698</td>\n",
       "      <td>3.969687</td>\n",
       "      <td>4.136185</td>\n",
       "      <td>3.661325</td>\n",
       "      <td>4.144862</td>\n",
       "      <td>4.661325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.375353</td>\n",
       "      <td>0.165715</td>\n",
       "      <td>0.452514</td>\n",
       "      <td>0.358791</td>\n",
       "      <td>0.931568</td>\n",
       "      <td>0.935785</td>\n",
       "      <td>0.541068</td>\n",
       "      <td>0.827867</td>\n",
       "      <td>0.734144</td>\n",
       "      <td>1.306921</td>\n",
       "      <td>...</td>\n",
       "      <td>2.485583</td>\n",
       "      <td>2.391860</td>\n",
       "      <td>2.678658</td>\n",
       "      <td>2.283941</td>\n",
       "      <td>2.288159</td>\n",
       "      <td>2.860935</td>\n",
       "      <td>2.767213</td>\n",
       "      <td>3.054011</td>\n",
       "      <td>2.844374</td>\n",
       "      <td>3.219726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0.334901</td>\n",
       "      <td>0.150730</td>\n",
       "      <td>0.469274</td>\n",
       "      <td>0.221761</td>\n",
       "      <td>0.976314</td>\n",
       "      <td>0.950128</td>\n",
       "      <td>0.485631</td>\n",
       "      <td>0.804175</td>\n",
       "      <td>0.556662</td>\n",
       "      <td>1.311215</td>\n",
       "      <td>...</td>\n",
       "      <td>2.546446</td>\n",
       "      <td>2.298933</td>\n",
       "      <td>2.617477</td>\n",
       "      <td>2.152980</td>\n",
       "      <td>2.126794</td>\n",
       "      <td>2.881347</td>\n",
       "      <td>2.633834</td>\n",
       "      <td>2.952378</td>\n",
       "      <td>2.768207</td>\n",
       "      <td>3.103108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.211665</td>\n",
       "      <td>0.098111</td>\n",
       "      <td>0.636872</td>\n",
       "      <td>0.287802</td>\n",
       "      <td>0.949203</td>\n",
       "      <td>0.951949</td>\n",
       "      <td>0.309777</td>\n",
       "      <td>0.848537</td>\n",
       "      <td>0.499467</td>\n",
       "      <td>1.160868</td>\n",
       "      <td>...</td>\n",
       "      <td>2.636134</td>\n",
       "      <td>2.287065</td>\n",
       "      <td>2.825825</td>\n",
       "      <td>2.183652</td>\n",
       "      <td>2.186399</td>\n",
       "      <td>2.847799</td>\n",
       "      <td>2.498730</td>\n",
       "      <td>3.037490</td>\n",
       "      <td>2.923936</td>\n",
       "      <td>3.135601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0.693321</td>\n",
       "      <td>0.066966</td>\n",
       "      <td>0.720670</td>\n",
       "      <td>0.349351</td>\n",
       "      <td>0.976777</td>\n",
       "      <td>0.959071</td>\n",
       "      <td>0.760287</td>\n",
       "      <td>1.413991</td>\n",
       "      <td>1.042672</td>\n",
       "      <td>1.670097</td>\n",
       "      <td>...</td>\n",
       "      <td>2.723484</td>\n",
       "      <td>2.352165</td>\n",
       "      <td>3.005869</td>\n",
       "      <td>2.807085</td>\n",
       "      <td>2.789379</td>\n",
       "      <td>3.416805</td>\n",
       "      <td>3.045486</td>\n",
       "      <td>3.699190</td>\n",
       "      <td>3.072835</td>\n",
       "      <td>3.766156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0.224835</td>\n",
       "      <td>0.099321</td>\n",
       "      <td>0.413408</td>\n",
       "      <td>0.438117</td>\n",
       "      <td>0.745311</td>\n",
       "      <td>0.953355</td>\n",
       "      <td>0.324156</td>\n",
       "      <td>0.638243</td>\n",
       "      <td>0.662952</td>\n",
       "      <td>0.970146</td>\n",
       "      <td>...</td>\n",
       "      <td>2.211395</td>\n",
       "      <td>2.236103</td>\n",
       "      <td>2.550190</td>\n",
       "      <td>1.920991</td>\n",
       "      <td>2.129036</td>\n",
       "      <td>2.436230</td>\n",
       "      <td>2.460939</td>\n",
       "      <td>2.775026</td>\n",
       "      <td>2.649511</td>\n",
       "      <td>2.874347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0.380997</td>\n",
       "      <td>0.316057</td>\n",
       "      <td>0.469274</td>\n",
       "      <td>0.327125</td>\n",
       "      <td>0.916639</td>\n",
       "      <td>0.941024</td>\n",
       "      <td>0.697054</td>\n",
       "      <td>0.850271</td>\n",
       "      <td>0.708122</td>\n",
       "      <td>1.297636</td>\n",
       "      <td>...</td>\n",
       "      <td>2.642993</td>\n",
       "      <td>2.500844</td>\n",
       "      <td>2.654061</td>\n",
       "      <td>2.410091</td>\n",
       "      <td>2.434476</td>\n",
       "      <td>3.023990</td>\n",
       "      <td>2.881841</td>\n",
       "      <td>3.035059</td>\n",
       "      <td>2.970118</td>\n",
       "      <td>3.351115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>123 rows Ã— 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        (Tm,)    (Lex,)   (Luhn,)    (Lsa,)     (Tr,)    (LDA,)  (Tm, Lex)   \n",
       "0    0.272813  0.481029  0.502793  0.310268  0.876167  0.946820   0.753842  \\\n",
       "1    0.170273  0.170805  0.430168  0.242011  0.855048  0.927874   0.341078   \n",
       "2    0.808090  0.077315  0.452514  0.378279  0.933820  0.931024   0.885405   \n",
       "3    0.516463  1.000000  0.525140  0.691638  0.975627  0.952457   1.516463   \n",
       "4    0.375353  0.165715  0.452514  0.358791  0.931568  0.935785   0.541068   \n",
       "..        ...       ...       ...       ...       ...       ...        ...   \n",
       "118  0.334901  0.150730  0.469274  0.221761  0.976314  0.950128   0.485631   \n",
       "119  0.211665  0.098111  0.636872  0.287802  0.949203  0.951949   0.309777   \n",
       "120  0.693321  0.066966  0.720670  0.349351  0.976777  0.959071   0.760287   \n",
       "121  0.224835  0.099321  0.413408  0.438117  0.745311  0.953355   0.324156   \n",
       "122  0.380997  0.316057  0.469274  0.327125  0.916639  0.941024   0.697054   \n",
       "\n",
       "     (Tm, Luhn)  (Tm, Lsa)  (Tm, Tr)  ...  (Lex, Luhn, Tr, LDA)   \n",
       "0      0.775606   0.583080  1.148979  ...              2.806809  \\\n",
       "1      0.600440   0.412284  1.025321  ...              2.383895   \n",
       "2      1.260604   1.186370  1.741910  ...              2.394673   \n",
       "3      1.041603   1.208101  1.492090  ...              3.453224   \n",
       "4      0.827867   0.734144  1.306921  ...              2.485583   \n",
       "..          ...        ...       ...  ...                   ...   \n",
       "118    0.804175   0.556662  1.311215  ...              2.546446   \n",
       "119    0.848537   0.499467  1.160868  ...              2.636134   \n",
       "120    1.413991   1.042672  1.670097  ...              2.723484   \n",
       "121    0.638243   0.662952  0.970146  ...              2.211395   \n",
       "122    0.850271   0.708122  1.297636  ...              2.642993   \n",
       "\n",
       "     (Lex, Lsa, Tr, LDA)  (Luhn, Lsa, Tr, LDA)  (Tm, Lex, Luhn, Lsa, Tr)   \n",
       "0               2.614283              2.636048                  2.443069  \\\n",
       "1               2.195738              2.455101                  1.868304   \n",
       "2               2.320438              2.695637                  2.650018   \n",
       "3               3.619723              3.144862                  3.708868   \n",
       "4               2.391860              2.678658                  2.283941   \n",
       "..                   ...                   ...                       ...   \n",
       "118             2.298933              2.617477                  2.152980   \n",
       "119             2.287065              2.825825                  2.183652   \n",
       "120             2.352165              3.005869                  2.807085   \n",
       "121             2.236103              2.550190                  1.920991   \n",
       "122             2.500844              2.654061                  2.410091   \n",
       "\n",
       "     (Tm, Lex, Luhn, Lsa, LDA)  (Tm, Lex, Luhn, Tr, LDA)   \n",
       "0                     2.513723                  3.079622  \\\n",
       "1                     1.941131                  2.554167   \n",
       "2                     2.647223                  3.202763   \n",
       "3                     3.685698                  3.969687   \n",
       "4                     2.288159                  2.860935   \n",
       "..                         ...                       ...   \n",
       "118                   2.126794                  2.881347   \n",
       "119                   2.186399                  2.847799   \n",
       "120                   2.789379                  3.416805   \n",
       "121                   2.129036                  2.436230   \n",
       "122                   2.434476                  3.023990   \n",
       "\n",
       "     (Tm, Lex, Lsa, Tr, LDA)  (Tm, Luhn, Lsa, Tr, LDA)   \n",
       "0                   2.887096                  2.908860  \\\n",
       "1                   2.366011                  2.625374   \n",
       "2                   3.128528                  3.503728   \n",
       "3                   4.136185                  3.661325   \n",
       "4                   2.767213                  3.054011   \n",
       "..                       ...                       ...   \n",
       "118                 2.633834                  2.952378   \n",
       "119                 2.498730                  3.037490   \n",
       "120                 3.045486                  3.699190   \n",
       "121                 2.460939                  2.775026   \n",
       "122                 2.881841                  3.035059   \n",
       "\n",
       "     (Lex, Luhn, Lsa, Tr, LDA)  (Tm, Lex, Luhn, Lsa, Tr, LDA)  \n",
       "0                     3.117076                       3.389889  \n",
       "1                     2.625906                       2.796179  \n",
       "2                     2.772952                       3.581042  \n",
       "3                     4.144862                       4.661325  \n",
       "4                     2.844374                       3.219726  \n",
       "..                         ...                            ...  \n",
       "118                   2.768207                       3.103108  \n",
       "119                   2.923936                       3.135601  \n",
       "120                   3.072835                       3.766156  \n",
       "121                   2.649511                       2.874347  \n",
       "122                   2.970118                       3.351115  \n",
       "\n",
       "[123 rows x 63 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# buildDF(filtered_sentences, sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cfb825-478b-4498-925c-4afaaf5c93ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
