{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTSUM(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTSUM, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "        self.linear = torch.nn.Linear(self.bert.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        scores = self.linear(pooled_output)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the lemmatizer and stopword list\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# Original sentences\n",
    "original_sentences = [\"The quick brown fox jumps over the lazy dog.\",\n",
    "                       \"This is a sentence.\",\n",
    "                       \"Another sentence here.\",\n",
    "                       \"This is a short sentence.\",\n",
    "                       \"The dog is lazy.\"]\n",
    "\n",
    "# Preprocess the sentences\n",
    "filtered_sentences = []\n",
    "for sentence in original_sentences:\n",
    "    # Tokenize the sentence\n",
    "    words = nltk.word_tokenize(sentence.lower())\n",
    "    \n",
    "    # Remove stopwords and lemmatize the remaining words\n",
    "    filtered_words = [lemmatizer.lemmatize(word) for word in words if word not in stopwords]\n",
    "    \n",
    "    # Convert the filtered words back to a sentence and add it to the list of filtered sentences\n",
    "    filtered_sentence = ' '.join(filtered_words)\n",
    "    filtered_sentences.append(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def bertsum_extractive_summarization(original_sentences, filtered_sentences, size=5):\n",
    "    \n",
    "#     # Set the random seed for PyTorch\n",
    "#     torch.manual_seed(42)\n",
    "#     # Tokenize original sentences\n",
    "#     tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "#     # input_ids = tokenizer(original_sentences, padding=True, truncation=True, return_tensors='pt').input_ids\n",
    "#     # attention_mask = tokenizer(original_sentences, padding=True, truncation=True, return_tensors='pt').attention_mask\n",
    "    \n",
    "#     # Tokenize filtered sentences\n",
    "#     input_ids_filtered = tokenizer(filtered_sentences, padding=True, truncation=True, return_tensors='pt').input_ids\n",
    "#     attention_mask_filtered = tokenizer(filtered_sentences, padding=True, truncation=True, return_tensors='pt').attention_mask\n",
    "    \n",
    "#     # Generate BERTSUM scores for filtered sentences\n",
    "#     model = BERTSUM()\n",
    "#     model.eval()\n",
    "#     scores = model(torch.tensor(input_ids_filtered), torch.tensor(attention_mask_filtered))\n",
    "\n",
    "#     # Sort scores in descending order\n",
    "#     sorted_scores, sorted_indices = torch.sort(scores.view(-1), descending=True)\n",
    "\n",
    "#     # Select top \"size\" sentences with highest scores\n",
    "#     top_indices = sorted_indices[:size].tolist()\n",
    "\n",
    "#     # Generate final summary\n",
    "#     summary = \"\"\n",
    "#     for index in top_indices:\n",
    "#         summary += original_sentences[index] + \" \"\n",
    "            \n",
    "#     return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary1 = bertsum_extractive_summarization(original_sentences, filtered_sentences, 2)\n",
    "# summary2 = bertsum_extractive_summarization(original_sentences, filtered_sentences, 2)\n",
    "# summary3 = bertsum_extractive_summarization(original_sentences, filtered_sentences, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(summary1)\n",
    "# print(summary2)\n",
    "# print(summary3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bertsum_extractive_summarization_batch(original_sentences, filtered_sentences, batch_size=10, size=5):\n",
    "    # Set the random seed for PyTorch\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # Tokenize original sentences\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Initialize the BERTSUM model\n",
    "    model = BERTSUM()\n",
    "    model.eval()\n",
    "\n",
    "    num_batches = len(filtered_sentences) // batch_size + int(len(filtered_sentences) % batch_size > 0)\n",
    "    all_scores = []\n",
    "\n",
    "    for batch_idx in range(num_batches):\n",
    "        # Tokenize filtered sentences in the current batch\n",
    "        batch_start = batch_idx * batch_size\n",
    "        batch_end = min((batch_idx + 1) * batch_size, len(filtered_sentences))\n",
    "        input_ids_filtered = tokenizer(filtered_sentences[batch_start:batch_end], padding=True, truncation=True, return_tensors='pt').input_ids\n",
    "        attention_mask_filtered = tokenizer(filtered_sentences[batch_start:batch_end], padding=True, truncation=True, return_tensors='pt').attention_mask\n",
    "\n",
    "        # Generate BERTSUM scores for filtered sentences in the current batch\n",
    "        scores = model(torch.tensor(input_ids_filtered), torch.tensor(attention_mask_filtered))\n",
    "\n",
    "        # Store scores for the current batch\n",
    "        all_scores.extend(scores.view(-1).tolist())\n",
    "\n",
    "    # Sort scores in descending order\n",
    "    sorted_indices = sorted(range(len(all_scores)), key=lambda i: all_scores[i], reverse=True)\n",
    "\n",
    "    # Select top \"size\" sentences with highest scores\n",
    "    top_indices = sorted_indices[:size]\n",
    "    # Generate final summary\n",
    "    summary = \"\"\n",
    "    for index in top_indices:\n",
    "        summary += original_sentences[index] + \" \"\n",
    "\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = bertsum_extractive_summarization_batch(original_sentences, filtered_sentences, size = 3)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from summarization_algorithm import *\n",
    "from preprocessing_algorithms import *\n",
    "from efficiency_scores import *\n",
    "df = pd.read_csv(\"DUC\\Duc_first_reference\\main_dataset\\Duc_dataset_first_ref_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, processed_sentences =process_one_column_df(df['Original Article'],remove_stopwords=True,lemmatization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_summaries = []\n",
    "for i in tqdm(range(49)):\n",
    "    list_of_summaries.append(bertsum_extractive_summarization_batch(sentences[i],processed_sentences[i],size = 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summarizer import Summarizer\n",
    "modeles = Summarizer()\n",
    "modeles(\"\".join(combined_sentences[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_summary = \"Honduras braced for potential catastrophe Tuesday as Hurricane Mitch \\r\\nroared through the northwest Caribbean, churning up high waves and \\r\\nintense rain that sent coastal residents scurrying for safer ground. In Belize, \\r\\na hurricane watch was in place and the government also closed schools \\r\\nand sent workers home early Monday. Jerry Jarrell, the weather \\r\\ncenter director, said Mitch was the strongest hurricane to strike \\r\\nthe Caribbean since 1988, when Gilbert killed more than 300 people. Maria \\r\\nGonzalez said she needed the gas to cook with when her firewood gets \\r\\nwet. In El Progreso, \\r\\n100 miles (160 kilometers) north of the Honduran capital of Tegucigalpa, \\r\\nthe army evacuated more than 5,000 people who live in low-lying banana \\r\\nplantations along the Ulua River, said Nolly Soliman, a resident. That meant the Honduran coast had been under hurricane \\r\\nconditions for more than a day. ``The hurricane has destroyed almost \\r\\neverything,'' said Mike Brown, a resident of Guanaja Island which \\r\\nwas within miles (kms) of the eye of the hurricane. At its, 4th graf pvs \\r\\n \\r\\nHurricane Mitch cut through the Honduran coast like a ripsaw Thursday, \\r\\nits devastating winds whirling for a third day through resort islands \\r\\nand mainland communities. About 10,000 residents fled to crowded shelters \\r\\nin schools, churches and firehouses. Only a few hotels and offices with their own \\r\\ngenerators had electricity. Wind-whipped waves almost buried some \\r\\nhouses near the shore. Police and soldiers patrolled the streets, and a few \\r\\npeople wandered amid the boarded-up houses. ``We couldn't go out on a boat, we couldn't go snorkeling. The U.S. \\r\\nAgency for International Development sent two helicopters each to \\r\\nBelize and Honduras to help in search, rescue and relief efforts. At least 231 people have been confirmed dead in Honduras from former-hurricane \\r\\nMitch, bringing the storm's death toll in the region to 357, the National \\r\\nEmergency Commission said Saturday. By late \\r\\nSunday, Mitch's winds, once near 180 mph (290 kph), had dropped to \\r\\nnear 30 mph (50 kph), and the storm _ now classified as a tropical \\r\\ndepression _ was near Tapachula, on Mexico's southern Pacific coast \\r\\nnear the Guatemalan border. That is in addition to least another 600 people elsewhere in the country, \\r\\nBolanos said. But its own regional affiliate in \\r\\nSan Miguel province reported 125 dead there alone. Guatemala reported \\r\\n100 storm-related deaths. the EU approved for the region on Friday. Pope John Paul II appealed for aid Wednesday for the Central American \\r\\ncountries stricken by hurricane Mitch and said he feels close to the \\r\\nthousands who are suffering. Among those attending the audience were six \\r\\nRussian cosmonauts taking a special course in Italy. President Carlos Flores declared Hurricane Mitch had \\r\\nset back Honduras' development by 50 years. In the capital, Tegucigalpa, Mexican rescue teams began searching \\r\\nfor avalanche victims. ``We have more access to places affected by the storm,'' Urbizo explained. El Salvador reported 239 dead; Guatemala said 194 of its \\r\\npeople had been killed. Hillary Rodham Clinton also will \\r\\ntravel to the region, visiting Nicaragua and Honduras on Nov. 16. The estimated \\r\\nnumber of homeless dropped from 580,000 to 569,000 Thursday. ``Our problem is that \\r\\nall of the country has been affected,'' he said. Dozens \\r\\nof homes were swept into the river. Foreign aid and pledges of assistance poured into Central America, \\r\\nbut damage to roads and bridges reduced the amount of supplies reaching \\r\\nhundreds of isolated communities to a trickle: only as much as could \\r\\nbe dropped from a helicopter, when the aircraft can get through. In many \\r\\nnearby villages, residents have gone days without potable water or \\r\\nfood. ``It's a coincidence that the ships \\r\\nare there but they've got men and equipment that can be put to work \\r\\nin an organized way,'' said International Development Secretary Clare \\r\\nShort.\"\n",
    "original_summary = df.iloc[1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficiency_dict = calculate_efficiency(predicted_summary,original_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sum = pd.DataFrame(list_of_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = rouge_scores_df(df,df_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_scores = df_avg_by_column(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_scores"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
