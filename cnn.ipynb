{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cnn_dailymail (C:/Users/basel/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ced670d4f9d4f9c800776387e17d57f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the CNN/Daily Mail dataset\n",
    "dataset = load_dataset('cnn_dailymail', '3.0.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cnn_dailymail (C:/Users/basel/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6236ad13490e446abf3759281d040da0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from datasets import load_dataset\n",
    "# import pandas as pd\n",
    "\n",
    "# # Load the CNN/Daily Mail dataset\n",
    "# dataset = load_dataset('cnn_dailymail', '3.0.0')\n",
    "\n",
    "# # Create a DataFrame with Original Articles and Original Summary\n",
    "# cnn_df = pd.DataFrame({\n",
    "#     'Original Articles': dataset['train']['article'],\n",
    "#     'Original Summary': dataset['train']['highlights']\n",
    "# })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cnn_dailymail (C:/Users/basel/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32279e325a54468892f2a21200527157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CNN/Daily Mail dataset\n",
    "dataset = load_dataset('cnn_dailymail', '3.0.0')\n",
    "\n",
    "# Combine the train and validation splits\n",
    "articles = dataset['train']['article'] + dataset['validation']['article'] + dataset['test']['article']\n",
    "summaries = dataset['train']['highlights'] + dataset['validation']['highlights'] + dataset['test']['highlights']\n",
    "\n",
    "# Create a DataFrame with Original Articles and Original Summary\n",
    "cnn_df = pd.DataFrame({\n",
    "    'Original Articles': articles,\n",
    "    'Original Summary': summaries\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Articles</th>\n",
       "      <th>Original Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LONDON, England (Reuters) -- Harry Potter star...</td>\n",
       "      <td>Harry Potter star Daniel Radcliffe gets £20M f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Editor's note: In our Behind the Scenes series...</td>\n",
       "      <td>Mentally ill inmates in Miami are housed on th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MINNEAPOLIS, Minnesota (CNN) -- Drivers who we...</td>\n",
       "      <td>NEW: \"I thought I was going to die,\" driver sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WASHINGTON (CNN) -- Doctors removed five small...</td>\n",
       "      <td>Five small polyps found during procedure; \"non...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(CNN)  -- The National Football League has ind...</td>\n",
       "      <td>NEW: NFL chief, Atlanta Falcons owner critical...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311966</th>\n",
       "      <td>Telecom watchdogs are to stop a rip-off that a...</td>\n",
       "      <td>Operators are charging up to 20p a minute - ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311967</th>\n",
       "      <td>The chilling reenactment of how executions are...</td>\n",
       "      <td>Bali Nine ringleaders will face the firing squ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311968</th>\n",
       "      <td>It is a week which has seen him in deep water ...</td>\n",
       "      <td>Hardy was convicted of domestic abuse against ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311969</th>\n",
       "      <td>Despite the hype surrounding its first watch, ...</td>\n",
       "      <td>Apple sold more than 61 million iPhones in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311970</th>\n",
       "      <td>Angus Hawley's brother has spoken of his shock...</td>\n",
       "      <td>Angus Hawley's brother said his late sibling '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>311971 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Original Articles   \n",
       "0       LONDON, England (Reuters) -- Harry Potter star...  \\\n",
       "1       Editor's note: In our Behind the Scenes series...   \n",
       "2       MINNEAPOLIS, Minnesota (CNN) -- Drivers who we...   \n",
       "3       WASHINGTON (CNN) -- Doctors removed five small...   \n",
       "4       (CNN)  -- The National Football League has ind...   \n",
       "...                                                   ...   \n",
       "311966  Telecom watchdogs are to stop a rip-off that a...   \n",
       "311967  The chilling reenactment of how executions are...   \n",
       "311968  It is a week which has seen him in deep water ...   \n",
       "311969  Despite the hype surrounding its first watch, ...   \n",
       "311970  Angus Hawley's brother has spoken of his shock...   \n",
       "\n",
       "                                         Original Summary  \n",
       "0       Harry Potter star Daniel Radcliffe gets £20M f...  \n",
       "1       Mentally ill inmates in Miami are housed on th...  \n",
       "2       NEW: \"I thought I was going to die,\" driver sa...  \n",
       "3       Five small polyps found during procedure; \"non...  \n",
       "4       NEW: NFL chief, Atlanta Falcons owner critical...  \n",
       "...                                                   ...  \n",
       "311966  Operators are charging up to 20p a minute - ev...  \n",
       "311967  Bali Nine ringleaders will face the firing squ...  \n",
       "311968  Hardy was convicted of domestic abuse against ...  \n",
       "311969  Apple sold more than 61 million iPhones in the...  \n",
       "311970  Angus Hawley's brother said his late sibling '...  \n",
       "\n",
       "[311971 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_df_105k = cnn_df[:105000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Articles</th>\n",
       "      <th>Original Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LONDON, England (Reuters) -- Harry Potter star...</td>\n",
       "      <td>Harry Potter star Daniel Radcliffe gets £20M f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Editor's note: In our Behind the Scenes series...</td>\n",
       "      <td>Mentally ill inmates in Miami are housed on th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MINNEAPOLIS, Minnesota (CNN) -- Drivers who we...</td>\n",
       "      <td>NEW: \"I thought I was going to die,\" driver sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WASHINGTON (CNN) -- Doctors removed five small...</td>\n",
       "      <td>Five small polyps found during procedure; \"non...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(CNN)  -- The National Football League has ind...</td>\n",
       "      <td>NEW: NFL chief, Atlanta Falcons owner critical...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104995</th>\n",
       "      <td>By . Emma Clark . PUBLISHED: . 05:59 EST, 22 O...</td>\n",
       "      <td>Fog descends on the parts of the country halti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104996</th>\n",
       "      <td>By . Mark Duell and Emily Allen . PUBLISHED: ....</td>\n",
       "      <td>Taxi driver Christopher Halliwell, 48, of Swin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104997</th>\n",
       "      <td>By . Alex Ward . PUBLISHED: . 10:55 EST, 16 Oc...</td>\n",
       "      <td>Rugby prop Benjamin Blake admitted causing act...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104998</th>\n",
       "      <td>By . Eddie Wrenn . PUBLISHED: . 04:11 EST, 19 ...</td>\n",
       "      <td>Dinner party held by the edge of the Nile in Z...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104999</th>\n",
       "      <td>By . Beth Stebner . PUBLISHED: . 01:28 EST, 28...</td>\n",
       "      <td>Family of Yoselyn Ortega said that nanny had s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Original Articles   \n",
       "0       LONDON, England (Reuters) -- Harry Potter star...  \\\n",
       "1       Editor's note: In our Behind the Scenes series...   \n",
       "2       MINNEAPOLIS, Minnesota (CNN) -- Drivers who we...   \n",
       "3       WASHINGTON (CNN) -- Doctors removed five small...   \n",
       "4       (CNN)  -- The National Football League has ind...   \n",
       "...                                                   ...   \n",
       "104995  By . Emma Clark . PUBLISHED: . 05:59 EST, 22 O...   \n",
       "104996  By . Mark Duell and Emily Allen . PUBLISHED: ....   \n",
       "104997  By . Alex Ward . PUBLISHED: . 10:55 EST, 16 Oc...   \n",
       "104998  By . Eddie Wrenn . PUBLISHED: . 04:11 EST, 19 ...   \n",
       "104999  By . Beth Stebner . PUBLISHED: . 01:28 EST, 28...   \n",
       "\n",
       "                                         Original Summary  \n",
       "0       Harry Potter star Daniel Radcliffe gets £20M f...  \n",
       "1       Mentally ill inmates in Miami are housed on th...  \n",
       "2       NEW: \"I thought I was going to die,\" driver sa...  \n",
       "3       Five small polyps found during procedure; \"non...  \n",
       "4       NEW: NFL chief, Atlanta Falcons owner critical...  \n",
       "...                                                   ...  \n",
       "104995  Fog descends on the parts of the country halti...  \n",
       "104996  Taxi driver Christopher Halliwell, 48, of Swin...  \n",
       "104997  Rugby prop Benjamin Blake admitted causing act...  \n",
       "104998  Dinner party held by the edge of the Nile in Z...  \n",
       "104999  Family of Yoselyn Ortega said that nanny had s...  \n",
       "\n",
       "[105000 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_df_105k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summarization_algorithm import *\n",
    "from preprocessing_algorithms import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 68460/105000 [25:00:39<79:31:14,  7.83s/it]   "
     ]
    }
   ],
   "source": [
    "sentences , filtered_sentences = process_one_column_df(cnn_df_105k['Original Articles'],True,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_list_of_lists(data, file_path):\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "def load_list_of_lists(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save_list_of_lists(sentences, 'sentences_cnn_105k.pkl')\n",
    "save_list_of_lists(filtered_sentences, 'filtered_sentences_cnn_105k.pkl')\n",
    "#sentences_cnn_105k = load_list_of_lists('sentences_cnn_105k.pkl')\n",
    "#filtered_sentences_cnn_105k = load_list_of_lists('filtered_sentences_cnn_105k.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_with(list_of_filtered_articles, list_of_articles, summary_algorithm, size=0.2):\n",
    "    rows = len(list_of_articles)\n",
    "    summarized_text = []\n",
    "    for row in tqdm(range(rows)):\n",
    "        sentences = list_of_articles[row]\n",
    "        filtered_sentences = list_of_filtered_articles[row]\n",
    "        summary_size = int(size * len(sentences)) if 0 < size < 1 else int(size)\n",
    "        summary = summary_algorithm(filtered_sentences, sentences, size=summary_size)\n",
    "        summarized_text.append(summary)\n",
    "    summary_df = pd.DataFrame(summarized_text, columns=[f'{summary_algorithm.__name__} summary'])\n",
    "    return summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [00:00<00:00, 4832.82it/s]\n"
     ]
    }
   ],
   "source": [
    "luhn_summary = summarize_with(filtered_sentences,sentences,luhn_algorithm,0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>luhn_algorithm summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LONDON, England (Reuters) -- Harry Potter star...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mentally ill people often won't do what they'r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MINNEAPOLIS, Minnesota (CNN) -- Drivers who we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>He planned to have lunch at Camp David and hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Vick said he would plead guilty to one count o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>The District of Columbia Court of Appeals \"rul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>Sheriff's spokesman Carlos Padilla said last w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>(CNN) -- Former first lady Barbara Bush was mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>All 115 people aboard the Continental Airlines...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>\"Iraq's non-Muslim religious minorities -- par...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 luhn_algorithm summary\n",
       "0     LONDON, England (Reuters) -- Harry Potter star...\n",
       "1     Mentally ill people often won't do what they'r...\n",
       "2     MINNEAPOLIS, Minnesota (CNN) -- Drivers who we...\n",
       "3     He planned to have lunch at Camp David and hav...\n",
       "4     Vick said he would plead guilty to one count o...\n",
       "...                                                 ...\n",
       "1495  The District of Columbia Court of Appeals \"rul...\n",
       "1496  Sheriff's spokesman Carlos Padilla said last w...\n",
       "1497  (CNN) -- Former first lady Barbara Bush was mo...\n",
       "1498  All 115 people aboard the Continental Airlines...\n",
       "1499  \"Iraq's non-Muslim religious minorities -- par...\n",
       "\n",
       "[1500 rows x 1 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "luhn_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficiency_scores import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [38:10<00:00,  1.53s/it]\n"
     ]
    }
   ],
   "source": [
    "luhn_scores = rouge_scores_df(cnn_mini_df,luhn_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1_fmeasure</th>\n",
       "      <th>rouge1_precision</th>\n",
       "      <th>rouge1_recall</th>\n",
       "      <th>rouge2_fmeasure</th>\n",
       "      <th>rouge2_precision</th>\n",
       "      <th>rouge2_recall</th>\n",
       "      <th>rougeL_fmeasure</th>\n",
       "      <th>rougeL_precision</th>\n",
       "      <th>rougeL_recall</th>\n",
       "      <th>rougeLsum_fmeasure</th>\n",
       "      <th>rougeLsum_precision</th>\n",
       "      <th>rougeLsum_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tensor(0.2500)</td>\n",
       "      <td>tensor(0.1525)</td>\n",
       "      <td>tensor(0.6923)</td>\n",
       "      <td>tensor(0.1776)</td>\n",
       "      <td>tensor(0.1080)</td>\n",
       "      <td>tensor(0.5000)</td>\n",
       "      <td>tensor(0.2315)</td>\n",
       "      <td>tensor(0.1412)</td>\n",
       "      <td>tensor(0.6410)</td>\n",
       "      <td>tensor(0.2407)</td>\n",
       "      <td>tensor(0.1469)</td>\n",
       "      <td>tensor(0.6667)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tensor(0.2207)</td>\n",
       "      <td>tensor(0.1320)</td>\n",
       "      <td>tensor(0.6735)</td>\n",
       "      <td>tensor(0.0741)</td>\n",
       "      <td>tensor(0.0442)</td>\n",
       "      <td>tensor(0.2292)</td>\n",
       "      <td>tensor(0.1338)</td>\n",
       "      <td>tensor(0.0800)</td>\n",
       "      <td>tensor(0.4082)</td>\n",
       "      <td>tensor(0.1338)</td>\n",
       "      <td>tensor(0.0800)</td>\n",
       "      <td>tensor(0.4082)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tensor(0.1164)</td>\n",
       "      <td>tensor(0.0684)</td>\n",
       "      <td>tensor(0.3902)</td>\n",
       "      <td>tensor(0.0220)</td>\n",
       "      <td>tensor(0.0129)</td>\n",
       "      <td>tensor(0.0750)</td>\n",
       "      <td>tensor(0.0655)</td>\n",
       "      <td>tensor(0.0385)</td>\n",
       "      <td>tensor(0.2195)</td>\n",
       "      <td>tensor(0.1091)</td>\n",
       "      <td>tensor(0.0641)</td>\n",
       "      <td>tensor(0.3659)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tensor(0.1266)</td>\n",
       "      <td>tensor(0.0746)</td>\n",
       "      <td>tensor(0.4167)</td>\n",
       "      <td>tensor(0.0256)</td>\n",
       "      <td>tensor(0.0150)</td>\n",
       "      <td>tensor(0.0870)</td>\n",
       "      <td>tensor(0.0506)</td>\n",
       "      <td>tensor(0.0299)</td>\n",
       "      <td>tensor(0.1667)</td>\n",
       "      <td>tensor(0.0886)</td>\n",
       "      <td>tensor(0.0522)</td>\n",
       "      <td>tensor(0.2917)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tensor(0.1183)</td>\n",
       "      <td>tensor(0.0669)</td>\n",
       "      <td>tensor(0.5122)</td>\n",
       "      <td>tensor(0.0397)</td>\n",
       "      <td>tensor(0.0224)</td>\n",
       "      <td>tensor(0.1750)</td>\n",
       "      <td>tensor(0.0676)</td>\n",
       "      <td>tensor(0.0382)</td>\n",
       "      <td>tensor(0.2927)</td>\n",
       "      <td>tensor(0.1070)</td>\n",
       "      <td>tensor(0.0605)</td>\n",
       "      <td>tensor(0.4634)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>tensor(0.1294)</td>\n",
       "      <td>tensor(0.0774)</td>\n",
       "      <td>tensor(0.3939)</td>\n",
       "      <td>tensor(0.0101)</td>\n",
       "      <td>tensor(0.0060)</td>\n",
       "      <td>tensor(0.0312)</td>\n",
       "      <td>tensor(0.0697)</td>\n",
       "      <td>tensor(0.0417)</td>\n",
       "      <td>tensor(0.2121)</td>\n",
       "      <td>tensor(0.0796)</td>\n",
       "      <td>tensor(0.0476)</td>\n",
       "      <td>tensor(0.2424)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>tensor(0.2927)</td>\n",
       "      <td>tensor(0.1818)</td>\n",
       "      <td>tensor(0.7500)</td>\n",
       "      <td>tensor(0.1148)</td>\n",
       "      <td>tensor(0.0711)</td>\n",
       "      <td>tensor(0.2979)</td>\n",
       "      <td>tensor(0.1626)</td>\n",
       "      <td>tensor(0.1010)</td>\n",
       "      <td>tensor(0.4167)</td>\n",
       "      <td>tensor(0.2520)</td>\n",
       "      <td>tensor(0.1566)</td>\n",
       "      <td>tensor(0.6458)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>tensor(0.3019)</td>\n",
       "      <td>tensor(0.2462)</td>\n",
       "      <td>tensor(0.3902)</td>\n",
       "      <td>tensor(0.1923)</td>\n",
       "      <td>tensor(0.1562)</td>\n",
       "      <td>tensor(0.2500)</td>\n",
       "      <td>tensor(0.2830)</td>\n",
       "      <td>tensor(0.2308)</td>\n",
       "      <td>tensor(0.3659)</td>\n",
       "      <td>tensor(0.2830)</td>\n",
       "      <td>tensor(0.2308)</td>\n",
       "      <td>tensor(0.3659)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>tensor(0.1799)</td>\n",
       "      <td>tensor(0.1037)</td>\n",
       "      <td>tensor(0.6757)</td>\n",
       "      <td>tensor(0.0507)</td>\n",
       "      <td>tensor(0.0292)</td>\n",
       "      <td>tensor(0.1944)</td>\n",
       "      <td>tensor(0.1079)</td>\n",
       "      <td>tensor(0.0622)</td>\n",
       "      <td>tensor(0.4054)</td>\n",
       "      <td>tensor(0.1655)</td>\n",
       "      <td>tensor(0.0954)</td>\n",
       "      <td>tensor(0.6216)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>tensor(0.1208)</td>\n",
       "      <td>tensor(0.0865)</td>\n",
       "      <td>tensor(0.2000)</td>\n",
       "      <td>tensor(0.0272)</td>\n",
       "      <td>tensor(0.0194)</td>\n",
       "      <td>tensor(0.0455)</td>\n",
       "      <td>tensor(0.0940)</td>\n",
       "      <td>tensor(0.0673)</td>\n",
       "      <td>tensor(0.1556)</td>\n",
       "      <td>tensor(0.1074)</td>\n",
       "      <td>tensor(0.0769)</td>\n",
       "      <td>tensor(0.1778)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     rouge1_fmeasure rouge1_precision   rouge1_recall rouge2_fmeasure   \n",
       "0     tensor(0.2500)   tensor(0.1525)  tensor(0.6923)  tensor(0.1776)  \\\n",
       "1     tensor(0.2207)   tensor(0.1320)  tensor(0.6735)  tensor(0.0741)   \n",
       "2     tensor(0.1164)   tensor(0.0684)  tensor(0.3902)  tensor(0.0220)   \n",
       "3     tensor(0.1266)   tensor(0.0746)  tensor(0.4167)  tensor(0.0256)   \n",
       "4     tensor(0.1183)   tensor(0.0669)  tensor(0.5122)  tensor(0.0397)   \n",
       "...              ...              ...             ...             ...   \n",
       "1495  tensor(0.1294)   tensor(0.0774)  tensor(0.3939)  tensor(0.0101)   \n",
       "1496  tensor(0.2927)   tensor(0.1818)  tensor(0.7500)  tensor(0.1148)   \n",
       "1497  tensor(0.3019)   tensor(0.2462)  tensor(0.3902)  tensor(0.1923)   \n",
       "1498  tensor(0.1799)   tensor(0.1037)  tensor(0.6757)  tensor(0.0507)   \n",
       "1499  tensor(0.1208)   tensor(0.0865)  tensor(0.2000)  tensor(0.0272)   \n",
       "\n",
       "     rouge2_precision   rouge2_recall rougeL_fmeasure rougeL_precision   \n",
       "0      tensor(0.1080)  tensor(0.5000)  tensor(0.2315)   tensor(0.1412)  \\\n",
       "1      tensor(0.0442)  tensor(0.2292)  tensor(0.1338)   tensor(0.0800)   \n",
       "2      tensor(0.0129)  tensor(0.0750)  tensor(0.0655)   tensor(0.0385)   \n",
       "3      tensor(0.0150)  tensor(0.0870)  tensor(0.0506)   tensor(0.0299)   \n",
       "4      tensor(0.0224)  tensor(0.1750)  tensor(0.0676)   tensor(0.0382)   \n",
       "...               ...             ...             ...              ...   \n",
       "1495   tensor(0.0060)  tensor(0.0312)  tensor(0.0697)   tensor(0.0417)   \n",
       "1496   tensor(0.0711)  tensor(0.2979)  tensor(0.1626)   tensor(0.1010)   \n",
       "1497   tensor(0.1562)  tensor(0.2500)  tensor(0.2830)   tensor(0.2308)   \n",
       "1498   tensor(0.0292)  tensor(0.1944)  tensor(0.1079)   tensor(0.0622)   \n",
       "1499   tensor(0.0194)  tensor(0.0455)  tensor(0.0940)   tensor(0.0673)   \n",
       "\n",
       "       rougeL_recall rougeLsum_fmeasure rougeLsum_precision rougeLsum_recall  \n",
       "0     tensor(0.6410)     tensor(0.2407)      tensor(0.1469)   tensor(0.6667)  \n",
       "1     tensor(0.4082)     tensor(0.1338)      tensor(0.0800)   tensor(0.4082)  \n",
       "2     tensor(0.2195)     tensor(0.1091)      tensor(0.0641)   tensor(0.3659)  \n",
       "3     tensor(0.1667)     tensor(0.0886)      tensor(0.0522)   tensor(0.2917)  \n",
       "4     tensor(0.2927)     tensor(0.1070)      tensor(0.0605)   tensor(0.4634)  \n",
       "...              ...                ...                 ...              ...  \n",
       "1495  tensor(0.2121)     tensor(0.0796)      tensor(0.0476)   tensor(0.2424)  \n",
       "1496  tensor(0.4167)     tensor(0.2520)      tensor(0.1566)   tensor(0.6458)  \n",
       "1497  tensor(0.3659)     tensor(0.2830)      tensor(0.2308)   tensor(0.3659)  \n",
       "1498  tensor(0.4054)     tensor(0.1655)      tensor(0.0954)   tensor(0.6216)  \n",
       "1499  tensor(0.1556)     tensor(0.1074)      tensor(0.0769)   tensor(0.1778)  \n",
       "\n",
       "[1500 rows x 12 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "luhn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "luhn_avg = df_avg_by_column(luhn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rouge1_fmeasure        tensor(0.2153)\n",
       "rouge1_precision       tensor(0.1442)\n",
       "rouge1_recall          tensor(0.5482)\n",
       "rouge2_fmeasure        tensor(0.0767)\n",
       "rouge2_precision       tensor(0.0519)\n",
       "rouge2_recall          tensor(0.1905)\n",
       "rougeL_fmeasure        tensor(0.1361)\n",
       "rougeL_precision       tensor(0.0916)\n",
       "rougeL_recall          tensor(0.3441)\n",
       "rougeLsum_fmeasure     tensor(0.1845)\n",
       "rougeLsum_precision    tensor(0.1235)\n",
       "rougeLsum_recall       tensor(0.4713)\n",
       "dtype: object"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "luhn_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "luhn_scores.to_csv('cnn_luhn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Original Articles', 'Original Summary'], dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_mini_df_loaded.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average length of the original summaries over the entire dataset is 5.06 sentences.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Calculate the number of summary sentences for each article\n",
    "num_summary_sentences = [len(article.split('.')) for article in cnn_mini_df['Original Summary']]\n",
    "\n",
    "# Calculate the average number of summary sentences per article\n",
    "avg_num_summary_sentences = sum(num_summary_sentences) / len(num_summary_sentences)\n",
    "\n",
    "# Print the result\n",
    "print(f\"The average length of the original summaries over the entire dataset is {avg_num_summary_sentences:.2f} sentences.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum summary length: 13\n",
      "Minimum summary length: 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get the number of summary sentences for each article\n",
    "\n",
    "# Calculate the number of summary sentences for each article\n",
    "summary_lengths = [len(article.split('.')) for article in cnn_mini_df['Original Summary']]\n",
    "max_summary_length = max(summary_lengths)\n",
    "min_summary_length = min(summary_lengths)\n",
    "\n",
    "# Get the maximum and minimum length of summary sentences in the dataset\n",
    "max_summary_length = max(summary_lengths)\n",
    "min_summary_length = min(summary_lengths)\n",
    "\n",
    "# Print the results\n",
    "print(\"Maximum summary length:\", max_summary_length)\n",
    "print(\"Minimum summary length:\", min_summary_length)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
