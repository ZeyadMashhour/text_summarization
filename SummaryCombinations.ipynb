{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4957ab89-14bd-4278-9f0e-08a43dfb6cef",
   "metadata": {},
   "source": [
    "# BASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be4a7e02-76f1-4fde-bfc1-c5d46afbe828",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Specialist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Big File...\n",
      "Finished Loading Big File.\n"
     ]
    }
   ],
   "source": [
    "# %run summarizationBackbone.ipynb\n",
    "%run summarizationBackboneDocLvl.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f672ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_package.text_processing import *\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6aa96b13-e850-4a11-b771-c90a55d0f7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, percentage):\n",
    "    try:\n",
    "        sentences, filtered_sentences = preprocessing_text_with_spacy(text,True,False)\n",
    "        df = buildDF(filtered_sentences, sentences)\n",
    "        summarizedRow = {\"Original Article\": ds[articleCol][i]}\n",
    "        for key in df.keys():\n",
    "            element = summarizeWith(sentences, df, key, percentage)\n",
    "            summarizedRow[key] = element\n",
    "        return summarizedRow\n",
    "    except Exception as e:\n",
    "        print(\"Error\",i, e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bd9740f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Summarization Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c00f58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'DUC\\duc\\Duc_dataset_first_ref_summary.csv'\n",
    "ds = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6cb9aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Column Names\n",
    "articleCol = \"Original Article\"\n",
    "summaryCol = \"Original Summary\"\n",
    "# Documents to be summarized\n",
    "start = 0\n",
    "end = len(ds)\n",
    "# Summarization Percentage [0-1]% if > 1 acts as number of sentences\n",
    "percentage = 5\n",
    "# Pre processing settings\n",
    "lemmatization = True\n",
    "remove_stopwords = True\n",
    "# Normalization Type\n",
    "isNormOnDataset = True \n",
    "# output file name [MAKE SURE TO CHANGE TO NOT OVERWRITE]\n",
    "outFileName = \"DUC_1st_5Sent_lemT_swT.csv\"\n",
    "ppFileName = \"PP_DUC_1st_5Sent_lemT_swT.csv\"\n",
    "ppSpecial = \"#!@!#\"\n",
    "# Weights Settings\n",
    "useWeights = True\n",
    "weights = {\"tm\":0.92,\"luhn\":1,\"lsa\":0.44,\"tr\":0.84,\"lex\":0.88,\"lda\":0.8}\n",
    "# doc scoring\n",
    "docScoringFolder = \"BBC_Sentence_Score\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af32bf3d-e8eb-4b98-9f87-df6e64b11ae4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "Algorithms = [\"Tm\",\"Lex\",\"Luhn\",\"Lsa\",\"Tr\",\"LDA\"]\n",
    "lstCols = get_combinations(Algorithms)\n",
    "cols = [\"Original Article\",\"Original Summary\"] + lstCols\n",
    "df3 = pd.DataFrame(columns = cols)\n",
    "df3.to_csv(outFileName, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "16c1f36d-3745-4111-9bc1-47a3505b5b22",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c908dbc7-2103-43d0-9b9e-2bfd15cef295",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 23/49 [01:08<01:18,  3.01s/it]"
     ]
    }
   ],
   "source": [
    "# sentences, processed_sentences = process_one_column_df(ds[articleCol],lemmatization,remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbed59b-0851-4b57-901f-0e13bf319f04",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# collected = []\n",
    "# for i in range(len(sentences)):\n",
    "#     collected.append([ds[articleCol][i],ds[summaryCol][i],ppSpecial.join(sentences[i]),ppSpecial.join(processed_sentences[i])])\n",
    "# df = pd.DataFrame(collected, columns =[articleCol,summaryCol,\"Sentences\",\"Filtered Sent\"]) \n",
    "# df.to_csv(ppFileName, index=False)\n",
    "# data = pd.read_csv(ppFileName)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "323f0948-8d29-4414-8ccc-00948524e45a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# one by one docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cf673d-6f12-4d33-aeae-ac8e3ee27f61",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def onebyoneSummarization(ds):\n",
    "#     summarizedDataset = []\n",
    "#     for i in tqdm(range(start,end,1)):\n",
    "#         try:\n",
    "#             # sentences, filtered_sentences = preprocessing_text_with_spacy(ds[articleCol][i],lemmatization,remove_stopwords)\n",
    "#             # Read Pre Processed Data\n",
    "#             data = pd.read_csv(ppFileName)\n",
    "#             sentences = data[\"Sentences\"][i].split(ppSpecial)\n",
    "#             filtered_sentences = data[\"Filtered Sent\"][i].split(ppSpecial)\n",
    "#             # build combination dataframe\n",
    "#             df = buildDF(filtered_sentences, sentences, useWeights, weights)\n",
    "\n",
    "#             summarizedRow = {\"Original Article\": ds[articleCol][i],\"Original Summary\": ds[summaryCol][i]}\n",
    "#             for key in df.keys():\n",
    "#                 element = summarizeWith(sentences, df, key, percentage)\n",
    "#                 summarizedRow[key] = element\n",
    "#             pd.DataFrame.from_dict(summarizedRow, orient='index').T.to_csv(outFileName,mode='a',header=False,index=False)\n",
    "#             # break\n",
    "#         except Exception as e:\n",
    "#             print(\"Error\",i, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfabaaa-67f6-4b44-87f1-fddff60a2e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [59:45<00:00, 73.17s/it]   \n"
     ]
    }
   ],
   "source": [
    "# onebyoneSummarization(ds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ccf8bd1c-6fae-4d5d-a003-af9f79323608",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Doc scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3d3b97-4d18-453c-af11-b3317cd8e030",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstCols = Algorithms\n",
    "data = pd.read_csv(ppFileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9914a36-385a-4e41-827b-49789361845b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [52:37<00:00, 64.44s/it] \n"
     ]
    }
   ],
   "source": [
    "def documentScoring(sentences,filtered_sentences):\n",
    "    maxes = []\n",
    "    mines = []\n",
    "    for i in tqdm(range(start,end,1)):\n",
    "        try:\n",
    "            # sentences = data[\"Sentences\"][i].split(ppSpecial)\n",
    "            # filtered_sentences = data[\"Filtered Sent\"][i].split(ppSpecial)\n",
    "            df = buildDF(filtered_sentences, sentences, useWeights, weights)\n",
    "            df.to_csv(docScoringFolder+\"/Document \" + str(i) + \".csv\", index=False)\n",
    "            maxes.append([df[Algorithms[0]].max(),df[Algorithms[1]].max(),df[Algorithms[2]].max(),df[Algorithms[3]].max(),df[Algorithms[4]].max(),df[Algorithms[5]].max()])\n",
    "            mines.append([df[Algorithms[0]].min(),df[Algorithms[1]].min(),df[Algorithms[2]].min(),df[Algorithms[3]].min(),df[Algorithms[4]].min(),df[Algorithms[5]].min()])\n",
    "        except Exception as e:\n",
    "            print(\"Error\",i, e)\n",
    "    df = pd.DataFrame(maxes, columns = Algorithms) \n",
    "    df.to_csv(docScoringFolder+\"/max.csv\", index=False)\n",
    "    df = pd.DataFrame(mines, columns = Algorithms) \n",
    "    df.to_csv(docScoringFolder+\"/min.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "170d6373-4633-44ed-a4f2-f786efafd164",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16332642-b984-4bb8-a0a1-8b739f660233",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxdf = pd.read_csv(docScoringFolder+\"/max.csv\")\n",
    "mindf = pd.read_csv(docScoringFolder+\"/min.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349b56c7-db7f-45ce-8549-39929b3290b1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "absmax = [maxdf[Algorithms[0]].max(),maxdf[Algorithms[1]].max(),maxdf[Algorithms[2]].max(),maxdf[Algorithms[3]].max(),maxdf[Algorithms[4]].max(),maxdf[Algorithms[5]].max()]\n",
    "absmin = [mindf[Algorithms[0]].min(),mindf[Algorithms[1]].min(),mindf[Algorithms[2]].min(),mindf[Algorithms[3]].min(),mindf[Algorithms[4]].min(),mindf[Algorithms[5]].min()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fff39fd-701d-4701-8767-f8097ee8f795",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(values, min_value, max_value):\n",
    "    normalized_values = [(x - min_value) / (max_value - min_value) for x in values]\n",
    "    return normalized_values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "91612f11-1a3a-46c4-8b10-6f90b18961f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "Main Function for dataset level norm (Merged on next cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f43594-2b7c-4ad6-9b9d-2aec312fdf10",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "    # # dataset = []\n",
    "    # for i in tqdm(range(start,end,1)):\n",
    "    #     # try:\n",
    "    #         # Normalization\n",
    "    #         df = pd.read_csv(\"DUC Sentence Score by Document/Document \"+ str(i) +\".csv\")\n",
    "    #         norm2dlist = pd.DataFrame()\n",
    "    #         norm2dlist[\"Sent\"] = df[\"Sent\"]\n",
    "    #         for key in range(len(Algorithms)):\n",
    "    #             norm2dlist[Algorithms[key]] = normalize(df[Algorithms[key]],absmin[key],absmax[key])\n",
    "    #         # Combinations  \n",
    "    #         combdf = allCombsDf(norm2dlist)\n",
    "    #         # Summarization\n",
    "    #         summarizedRow = {\"Original Article\": ds[articleCol][i],\"Original Summary\": ds[summaryCol][i]}\n",
    "    #         for key in combdf.keys():\n",
    "    #             element = summarizeWith(df[\"Sent\"], combdf, key, percentage)\n",
    "    #             summarizedRow[key] = element\n",
    "    #         pd.DataFrame.from_dict(summarizedRow, orient='index').T.to_csv(outFileName,mode='a',header=False,index=False)\n",
    "    #     # except Exception as e:\n",
    "    #     #     print(\"Error\",i, e)\n",
    "    #     #     break\n",
    "    #     # dataset.append(norm2dlist)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d1104c5b-69db-40f4-9b3b-0692aaeda4f9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee92620-7c9f-46be-a5cc-6f0fc4fb208e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [00:13<00:00,  3.67it/s]\n"
     ]
    }
   ],
   "source": [
    "    # dataset = []\n",
    "    for i in tqdm(range(start,end,1)):\n",
    "        # try:\n",
    "            # Normalization\n",
    "            df = pd.read_csv(docScoringFolder+\"/Document \"+ str(i) +\".csv\")\n",
    "            normdf = pd.DataFrame()\n",
    "            normdf[\"Sent\"] = df[\"Sent\"]\n",
    "            for key in range(len(Algorithms)):\n",
    "                if(isNormOnDataset):\n",
    "                    normMax = absmax[key]\n",
    "                    normMin = absmin[key]\n",
    "                else:\n",
    "                    normMax = df[Algorithms[key]].max()\n",
    "                    normMin = df[Algorithms[key]].min()\n",
    "                normdf[Algorithms[key]] = normalize(df[Algorithms[key]],normMin,normMax)\n",
    "            # Combinations  \n",
    "            combdf = allCombsDf(normdf)\n",
    "            # Summarization\n",
    "            summarizedRow = {\"Original Article\": ds[articleCol][i],\"Original Summary\": ds[summaryCol][i]}\n",
    "            for key in combdf.keys():\n",
    "                element = summarizeWith(df[\"Sent\"], combdf, key, percentage)\n",
    "                summarizedRow[key] = element\n",
    "            pd.DataFrame.from_dict(summarizedRow, orient='index').T.to_csv(outFileName,mode='a',header=False,index=False)\n",
    "        # except Exception as e:\n",
    "        #     print(\"Error\",i, e)\n",
    "        #     break\n",
    "        # dataset.append(norm2dlist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
