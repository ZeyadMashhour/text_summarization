{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "def bleu_eval(ref, hyp):\n",
    "    \"\"\"\n",
    "    Computes the BLEU score for a text summary.\n",
    "\n",
    "    Args:\n",
    "        ref: A list of reference sentences.\n",
    "        hyp: The summary sentence to be evaluated.\n",
    "\n",
    "    Returns:\n",
    "        The BLEU score for the summary.\n",
    "    \"\"\"\n",
    "    # Tokenize the reference and summary sentences\n",
    "    ref_tokens = [nltk.word_tokenize(sent.lower()) for sent in ref]\n",
    "    hyp_tokens = nltk.word_tokenize(hyp.lower())\n",
    "\n",
    "    # Compute the BLEU score for the summary\n",
    "    weights = [0.25] * 4  # weights for BLEU-4\n",
    "    smoothing_fn = SmoothingFunction().method1  # use method1 smoothing\n",
    "    bleu_score = sentence_bleu(ref_tokens, hyp_tokens, weights, smoothing_function=smoothing_fn)\n",
    "\n",
    "    return bleu_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def meteor_eval(ref, hyp):\n",
    "    \"\"\"\n",
    "    Computes the METEOR score for a text summary.\n",
    "\n",
    "    Args:\n",
    "        ref: A list of reference sentences.\n",
    "        hyp: The summary sentence to be evaluated.\n",
    "\n",
    "    Returns:\n",
    "        The METEOR score for the summary.\n",
    "    \"\"\"\n",
    "    # Tokenize the reference and summary sentences\n",
    "    ref_tokens = [nltk.word_tokenize(sent.lower()) for sent in ref]\n",
    "    hyp_tokens = nltk.word_tokenize(hyp.lower())\n",
    "\n",
    "    # Compute the METEOR score for the summary\n",
    "    meteor_score = nltk.translate.meteor_score.meteor_score(ref_tokens, hyp_tokens)\n",
    "\n",
    "    return meteor_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 0.13512001548070346\n",
      "MEREOR score: 0.5260416666666666\n"
     ]
    }
   ],
   "source": [
    "ref = [\"The cat sat on the mat.\", \"The dog ate my homework.\"]\n",
    "hyp = \"The cat ate the dog.\"\n",
    "\n",
    "bleu_score = bleu_eval(ref, hyp)\n",
    "print(\"BLEU score:\", bleu_score)\n",
    "\n",
    "\n",
    "meteor_score = meteor_eval(ref, hyp)\n",
    "print(\"MEREOR score:\", meteor_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
