{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2358b1a8",
   "metadata": {},
   "source": [
    "<h1> imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6549e121",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_package.text_processing import *\n",
    "from my_package.summary_evalution import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "946228a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm as tqdm\n",
    "import itertools\n",
    "#lda\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c154778c-9df1-4595-98b1-f7e5989b094a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "from lexrank.utils.text import tokenize\n",
    "from lexrank.mappings.stopwords import STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd10fde3-67ff-4f98-8c39-b532d3c095f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open(\"SummaryInput.txt\", \"r\", encoding=\"utf8\")\n",
    "# text = \"\"\n",
    "# for line in f:\n",
    "#     text += line.replace('!','.').replace('?','.').replace('\\n',' ')\n",
    "# f.close()\n",
    "# filtered_sentences,sentences = tokenization(text)  # Tokenize the text into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe4b734d-cc74-491b-91a1-cf498889c282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = text_matching(filtered_sentences, sentences)\n",
    "# buildDF(filtered_sentences, sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11a8f555-f23a-49ae-b781-d76c7bc8bc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensemble_dic = getTextRanks(filtered_sentences, sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23f01c93-a6c2-4e44-8f96-aac9d8042296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensemble_dic[0][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd40aa51",
   "metadata": {},
   "source": [
    "<h1> Pre Processing </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48ac4705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    filtered_sentences = remove_stop_words(sentences)\n",
    "    return filtered_sentences,sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fee1d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(sentences):\n",
    "    # define a set of stop words \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_sentences = []\n",
    "    for sentence in sentences:\n",
    "        # tokenize each sentence into words\n",
    "        word_tokens = word_tokenize(sentence)\n",
    "        # remove all stop words from each sentence\n",
    "        filtered_words = [word for word in word_tokens if not word in stop_words]\n",
    "        # join all filtered words back into a single sentence\n",
    "        filtered_sentence = ' '.join(filtered_words)\n",
    "        filtered_sentences.append(filtered_sentence)\n",
    "    return filtered_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "749dd8d5-5d86-4f58-8242-e7b5be0930d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(lst):\n",
    "    maxScore = max(lst)    \n",
    "    sortedScored = list(reversed(sorted(lst)))\n",
    "    \n",
    "    for i in range(len(lst)):\n",
    "        sentence_map[i].append(lst[i] / maxScore)\n",
    "        sentence_map[i].append(sortedScored.index(sent_scores[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dc771a",
   "metadata": {},
   "source": [
    "<h1> Text Matching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22053e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_matching(filtered_sentences, sentences):\n",
    "    word_frequencies = {}  # Create an empty dictionary to store the word frequencies\n",
    "    for sentence in filtered_sentences:  # Loop through each sentence in the text\n",
    "        words = nltk.word_tokenize(sentence)  # Tokenize each sentence into words\n",
    "        for word in words:  # Loop through each word in the sentence\n",
    "            if word not in word_frequencies.keys():  # Check if the word is already in the dictionary\n",
    "                word_frequencies[word] = 1  # If not, set its count to 1\n",
    "            else:\n",
    "                word_frequencies[word] += 1  # If yes, increment its count by 1\n",
    "    summary = []  # Create an empty list to store the summary sentences\n",
    "    sentence_map = {} # Create an empty dictionary to store the sentence scores\n",
    "    sent_scores = []\n",
    "    sent_index = 0\n",
    "    for sentence in sentences:  # Loop through each sentence in the text again\n",
    "        words = nltk.word_tokenize(sentence)  # Tokenize each sentence into words again\n",
    "        score = 0  # Initialize a score variable to 0\n",
    "        for word in words:  # Loop through each word in the sentence\n",
    "            if word in word_frequencies.keys():  # Check if the current word is present in our dictionary of word frequencies\n",
    "                score += word_frequencies[word]  # If yes, add its frequency to our score variable\n",
    "        sentence_map[sent_index] = [sentence,score]\n",
    "        sent_scores.append(score)\n",
    "        sent_index += 1\n",
    "    return sentence_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11eed9d",
   "metadata": {},
   "source": [
    "<h1> Luhn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56ac7536",
   "metadata": {},
   "outputs": [],
   "source": [
    "def luhn_algorithm(filtered_sentences, sentences):\n",
    "    # timeNow(\"Starting Luhn\")\n",
    "    # Initialize a list to store the summary\n",
    "    summary = []\n",
    "    # # Split the text into sentences\n",
    "    # sentences = text.split('.')\n",
    "    # Initialize a list to store the sentence scores\n",
    "    sentence_scores = []\n",
    "    sentence_map = {}\n",
    "    sent_index = 0\n",
    "    # Iterate through each sentence\n",
    "    for sentence in filtered_sentences:\n",
    "        # Split the sentence into words\n",
    "        words = sentence.split()\n",
    "        # Initialize a score for the sentence\n",
    "        score = 0\n",
    "        # Iterate through each word\n",
    "        for word in words:\n",
    "            # Calculate the score for the word\n",
    "            score += len(word)\n",
    "        # Add the score to the sentence scores list\n",
    "        sentence_scores.append(score)\n",
    "        sentence_map[sent_index] = [sentences[sent_index],score]\n",
    "        sent_index += 1\n",
    "    return sentence_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5953c6cc",
   "metadata": {},
   "source": [
    "<h1> Latent Semantic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eadf61f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_idf(filtered_sentences):\n",
    "    tfidfconverter = TfidfVectorizer()\n",
    "    X = tfidfconverter.fit_transform(filtered_sentences).toarray()\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e28c9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsa_algorithm(X):\n",
    "    svdmodel = TruncatedSVD(n_components=2)\n",
    "    svdmodel.fit(X)\n",
    "    result = svdmodel.transform(X)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ecd5caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsa_summarization(filtered_sentences, sentences):\n",
    "    # timeNow(\"Starting LSA\")\n",
    "    # sentences = tokenization(text)\n",
    "    # filtered_sentences = remove_stop_words(sentences)\n",
    "    X = create_tf_idf(filtered_sentences)\n",
    "    result = lsa_algorithm(X)\n",
    "    scores = result[:,1]\n",
    "    summary = \"\"\n",
    "    sentence_map = {}\n",
    "    for i in range (len (scores)):\n",
    "        sentence_map[i] = [sentences[i], scores[i]]\n",
    "    # timeNow(\"Finished LSA\")\n",
    "    return sentence_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353a70df",
   "metadata": {},
   "source": [
    "<h1> Text Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e98e9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4a5d957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Big File...\n",
      "Finished Loading Big File.\n"
     ]
    }
   ],
   "source": [
    "if not(word_embeddings):\n",
    "    print(\"Loading Big File...\")\n",
    "    f = open('glove.6B.100d.txt', encoding='utf-8')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        word_embeddings[word] = coefs\n",
    "    f.close()\n",
    "    print(\"Finished Loading Big File.\")\n",
    "def textRank(filtered_sentences, sentences):\n",
    "    # timeNow(\"Starting Text Rank\")\n",
    "    sentence_vectors = []\n",
    "    for i in filtered_sentences:\n",
    "        if len(i) != 0:\n",
    "            v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "        else:\n",
    "            v = np.zeros((100))\n",
    "        sentence_vectors.append(v)\n",
    "\n",
    "    # similarity matrix\n",
    "    sim_mat = np.zeros([len(sentences), len(sentences)])\n",
    "\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(len(sentences)):\n",
    "            if i != j:\n",
    "                sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]   \n",
    "    #begining Graph stap\n",
    "    import networkx as nx\n",
    "    nx_graph = nx.from_numpy_array(sim_mat)\n",
    "    scores = nx.pagerank(nx_graph)\n",
    "    sent_map = {}\n",
    "    for index in range(len(sentences)):\n",
    "        sent_map[index] = [sentences[index] , scores[index]]\n",
    "    #ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
    "    # timeNow(\"Finished Text Rank\")\n",
    "    return sent_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5264d8c2-a10f-409f-b7a2-7c1f2b4a06a6",
   "metadata": {},
   "source": [
    "<h1> Lex Rank </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0314214-ab42-4945-83da-f0703d9175c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LexRank_algorithm(filtered_sentences,sentences,size=5,threshold = 0.095):\n",
    "    tfidfconverter = TfidfVectorizer()\n",
    "    tf_idf = tfidfconverter.fit_transform(filtered_sentences).toarray()\n",
    "    length = len(tf_idf)\n",
    "    similarity_matrix = np.zeros([length] * 2)\n",
    "    \n",
    "    for i in range(length):\n",
    "        for j in range(i, length):\n",
    "            similarity = cosine_similarity(tf_idf[i],tf_idf[j],i,j)\n",
    "\n",
    "            if similarity:\n",
    "                similarity_matrix[i, j] = similarity\n",
    "                similarity_matrix[j, i] = similarity    \n",
    "    \n",
    "    def get_summary(sentences,similarity_matrix,threshold,summary_size=1):\n",
    "\n",
    "        if not isinstance(summary_size, int) or summary_size < 1:\n",
    "            raise ValueError('\\'summary_size\\' should be a positive integer')\n",
    "\n",
    "        lex_scores = rank_sentences(sentences,similarity_matrix,threshold)\n",
    "\n",
    "        sorted_ix = np.argsort(lex_scores)[::-1]\n",
    "\n",
    "        summary_index=[]\n",
    "        for i in sorted_ix[:summary_size]:\n",
    "            summary_index.append(i)\n",
    "        #print(summary_index)\n",
    "        return lex_scores,summary_index\n",
    "\n",
    "    scores , summary_index = get_summary(sentences,similarity_matrix,threshold,size)\n",
    "\n",
    "    sentence_map = {}\n",
    "    for i in range (len (scores)):\n",
    "        sentence_map[i] = [sentences[i], scores[i]]\n",
    "    return sentence_map\n",
    "\n",
    "def connected_nodes(matrix):\n",
    "    _, labels = connected_components(matrix)\n",
    "    z = csr_matrix(matrix)\n",
    "    groups = []\n",
    "    for tag in np.unique(labels):\n",
    "        group = np.where(labels == tag)[0]\n",
    "        groups.append(group)\n",
    "    return groups\n",
    "\n",
    "def cosine_similarity(list_1, list_2,i,j):\n",
    "        if i == j :\n",
    "            return 1\n",
    "        dot = np.dot(list_1, list_2)\n",
    "        if math.isclose(dot, 0):\n",
    "            return 0\n",
    "        norm = (np.linalg.norm(list_1) * np.linalg.norm(list_2))\n",
    "        cos_sim = dot / norm\n",
    "        return cos_sim\n",
    "    \n",
    "    \n",
    "def stationary_distribution(transition_matrix,normalized=True):\n",
    "    n_1, n_2 = transition_matrix.shape\n",
    "    if n_1 != n_2:\n",
    "        raise ValueError('\\'transition_matrix\\' should be square')\n",
    "\n",
    "    distribution = np.zeros(n_1)\n",
    "    grouped_indices = connected_nodes(transition_matrix)\n",
    "\n",
    "    for group in grouped_indices:\n",
    "        t_matrix = transition_matrix[np.ix_(group, group)]\n",
    "        eigenvector = _power_method(t_matrix)\n",
    "        distribution[group] = eigenvector\n",
    "    if normalized:\n",
    "        distribution /= n_1\n",
    "    return distribution\n",
    "\n",
    "def _power_method(transition_matrix):\n",
    "    sentences_count = len(transition_matrix)\n",
    "    eigenvector = np.ones(sentences_count)\n",
    "    if len(eigenvector) == 1:\n",
    "        return eigenvector\n",
    "    transposed_matrix = transition_matrix.T\n",
    "    lambda_val = 1.0\n",
    "\n",
    "    while np.allclose(lambda_val, eigenvector):\n",
    "        eigenvector_next = np.dot(transposed_matrix, eigenvector)\n",
    "        lambda_val = np.linalg.norm(np.subtract(eigenvector_next, eigenvector))\n",
    "        eigenvector = eigenvector_next\n",
    "    return eigenvector\n",
    "\n",
    "\n",
    "def create_markov_matrix(weights_matrix):\n",
    "    n_1, n_2 = weights_matrix.shape\n",
    "    if n_1 != n_2:\n",
    "        raise ValueError('\\'weights_matrix\\' should be square')\n",
    "\n",
    "    row_sum = weights_matrix.sum(axis=1, keepdims=True)\n",
    "\n",
    "    return weights_matrix / row_sum\n",
    "\n",
    "def create_markov_matrix_discrete(weights_matrix, threshold):\n",
    "    discrete_weights_matrix = weights_matrix#np.zeros(weights_matrix.shape)\n",
    "    #print(discrete_weights_matrix)\n",
    "    ixs = np.where(weights_matrix >= threshold)\n",
    "    discrete_weights_matrix[ixs] = 1\n",
    "    #print(discrete_weights_matrix)\n",
    "\n",
    "    return create_markov_matrix(discrete_weights_matrix)\n",
    "\n",
    "def degree_centrality_scores(similarity_matrix,threshold=None,increase_power=True):\n",
    "    if not (threshold is None or isinstance(threshold, float) and 0 <= threshold < 1):\n",
    "        raise ValueError(\n",
    "            '\\'threshold\\' should be a floating-point number '\n",
    "            'from the interval [0, 1) or None')\n",
    "\n",
    "    if threshold is None:\n",
    "        markov_matrix = create_markov_matrix(similarity_matrix)\n",
    "\n",
    "    else:\n",
    "        markov_matrix = create_markov_matrix_discrete(similarity_matrix,threshold)\n",
    "\n",
    "    scores = stationary_distribution(markov_matrix,normalized=True)\n",
    "    return scores\n",
    "\n",
    "def rank_sentences(sentences,similarity_matrix,threshold=0.03):  \n",
    "    scores = degree_centrality_scores(similarity_matrix,threshold)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f0e2ab-75a9-40fd-82cf-d6b596678f8a",
   "metadata": {},
   "source": [
    "<h1> LDA </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1cbb38b-4a6f-4299-989c-b53e03ac072c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_algorithm(filtered_sentences,sentences,num_topics = 3):\n",
    "    # Create a CountVectorizer object to preprocess the text\n",
    "    vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "    # Fit and transform the vectorizer to the text\n",
    "    doc_term_matrix = vectorizer.fit_transform(filtered_sentences)\n",
    "\n",
    "    # Create an LDA object using Gibbs sampling\n",
    "    lda = LatentDirichletAllocation(n_components=num_topics, learning_method='batch', random_state=42)\n",
    "\n",
    "    # Fit the LDA model to the document-term matrix\n",
    "    lda.fit(doc_term_matrix)\n",
    "\n",
    "    # Extract the topic summaries\n",
    "    topic_word_distributions = lda.components_\n",
    "    for i, topic_dist in enumerate(topic_word_distributions):\n",
    "        topic_words = [vectorizer.get_feature_names_out()[j] for j in topic_dist.argsort()[:-6:-1]]\n",
    "        #print(f\"Topic {i}: {' '.join(topic_words)}\")\n",
    "\n",
    "    # Extract the topic distribution for each sentence\n",
    "    sentence_topic_distributions = lda.transform(doc_term_matrix)\n",
    "\n",
    "    # Calculate the sentence scores\n",
    "    sentence_scores = sentence_topic_distributions.max(axis=1)\n",
    "\n",
    "    # Sort the sentences by score in descending order\n",
    "    sorted_indices = sentence_scores.argsort()[::-1]\n",
    "\n",
    "    # Extract the top N sentences\n",
    "    # top_indices = sorted_indices[:size]\n",
    "    # top_sentences = [sentences[i] for i in top_indices]\n",
    "\n",
    "    # Print the summary\n",
    "    #print('\\n'.join(top_sentences))\n",
    "    sent_map = {}\n",
    "    for i in range(len(sentences)):\n",
    "        sent_map[i] = [sentences[i],sentence_scores[i],sorted_indices[i]]\n",
    "        \n",
    "    return sent_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d07358",
   "metadata": {},
   "source": [
    "<h1> Summarizing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2695a682-7afc-46e8-8fc9-fc3bded6af73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createEnsembledic(tm_dic,luhn_dic,lsa_dic,tr_dic,lex_dic, lda_dic):\n",
    "    ensemble_dic = {}\n",
    "    finalScores = []\n",
    "    for key in tm_dic:\n",
    "        final_score = tm_dic[key][1] + luhn_dic[key][1]+ lsa_dic[key][1] + tr_dic[key][1] + lex_dic[key][1] + lda_dic[key][1]\n",
    "        finalScores.append(final_score)\n",
    "        scores_arr = [tm_dic[key][1],luhn_dic[key][1],lsa_dic[key][1],tr_dic[key][1],lex_dic[key][1],lda_dic[key][1]]\n",
    "        sent = tm_dic[key][0]\n",
    "        ensemble_dic[key] = [final_score, scores_arr, sent]\n",
    "    return ensemble_dic,finalScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df9a3251-bf35-49e6-ae46-de89cd994402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensembleSummary(ensemble_dic,finalScores,size):\n",
    "    finalScores = list(reversed(sorted(finalScores)))[:]\n",
    "    summaryScores = finalScores[:size] # number of summary sentences\n",
    "    summary = []\n",
    "    for key in ensemble_dic:\n",
    "        if(ensemble_dic[key][0] in summaryScores):\n",
    "            summary.append(ensemble_dic[key][2])\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7db1186a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractSummaryFromDic(dic, size):\n",
    "    summaryScores = []\n",
    "    summarySent = []\n",
    "    for i in range(size):\n",
    "        summaryScores.append(dic[i][1])\n",
    "        summarySent.append(dic[i][0])\n",
    "        \n",
    "    for i in range(size, len(dic), 1):\n",
    "        if(dic[i][1] > min(summaryScores)):\n",
    "            ind = summaryScores.index(min(summaryScores))\n",
    "            summaryScores[ind] = dic[i][1]\n",
    "            summarySent[ind] = dic[i][0]\n",
    "    return summarySent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16950de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTextRanks(filtered_sentences, sentences):\n",
    "    tm_dic = text_matching(filtered_sentences, sentences)\n",
    "    luhn_dic = luhn_algorithm(filtered_sentences, sentences)\n",
    "    lsa_dic = lsa_summarization(filtered_sentences, sentences)\n",
    "    tr_dic = textRank(filtered_sentences, sentences)\n",
    "    lex_dic = LexRank_algorithm(filtered_sentences, sentences)\n",
    "    lda_dic = lda_algorithm(filtered_sentences, sentences)\n",
    "    ensemble_dic, ensemble_scores = createEnsembledic(tm_dic, luhn_dic, lsa_dic, tr_dic, lex_dic, lda_dic)\n",
    "    return ensemble_dic#, ensemble_scores, tm_dic, luhn_dic, lsa_dic, tr_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe9b1bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combinations(arr):\n",
    "    combinations = []\n",
    "    for i in range(1,len(arr)+1):\n",
    "        for element in itertools.combinations(arr, i):\n",
    "            combinations.append(element)\n",
    "    return combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9c8afe88-38a2-442c-a79e-505b9e60547e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def allCombs(ensemble_dic, useWeights, weights):\n",
    "    Algorithms = [\"Sent\",\"Tm\",\"Lex\",\"Luhn\",\"Lsa\",\"Tr\",\"LDA\"]\n",
    "    lst = []\n",
    "    lstCols = Algorithms #get_combinations(Algorithms)\n",
    "    wtm = 1\n",
    "    wlex = 1\n",
    "    wluhn = 1\n",
    "    wlsa = 1\n",
    "    wtr = 1\n",
    "    wlda = 1\n",
    "    if(useWeights):\n",
    "        wtm = weights[\"tm\"]\n",
    "        wlex =  weights[\"lex\"]\n",
    "        wluhn =  weights[\"luhn\"]\n",
    "        wlsa =  weights[\"lsa\"]\n",
    "        wtr =  weights[\"tr\"]\n",
    "        wlda =  weights[\"lda\"]\n",
    "    for i in ensemble_dic:\n",
    "        tm = ensemble_dic[i][1][0] * wtm\n",
    "        luhn = ensemble_dic[i][1][1] * wluhn\n",
    "        lsa = ensemble_dic[i][1][2] * wlsa\n",
    "        tr = ensemble_dic[i][1][3] * wtr\n",
    "        lex = ensemble_dic[i][1][4] * wlex\n",
    "        lda = ensemble_dic[i][1][5] * wlda\n",
    "        values = [ensemble_dic[i][2],tm,lex,luhn,lsa,tr,lda]\n",
    "        lst.append(values)\n",
    "        # CombNums = get_combinations(values)\n",
    "        # for i,comb in enumerate(CombNums):\n",
    "        #     CombNums[i] = sum(comb)/len(comb)\n",
    "        # lst.append(CombNums)\n",
    "    combinationScores = pd.DataFrame(lst, columns = lstCols)     \n",
    "    return combinationScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5bd95b16-c17b-4206-b23b-a418e9eb9917",
   "metadata": {},
   "outputs": [],
   "source": [
    "def allCombsDf(df, useWeights = False, weights = []):\n",
    "    Algorithms = [\"Tm\",\"Lex\",\"Luhn\",\"Lsa\",\"Tr\",\"LDA\"]\n",
    "    lst = []\n",
    "    lstCols = get_combinations(Algorithms)\n",
    "    wtm = 1\n",
    "    wlex = 1\n",
    "    wluhn = 1\n",
    "    wlsa = 1\n",
    "    wtr = 1\n",
    "    wlda = 1\n",
    "    if(useWeights):\n",
    "        wtm = weights[\"tm\"]\n",
    "        wlex =  weights[\"lex\"]\n",
    "        wluhn =  weights[\"luhn\"]\n",
    "        wlsa =  weights[\"lsa\"]\n",
    "        wtr =  weights[\"tr\"]\n",
    "        wlda =  weights[\"lda\"]\n",
    "    for row in df.iterrows():\n",
    "        tm = row[1][Algorithms[0]] * wtm\n",
    "        luhn = row[1][Algorithms[1]] * wluhn\n",
    "        lsa = row[1][Algorithms[2]] * wlsa\n",
    "        tr = row[1][Algorithms[3]] * wtr\n",
    "        lex = row[1][Algorithms[4]] * wlex\n",
    "        lda = row[1][Algorithms[5]] * wlda\n",
    "        values = [tm,lex,luhn,lsa,tr,lda]\n",
    "\n",
    "        CombNums = get_combinations(values)\n",
    "        for i,comb in enumerate(CombNums):\n",
    "            CombNums[i] = sum(comb)/len(comb)\n",
    "        lst.append(CombNums)\n",
    "    combinationScores = pd.DataFrame(lst, columns = lstCols)     \n",
    "    return combinationScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c73d2545-2a0f-40ff-96ba-e5ce89125086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildDF(filtered_sentences, sentences, useWeights = False, weights = []):\n",
    "    ensemble_dic = getTextRanks(filtered_sentences, sentences)\n",
    "    df = allCombs(ensemble_dic, useWeights, weights)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6b6876d2-1f2e-4f0e-980b-0484a82c5e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarizeWith(sentences, df, algorithm, percentage):\n",
    "    if(percentage >= 1):\n",
    "        sent = int(percentage)\n",
    "    else:\n",
    "        sent = int(percentage * len(df))\n",
    "    summInds = df.nlargest(n=sent, columns=[algorithm])[algorithm].keys()\n",
    "    # summInds = sorted(summInds)\n",
    "    summ = \"\"\n",
    "    for i in summInds:\n",
    "        summ += sentences[i]\n",
    "    return summ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "79151fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open (\"summary input.txt\", \"r\") as myfile:\n",
    "#     data = myfile.read()\n",
    "#     filtered_sentences,sentences = tokenization(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6390a82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# buildDF(filtered_sentences, sentences)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
